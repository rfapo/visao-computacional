{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Módulo 8: Vision Transformers e Mecanismos de Atenção",
        "",
        "## Objetivos de Aprendizagem",
        "- Compreender os fundamentos dos mecanismos de atenção",
        "- Entender a arquitetura Vision Transformer (ViT)",
        "- Implementar modelos de atenção para visão computacional",
        "- Analisar vantagens e limitações dos Vision Transformers",
        "- Aplicar ViT em tarefas práticas de visão computacional",
        "",
        "---",
        "",
        "## 8.1 Introdução aos Mecanismos de Atenção",
        "",
        "**Mecanismos de Atenção** são componentes fundamentais que permitem aos modelos focar em partes específicas dos dados de entrada, revolucionando tanto o processamento de linguagem natural quanto a visão computacional.",
        "",
        "![Introdução Mecanismos Atenção](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo8/introducao_mecanismos_atencao.png)",
        "",
        "### Conceitos Fundamentais",
        "",
        "**Definição de Atenção:**",
        "- **Foco seletivo**: Capacidade de concentrar recursos computacionais em partes relevantes",
        "- **Pesos dinâmicos**: Atribuição de importância variável a diferentes elementos",
        "- **Contexto global**: Consideração de todas as informações disponíveis",
        "- **Processamento paralelo**: Capacidade de processar múltiplas relações simultaneamente",
        "",
        "**Analogia Biológica:**",
        "- **Atenção humana**: Foco seletivo em estímulos relevantes",
        "- **Processamento visual**: Concentração em características importantes",
        "- **Memória de trabalho**: Manutenção de informações relevantes",
        "- **Decisão**: Integração de múltiplas fontes de informação",
        "",
        "### Tipos de Atenção",
        "",
        "**1. Self-Attention (Atenção Própria):**",
        "- **Relações internas**: Cada elemento interage com todos os outros",
        "- **Pesos computados**: Importância calculada dinamicamente",
        "- **Contexto completo**: Consideração de toda a sequência",
        "- **Paralelização**: Processamento simultâneo",
        "",
        "**2. Multi-Head Attention:**",
        "- **Múltiplas cabeças**: Diferentes representações de atenção",
        "- **Diversidade**: Captura diferentes tipos de relações",
        "- **Concatenação**: Combinação de múltiplas cabeças",
        "- **Flexibilidade**: Maior capacidade expressiva",
        "",
        "**3. Cross-Attention:**",
        "- **Interação**: Entre sequências diferentes",
        "- **Query**: De uma sequência",
        "- **Key/Value**: De outra sequência",
        "- **Aplicação**: Tradução, geração, multimodal",
        "",
        "![Tipos de Atenção](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo8/tipos_atencao.png)",
        "",
        "### Evolução dos Mecanismos de Atenção",
        "",
        "**Progressão Histórica:**",
        "- **2014**: Attention mechanism introduzido para tradução",
        "- **2017**: Transformer com self-attention",
        "- **2018**: BERT com atenção bidirecional",
        "- **2020**: Vision Transformer (ViT) para imagens",
        "- **2021**: CLIP com atenção multimodal",
        "- **2022**: DALL-E com atenção para geração",
        "",
        "![Evolução Mecanismos Atenção](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo8/evolucao_mecanismos_atencao.png)",
        "",
        "**Marcos Importantes:**",
        "- **2014**: Neural Machine Translation by Jointly Learning to Align and Translate - Bahdanau et al.",
        "- **2017**: Attention Is All You Need - Vaswani et al.",
        "- **2020**: An Image is Worth 16x16 Words: Transformers for Image Recognition - Dosovitskiy et al.",
        "- **2021**: CLIP: Learning Transferable Visual Representations - Radford et al.",
        "",
        "**Referências:**",
        "- [Attention Is All You Need - Vaswani et al.](https://arxiv.org/abs/1706.03762)",
        "- [An Image is Worth 16x16 Words: Transformers for Image Recognition - Dosovitskiy et al.](https://arxiv.org/abs/2010.11929)",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.2 Demonstração Prática: Mecanismos de Atenção\n",
        "\n",
        "Vamos implementar e visualizar diferentes tipos de atenção:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "class AttentionMechanismsDemo:\n",
        "    \"\"\"Demonstração de diferentes mecanismos de atenção\"\"\"\n",
        "    \n",
        "    def __init__(self, seq_len=10, d_model=64):\n",
        "        self.seq_len = seq_len\n",
        "        self.d_model = d_model\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "    def create_sample_data(self):\n",
        "        \"\"\"Cria dados de exemplo para demonstração\"\"\"\n",
        "        \n",
        "        # Criar sequência de entrada\n",
        "        x = torch.randn(1, self.seq_len, self.d_model)\n",
        "        \n",
        "        # Adicionar padrões específicos\n",
        "        # Padrão 1: Valores altos no meio\n",
        "        x[0, 3:7, :] += 2.0\n",
        "        \n",
        "        # Padrão 2: Valores baixos no início\n",
        "        x[0, 0:2, :] -= 1.5\n",
        "        \n",
        "        # Padrão 3: Valores médios no final\n",
        "        x[0, 8:, :] += 0.5\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def self_attention(self, x):\n",
        "        \"\"\"Implementa self-attention\"\"\"\n",
        "        \n",
        "        # Linear transformations\n",
        "        W_q = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        W_k = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        W_v = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        \n",
        "        # Compute Q, K, V\n",
        "        Q = W_q(x)\n",
        "        K = W_k(x)\n",
        "        V = W_v(x)\n",
        "        \n",
        "        # Compute attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_model)\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        \n",
        "        # Apply attention to values\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        \n",
        "        return output, attention_weights\n",
        "    \n",
        "    def multi_head_attention(self, x, num_heads=8):\n",
        "        \"\"\"Implementa multi-head attention\"\"\"\n",
        "        \n",
        "        class MultiHeadAttention(nn.Module):\n",
        "            def __init__(self, d_model, num_heads):\n",
        "                super(MultiHeadAttention, self).__init__()\n",
        "                self.d_model = d_model\n",
        "                self.num_heads = num_heads\n",
        "                self.d_k = d_model // num_heads\n",
        "                \n",
        "                self.W_q = nn.Linear(d_model, d_model)\n",
        "                self.W_k = nn.Linear(d_model, d_model)\n",
        "                self.W_v = nn.Linear(d_model, d_model)\n",
        "                self.W_o = nn.Linear(d_model, d_model)\n",
        "                \n",
        "            def forward(self, x):\n",
        "                batch_size, seq_len, d_model = x.size()\n",
        "                \n",
        "                # Linear transformations\n",
        "                Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "                K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "                V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "                \n",
        "                # Compute attention scores\n",
        "                scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
        "                attention_weights = F.softmax(scores, dim=-1)\n",
        "                \n",
        "                # Apply attention to values\n",
        "                attended_values = torch.matmul(attention_weights, V)\n",
        "                \n",
        "                # Concatenate heads\n",
        "                attended_values = attended_values.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
        "                \n",
        "                # Final linear transformation\n",
        "                output = self.W_o(attended_values)\n",
        "                \n",
        "                return output, attention_weights\n",
        "        \n",
        "        mha = MultiHeadAttention(self.d_model, num_heads)\n",
        "        output, attention_weights = mha(x)\n",
        "        \n",
        "        return output, attention_weights\n",
        "    \n",
        "    def cross_attention(self, x, context):\n",
        "        \"\"\"Implementa cross-attention\"\"\"\n",
        "        \n",
        "        # Linear transformations\n",
        "        W_q = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        W_k = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        W_v = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        \n",
        "        # Compute Q from x, K and V from context\n",
        "        Q = W_q(x)\n",
        "        K = W_k(context)\n",
        "        V = W_v(context)\n",
        "        \n",
        "        # Compute attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_model)\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        \n",
        "        # Apply attention to values\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        \n",
        "        return output, attention_weights\n",
        "    \n",
        "    def visualize_attention_weights(self, attention_weights, title=\"Attention Weights\"):\n",
        "        \"\"\"Visualiza os pesos de atenção\"\"\"\n",
        "        \n",
        "        # Converter para numpy\n",
        "        if isinstance(attention_weights, torch.Tensor):\n",
        "            attention_weights = attention_weights.detach().numpy()\n",
        "        \n",
        "        # Se for multi-head, usar a média\n",
        "        if len(attention_weights.shape) == 4:  # (batch, heads, seq, seq)\n",
        "            attention_weights = np.mean(attention_weights, axis=1)\n",
        "        \n",
        "        # Remover dimensão do batch se existir\n",
        "        if len(attention_weights.shape) == 3:\n",
        "            attention_weights = attention_weights[0]\n",
        "        \n",
        "        # Criar heatmap\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(attention_weights, \n",
        "                    annot=True, \n",
        "                    fmt='.2f', \n",
        "                    cmap='Blues',\n",
        "                    xticklabels=[f'Pos {i}' for i in range(attention_weights.shape[1])],\n",
        "                    yticklabels=[f'Pos {i}' for i in range(attention_weights.shape[0])])\n",
        "        \n",
        "        plt.title(title)\n",
        "        plt.xlabel('Key Positions')\n",
        "        plt.ylabel('Query Positions')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return attention_weights\n",
        "    \n",
        "    def analyze_attention_patterns(self, attention_weights):\n",
        "        \"\"\"Analisa padrões de atenção\"\"\"\n",
        "        \n",
        "        # Converter para numpy\n",
        "        if isinstance(attention_weights, torch.Tensor):\n",
        "            attention_weights = attention_weights.detach().numpy()\n",
        "        \n",
        "        # Se for multi-head, usar a média\n",
        "        if len(attention_weights.shape) == 4:\n",
        "            attention_weights = np.mean(attention_weights, axis=1)\n",
        "        \n",
        "        # Remover dimensão do batch se existir\n",
        "        if len(attention_weights.shape) == 3:\n",
        "            attention_weights = attention_weights[0]\n",
        "        \n",
        "        # Análise dos padrões\n",
        "        print(f\"\\n=== ANÁLISE DOS PADRÕES DE ATENÇÃO ===\")\n",
        "        \n",
        "        # Entropia da atenção\n",
        "        entropy = -np.sum(attention_weights * np.log(attention_weights + 1e-8), axis=1)\n",
        "        print(f\"Entropia média: {np.mean(entropy):.3f}\")\n",
        "        print(f\"Entropia máxima: {np.max(entropy):.3f}\")\n",
        "        print(f\"Entropia mínima: {np.min(entropy):.3f}\")\n",
        "        \n",
        "        # Concentração da atenção\n",
        "        max_attention = np.max(attention_weights, axis=1)\n",
        "        print(f\"\\nConcentração máxima média: {np.mean(max_attention):.3f}\")\n",
        "        print(f\"Posições com maior concentração: {np.argmax(max_attention)}\")\n",
        "        \n",
        "        # Padrões locais vs globais\n",
        "        diagonal_attention = np.diag(attention_weights)\n",
        "        off_diagonal_attention = attention_weights - np.diag(diagonal_attention)\n",
        "        \n",
        "        print(f\"\\nAtenção diagonal média: {np.mean(diagonal_attention):.3f}\")\n",
        "        print(f\"Atenção off-diagonal média: {np.mean(off_diagonal_attention):.3f}\")\n",
        "        \n",
        "        return {\n",
        "            'entropy': entropy,\n",
        "            'max_attention': max_attention,\n",
        "            'diagonal_attention': diagonal_attention,\n",
        "            'off_diagonal_attention': off_diagonal_attention\n",
        "        }\n",
        "\n",
        "def demonstrate_attention_mechanisms():\n",
        "    \"\"\"Demonstra diferentes mecanismos de atenção\"\"\"\n",
        "    \n",
        "    print(\"=== Demonstração de Mecanismos de Atenção ===\")\n",
        "    \n",
        "    # Criar instância\n",
        "    demo = AttentionMechanismsDemo(seq_len=10, d_model=64)\n",
        "    \n",
        "    # Criar dados de exemplo\n",
        "    print(\"\\nCriando dados de exemplo...\")\n",
        "    x = demo.create_sample_data()\n",
        "    \n",
        "    # Self-attention\n",
        "    print(\"\\nExecutando self-attention...\")\n",
        "    sa_output, sa_weights = demo.self_attention(x)\n",
        "    demo.visualize_attention_weights(sa_weights, \"Self-Attention\")\n",
        "    sa_analysis = demo.analyze_attention_patterns(sa_weights)\n",
        "    \n",
        "    # Multi-head attention\n",
        "    print(\"\\nExecutando multi-head attention...\")\n",
        "    mha_output, mha_weights = demo.multi_head_attention(x, num_heads=8)\n",
        "    demo.visualize_attention_weights(mha_weights, \"Multi-Head Attention (Média)\")\n",
        "    mha_analysis = demo.analyze_attention_patterns(mha_weights)\n",
        "    \n",
        "    # Cross-attention\n",
        "    print(\"\\nExecutando cross-attention...\")\n",
        "    context = torch.randn(1, 8, 64)  # Contexto diferente\n",
        "    ca_output, ca_weights = demo.cross_attention(x, context)\n",
        "    demo.visualize_attention_weights(ca_weights, \"Cross-Attention\")\n",
        "    ca_analysis = demo.analyze_attention_patterns(ca_weights)\n",
        "    \n",
        "    return sa_weights, mha_weights, ca_weights\n",
        "\n",
        "# Executar demonstração\n",
        "sa_weights, mha_weights, ca_weights = demonstrate_attention_mechanisms()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análise dos Resultados\n",
        "\n",
        "**Mecanismos de Atenção Observados:**\n",
        "\n",
        "1. **Self-Attention**: Cada posição interage com todas as outras\n",
        "2. **Multi-Head Attention**: Múltiplas representações de atenção\n",
        "3. **Cross-Attention**: Interação entre sequências diferentes\n",
        "4. **Padrões**: Concentração em posições relevantes\n",
        "\n",
        "**Insights Importantes:**\n",
        "- **Foco seletivo**: Atenção concentrada em posições importantes\n",
        "- **Contexto global**: Consideração de toda a sequência\n",
        "- **Paralelização**: Processamento simultâneo de relações\n",
        "- **Flexibilidade**: Adaptação a diferentes padrões\n",
        "\n",
        "**Referências:**\n",
        "- [Attention Is All You Need - Vaswani et al.](https://arxiv.org/abs/1706.03762)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.3 Vision Transformers (ViT)",
        "",
        "**Vision Transformers** são uma adaptação da arquitetura Transformer para processamento de imagens, revolucionando a visão computacional ao aplicar mecanismos de atenção diretamente em patches de imagem.",
        "",
        "![Vision Transformers](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo8/vision_transformers.png)",
        "",
        "### Arquitetura Vision Transformer",
        "",
        "**Componentes Principais:**",
        "",
        "**1. Patch Embedding:**",
        "- **Divisão em patches**: Imagem dividida em patches de 16×16 ou 32×32",
        "- **Embedding linear**: Cada patch convertido em vetor de dimensão fixa",
        "- **Posição**: Embedding de posição adicionado a cada patch",
        "- **CLS token**: Token especial para classificação",
        "",
        "**2. Transformer Encoder:**",
        "- **Multi-Head Attention**: Múltiplas cabeças de atenção",
        "- **Feed-Forward Networks**: Redes feed-forward",
        "- **Layer Normalization**: Normalização de camadas",
        "- **Residual Connections**: Conexões residuais",
        "",
        "![Arquitetura Vision Transformer](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo8/arquitetura_vision_transformer.png)",
        "",
        "**3. Classification Head:**",
        "- **CLS token**: Token de classificação",
        "- **MLP**: Multi-layer perceptron",
        "- **Softmax**: Probabilidades de classe",
        "- **Output**: Predição da classe",
        "",
        "### Processo de Processamento",
        "",
        "**1. Pré-processamento:**",
        "- **Redimensionamento**: Imagem redimensionada para tamanho fixo",
        "- **Divisão em patches**: Imagem dividida em patches",
        "- **Embedding**: Patches convertidos em vetores",
        "- **Posição**: Embedding de posição adicionado",
        "",
        "**2. Processamento:**",
        "- **Multi-Head Attention**: Atenção entre patches",
        "- **Feed-Forward**: Processamento feed-forward",
        "- **Residual**: Conexões residuais",
        "- **Normalization**: Normalização de camadas",
        "",
        "**3. Classificação:**",
        "- **CLS token**: Token de classificação",
        "- **MLP**: Multi-layer perceptron",
        "- **Softmax**: Probabilidades",
        "- **Predição**: Classe final",
        "",
        "### Vantagens dos Vision Transformers",
        "",
        "**1. Escalabilidade:**",
        "- **Dados**: Performance melhora com mais dados",
        "- **Modelo**: Arquitetura escalável",
        "- **Computação**: Eficiência computacional",
        "- **Paralelização**: Processamento paralelo",
        "",
        "**2. Flexibilidade:**",
        "- **Arquitetura**: Arquitetura uniforme",
        "- **Tarefas**: Adaptável a diferentes tarefas",
        "- **Domínios**: Funciona em diferentes domínios",
        "- **Integração**: Fácil integração com outros modelos",
        "",
        "**3. Interpretabilidade:**",
        "- **Atenção**: Pesos de atenção interpretáveis",
        "- **Patches**: Foco em patches específicos",
        "- **Visualização**: Visualização de atenção",
        "- **Análise**: Análise de padrões",
        "",
        "![Vantagens Vision Transformers](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo8/vantagens_vision_transformers.png)",
        "",
        "**Referências:**",
        "- [An Image is Worth 16x16 Words: Transformers for Image Recognition - Dosovitskiy et al.](https://arxiv.org/abs/2010.11929)",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.4 Demonstração Prática: Vision Transformer Simples\n",
        "\n",
        "Vamos implementar e visualizar um Vision Transformer simples:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "class SimpleVisionTransformer:\n",
        "    \"\"\"Implementação de um Vision Transformer simples para demonstração\"\"\"\n",
        "    \n",
        "    def __init__(self, img_size=64, patch_size=8, num_classes=10, d_model=128, num_heads=8, num_layers=4):\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_classes = num_classes\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "        # Calcular número de patches\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        \n",
        "        # Inicializar modelo\n",
        "        self.model = self._build_vit()\n",
        "        \n",
        "    def _build_vit(self):\n",
        "        \"\"\"Constrói o Vision Transformer\"\"\"\n",
        "        \n",
        "        class VisionTransformer(nn.Module):\n",
        "            def __init__(self, img_size, patch_size, num_classes, d_model, num_heads, num_layers, num_patches):\n",
        "                super(VisionTransformer, self).__init__()\n",
        "                \n",
        "                self.img_size = img_size\n",
        "                self.patch_size = patch_size\n",
        "                self.num_classes = num_classes\n",
        "                self.d_model = d_model\n",
        "                self.num_heads = num_heads\n",
        "                self.num_layers = num_layers\n",
        "                self.num_patches = num_patches\n",
        "                \n",
        "                # Patch embedding\n",
        "                self.patch_embedding = nn.Conv2d(3, d_model, kernel_size=patch_size, stride=patch_size)\n",
        "                \n",
        "                # Positional embedding\n",
        "                self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, d_model))\n",
        "                \n",
        "                # CLS token\n",
        "                self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
        "                \n",
        "                # Transformer encoder\n",
        "                encoder_layer = nn.TransformerEncoderLayer(\n",
        "                    d_model=d_model,\n",
        "                    nhead=num_heads,\n",
        "                    dim_feedforward=d_model * 4,\n",
        "                    dropout=0.1,\n",
        "                    batch_first=True\n",
        "                )\n",
        "                self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "                \n",
        "                # Classification head\n",
        "                self.classification_head = nn.Sequential(\n",
        "                    nn.LayerNorm(d_model),\n",
        "                    nn.Linear(d_model, d_model // 2),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(0.1),\n",
        "                    nn.Linear(d_model // 2, num_classes)\n",
        "                )\n",
        "                \n",
        "            def forward(self, x):\n",
        "                batch_size = x.shape[0]\n",
        "                \n",
        "                # Patch embedding\n",
        "                x = self.patch_embedding(x)  # (batch, d_model, H/patch_size, W/patch_size)\n",
        "                x = x.flatten(2).transpose(1, 2)  # (batch, num_patches, d_model)\n",
        "                \n",
        "                # Adicionar CLS token\n",
        "                cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
        "                x = torch.cat((cls_tokens, x), dim=1)  # (batch, num_patches + 1, d_model)\n",
        "                \n",
        "                # Adicionar positional embedding\n",
        "                x = x + self.pos_embedding\n",
        "                \n",
        "                # Transformer encoder\n",
        "                x = self.transformer_encoder(x)\n",
        "                \n",
        "                # Classification head (usar apenas CLS token)\n",
        "                cls_output = x[:, 0]  # (batch, d_model)\n",
        "                output = self.classification_head(cls_output)\n",
        "                \n",
        "                return output, x\n",
        "        \n",
        "        return VisionTransformer(\n",
        "            self.img_size, self.patch_size, self.num_classes,\n",
        "            self.d_model, self.num_heads, self.num_layers, self.num_patches\n",
        "        ).to(self.device)\n",
        "    \n",
        "    def create_sample_images(self, num_images=16):\n",
        "        \"\"\"Cria imagens de exemplo para demonstração\"\"\"\n",
        "        \n",
        "        images = []\n",
        "        labels = []\n",
        "        \n",
        "        for i in range(num_images):\n",
        "            # Criar imagem base\n",
        "            img = np.zeros((self.img_size, self.img_size, 3), dtype=np.float32)\n",
        "            \n",
        "            # Adicionar padrões diferentes\n",
        "            if i % 4 == 0:\n",
        "                # Círculos\n",
        "                cv2.circle(img, (32, 32), 20, (1, 0, 0), -1)\n",
        "                label = 0\n",
        "            elif i % 4 == 1:\n",
        "                # Retângulos\n",
        "                cv2.rectangle(img, (20, 20), (44, 44), (0, 1, 0), -1)\n",
        "                label = 1\n",
        "            elif i % 4 == 2:\n",
        "                # Triângulos\n",
        "                pts = np.array([[32, 20], [20, 44], [44, 44]], np.int32)\n",
        "                cv2.fillPoly(img, [pts], (0, 0, 1))\n",
        "                label = 2\n",
        "            else:\n",
        "                # Linhas\n",
        "                cv2.line(img, (20, 20), (44, 44), (1, 1, 0), 3)\n",
        "                label = 3\n",
        "            \n",
        "            # Adicionar ruído\n",
        "            noise = np.random.normal(0, 0.1, img.shape)\n",
        "            img = np.clip(img + noise, 0, 1)\n",
        "            \n",
        "            images.append(img)\n",
        "            labels.append(label)\n",
        "        \n",
        "        return np.array(images), np.array(labels)\n",
        "    \n",
        "    def visualize_patches(self, image, patch_size=8):\n",
        "        \"\"\"Visualiza os patches de uma imagem\"\"\"\n",
        "        \n",
        "        # Converter para tensor\n",
        "        if isinstance(image, np.ndarray):\n",
        "            image = torch.FloatTensor(image).permute(2, 0, 1).unsqueeze(0)\n",
        "        \n",
        "        # Aplicar patch embedding\n",
        "        patches = self.model.patch_embedding(image)\n",
        "        patches = patches.flatten(2).transpose(1, 2)\n",
        "        \n",
        "        # Converter para numpy\n",
        "        patches = patches.squeeze().detach().numpy()\n",
        "        \n",
        "        # Visualizar patches\n",
        "        num_patches_per_side = self.img_size // patch_size\n",
        "        \n",
        "        fig, axes = plt.subplots(num_patches_per_side, num_patches_per_side, figsize=(12, 12))\n",
        "        \n",
        "        for i in range(num_patches_per_side):\n",
        "            for j in range(num_patches_per_side):\n",
        "                patch_idx = i * num_patches_per_side + j\n",
        "                patch = patches[patch_idx]\n",
        "                \n",
        "                # Normalizar patch\n",
        "                patch_normalized = (patch - patch.min()) / (patch.max() - patch.min() + 1e-8)\n",
        "                \n",
        "                # Visualizar como heatmap\n",
        "                axes[i, j].imshow(patch_normalized.reshape(patch_size, patch_size), cmap='viridis')\n",
        "                axes[i, j].set_title(f'Patch {patch_idx}')\n",
        "                axes[i, j].axis('off')\n",
        "        \n",
        "        plt.suptitle('Visualização dos Patches', fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return patches\n",
        "    \n",
        "    def visualize_attention(self, image, layer_idx=0):\n",
        "        \"\"\"Visualiza os pesos de atenção\"\"\"\n",
        "        \n",
        "        # Converter para tensor\n",
        "        if isinstance(image, np.ndarray):\n",
        "            image = torch.FloatTensor(image).permute(2, 0, 1).unsqueeze(0).to(self.device)\n",
        "        \n",
        "        # Forward pass\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            output, features = self.model(image)\n",
        "        \n",
        "        # Simular pesos de atenção (em implementação real, extrair do modelo)\n",
        "        num_patches = self.num_patches + 1  # +1 para CLS token\n",
        "        attention_weights = torch.randn(num_patches, num_patches)\n",
        "        attention_weights = F.softmax(attention_weights, dim=-1)\n",
        "        \n",
        "        # Visualizar atenção do CLS token\n",
        "        cls_attention = attention_weights[0, 1:]  # Excluir CLS token\n",
        "        cls_attention = cls_attention.reshape(self.img_size // self.patch_size, self.img_size // self.patch_size)\n",
        "        \n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        \n",
        "        # Imagem original\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            img_np = image.squeeze().permute(1, 2, 0).cpu().numpy()\n",
        "        else:\n",
        "            img_np = image\n",
        "        \n",
        "        axes[0].imshow(img_np)\n",
        "        axes[0].set_title('Imagem Original')\n",
        "        axes[0].axis('off')\n",
        "        \n",
        "        # Mapa de atenção\n",
        "        im = axes[1].imshow(cls_attention.cpu().numpy(), cmap='hot', interpolation='nearest')\n",
        "        axes[1].set_title('Mapa de Atenção (CLS Token)')\n",
        "        axes[1].axis('off')\n",
        "        \n",
        "        # Adicionar colorbar\n",
        "        plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n",
        "        \n",
        "        plt.suptitle('Visualização da Atenção', fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return cls_attention\n",
        "    \n",
        "    def analyze_model_performance(self, images, labels):\n",
        "        \"\"\"Analisa a performance do modelo\"\"\"\n",
        "        \n",
        "        # Converter para tensor\n",
        "        images_tensor = torch.FloatTensor(images).permute(0, 3, 1, 2).to(self.device)\n",
        "        labels_tensor = torch.LongTensor(labels).to(self.device)\n",
        "        \n",
        "        # Forward pass\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs, features = self.model(images_tensor)\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "            probabilities = F.softmax(outputs, dim=1)\n",
        "        \n",
        "        # Calcular métricas\n",
        "        accuracy = (predictions == labels_tensor).float().mean().item()\n",
        "        \n",
        "        # Visualizar resultados\n",
        "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "        \n",
        "        for i in range(8):\n",
        "            row = i // 4\n",
        "            col = i % 4\n",
        "            \n",
        "            # Imagem\n",
        "            axes[row, col].imshow(images[i])\n",
        "            \n",
        "            # Título com predição\n",
        "            pred_class = predictions[i].item()\n",
        "            true_class = labels[i]\n",
        "            confidence = probabilities[i, pred_class].item()\n",
        "            \n",
        "            title = f'Pred: {pred_class}, True: {true_class}\\nConf: {confidence:.2f}'\n",
        "            axes[row, col].set_title(title)\n",
        "            axes[row, col].axis('off')\n",
        "        \n",
        "        plt.suptitle(f'Performance do Vision Transformer (Accuracy: {accuracy:.2f})', fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Resumo das métricas\n",
        "        print(f\"\\n=== RESUMO DA PERFORMANCE ===\")\n",
        "        print(f\"Accuracy: {accuracy:.2f}\")\n",
        "        print(f\"Número de patches: {self.num_patches}\")\n",
        "        print(f\"Dimensão do modelo: {self.d_model}\")\n",
        "        print(f\"Número de cabeças: {self.num_heads}\")\n",
        "        print(f\"Número de camadas: {self.num_layers}\")\n",
        "        \n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'predictions': predictions.cpu().numpy(),\n",
        "            'probabilities': probabilities.cpu().numpy()\n",
        "        }\n",
        "\n",
        "def demonstrate_vision_transformer():\n",
        "    \"\"\"Demonstra um Vision Transformer simples\"\"\"\n",
        "    \n",
        "    print(\"=== Demonstração de Vision Transformer ===\")\n",
        "    print(\"\\nNota: Esta demonstração usa dados sintéticos para fins educacionais.\")\n",
        "    print(\"Em aplicações reais, use datasets reais de imagens.\")\n",
        "    \n",
        "    # Criar instância do ViT\n",
        "    vit = SimpleVisionTransformer(img_size=64, patch_size=8, num_classes=4, d_model=128, num_heads=8, num_layers=4)\n",
        "    \n",
        "    # Criar imagens de exemplo\n",
        "    print(\"\\nCriando imagens de exemplo...\")\n",
        "    images, labels = vit.create_sample_images(num_images=16)\n",
        "    \n",
        "    # Visualizar patches\n",
        "    print(\"\\nVisualizando patches...\")\n",
        "    patches = vit.visualize_patches(images[0])\n",
        "    \n",
        "    # Visualizar atenção\n",
        "    print(\"\\nVisualizando atenção...\")\n",
        "    attention = vit.visualize_attention(images[0])\n",
        "    \n",
        "    # Analisar performance\n",
        "    print(\"\\nAnalisando performance...\")\n",
        "    performance = vit.analyze_model_performance(images, labels)\n",
        "    \n",
        "    return vit, images, labels, performance\n",
        "\n",
        "# Executar demonstração\n",
        "vit_model, sample_images, sample_labels, model_performance = demonstrate_vision_transformer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análise dos Resultados\n",
        "\n",
        "**Vision Transformer Observado:**\n",
        "\n",
        "1. **Patches**: Imagem dividida em patches de 8×8\n",
        "2. **Atenção**: Mapa de atenção do CLS token\n",
        "3. **Performance**: Accuracy na classificação\n",
        "4. **Arquitetura**: Encoder com múltiplas camadas\n",
        "\n",
        "**Insights Importantes:**\n",
        "- **Patches**: Divisão eficiente da imagem\n",
        "- **Atenção**: Foco em regiões relevantes\n",
        "- **Escalabilidade**: Arquitetura escalável\n",
        "- **Flexibilidade**: Adaptável a diferentes tarefas\n",
        "\n",
        "**Referências:**\n",
        "- [An Image is Worth 16x16 Words: Transformers for Image Recognition - Dosovitskiy et al.](https://arxiv.org/abs/2010.11929)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8.5 Comparação: CNNs vs Vision Transformers",
        "",
        "### Diferenças Fundamentais",
        "",
        "**CNNs:**",
        "- **Indução**: Indução de bias local",
        "- **Convolução**: Operações locais",
        "- **Hierarquia**: Processamento hierárquico",
        "- **Eficiência**: Eficiente para dados limitados",
        "",
        "**Vision Transformers:**",
        "- **Atenção**: Atenção global",
        "- **Patches**: Processamento de patches",
        "- **Uniformidade**: Arquitetura uniforme",
        "- **Escalabilidade**: Eficiente para grandes datasets",
        "",
        "![Comparação CNNs vs Vision Transformers](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo8/comparacao_cnns_vit.png)",
        "",
        "### Vantagens e Desvantagens",
        "",
        "**CNNs - Vantagens:**",
        "- **Eficiência**: Eficiente para dados limitados",
        "- **Indução**: Bias local apropriado",
        "- **Maturidade**: Arquitetura madura",
        "- **Recursos**: Menos recursos computacionais",
        "",
        "**CNNs - Desvantagens:**",
        "- **Limitação**: Campo receptivo limitado",
        "- **Hierarquia**: Processamento hierárquico",
        "- **Flexibilidade**: Menos flexível",
        "- **Escalabilidade**: Limitada escalabilidade",
        "",
        "**Vision Transformers - Vantagens:**",
        "- **Escalabilidade**: Escalável com dados",
        "- **Atenção**: Atenção global",
        "- **Flexibilidade**: Arquitetura flexível",
        "- **Uniformidade**: Arquitetura uniforme",
        "",
        "**Vision Transformers - Desvantagens:**",
        "- **Dados**: Requer muitos dados",
        "- **Recursos**: Mais recursos computacionais",
        "- **Complexidade**: Mais complexo",
        "- **Indução**: Menos bias local",
        "",
        "### Aplicações Práticas",
        "",
        "**CNNs - Casos de Uso:**",
        "- **Dados limitados**: Quando há poucos dados",
        "- **Recursos limitados**: Quando há limitações computacionais",
        "- **Tarefas específicas**: Tarefas que se beneficiam de bias local",
        "- **Deploy**: Deploy em dispositivos com recursos limitados",
        "",
        "**Vision Transformers - Casos de Uso:**",
        "- **Grandes datasets**: Quando há muitos dados",
        "- **Recursos abundantes**: Quando há recursos computacionais",
        "- **Tarefas complexas**: Tarefas que requerem atenção global",
        "- **Pesquisa**: Pesquisa e desenvolvimento",
        "",
        "![Aplicações Práticas](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo8/aplicacoes_praticas.png)",
        "",
        "**Referências:**",
        "- [An Image is Worth 16x16 Words: Transformers for Image Recognition - Dosovitskiy et al.](https://arxiv.org/abs/2010.11929)",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumo do Módulo 8\n",
        "\n",
        "### Principais Conceitos Abordados\n",
        "\n",
        "1. **Mecanismos de Atenção**\n",
        "   - Conceitos fundamentais\n",
        "   - Tipos de atenção\n",
        "   - Evolução histórica\n",
        "\n",
        "2. **Vision Transformers**\n",
        "   - Arquitetura e componentes\n",
        "   - Processo de processamento\n",
        "   - Vantagens e limitações\n",
        "\n",
        "3. **Implementação Prática**\n",
        "   - Mecanismos de atenção\n",
        "   - Vision Transformer simples\n",
        "   - Visualização de resultados\n",
        "\n",
        "4. **Comparação e Aplicações**\n",
        "   - CNNs vs Vision Transformers\n",
        "   - Vantagens e desvantagens\n",
        "   - Casos de uso práticos\n",
        "\n",
        "### Demonstrações Práticas\n",
        "\n",
        "**1. Mecanismos de Atenção:**\n",
        "   - Self-attention\n",
        "   - Multi-head attention\n",
        "   - Cross-attention\n",
        "   - Visualização de pesos\n",
        "\n",
        "**2. Vision Transformer:**\n",
        "   - Patch embedding\n",
        "   - Transformer encoder\n",
        "   - Classification head\n",
        "   - Visualização de atenção\n",
        "\n",
        "### Próximos Passos\n",
        "\n",
        "No próximo módulo, exploraremos **Foundation Models para Visão Computacional**, onde aprenderemos sobre modelos como CLIP, DALL-E e GPT-4V.\n",
        "\n",
        "### Referências Principais\n",
        "\n",
        "- [Attention Is All You Need - Vaswani et al.](https://arxiv.org/abs/1706.03762)\n",
        "- [An Image is Worth 16x16 Words: Transformers for Image Recognition - Dosovitskiy et al.](https://arxiv.org/abs/2010.11929)\n",
        "- [CLIP: Learning Transferable Visual Representations - Radford et al.](https://arxiv.org/abs/2103.00020)\n",
        "\n",
        "---\n",
        "\n",
        "**Próximo Módulo**: Foundation Models para Visão Computacional\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}