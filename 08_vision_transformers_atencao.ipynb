{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MÃ³dulo 8: Vision Transformers e Mecanismos de AtenÃ§Ã£o\n",
        "\n",
        "## ğŸ¯ Objetivos de Aprendizagem\n",
        "\n",
        "Ao final deste mÃ³dulo, vocÃª serÃ¡ capaz de:\n",
        "\n",
        "- âœ… Compreender os fundamentos dos mecanismos de atenÃ§Ã£o\n",
        "- âœ… Entender a arquitetura Vision Transformer (ViT)\n",
        "- âœ… Implementar modelos de atenÃ§Ã£o para visÃ£o computacional\n",
        "- âœ… Analisar vantagens e limitaÃ§Ãµes dos Vision Transformers\n",
        "- âœ… Aplicar ViT em tarefas prÃ¡ticas de visÃ£o computacional\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¯ 8.1 IntroduÃ§Ã£o aos Mecanismos de AtenÃ§Ã£o\n",
        "\n",
        "### Conceito Fundamental\n",
        "\n",
        "**Mecanismos de AtenÃ§Ã£o** sÃ£o componentes fundamentais que permitem aos modelos focar em partes especÃ­ficas dos dados de entrada, revolucionando tanto o processamento de linguagem natural quanto a visÃ£o computacional.\n",
        "\n",
        "![IntroduÃ§Ã£o Mecanismos AtenÃ§Ã£o](https://github.com/rfapo/visao-computacional/blob/main/images/modulo8/introducao_mecanismos_atencao.png)\n",
        "\n",
        "### DefiniÃ§Ã£o e CaracterÃ­sticas\n",
        "\n",
        "**DefiniÃ§Ã£o de AtenÃ§Ã£o:**\n",
        "- **Foco seletivo**: Capacidade de concentrar recursos computacionais em partes relevantes\n",
        "- **Pesos dinÃ¢micos**: AtribuiÃ§Ã£o de importÃ¢ncia variÃ¡vel a diferentes elementos\n",
        "- **Contexto global**: ConsideraÃ§Ã£o de todas as informaÃ§Ãµes disponÃ­veis\n",
        "- **Processamento paralelo**: Capacidade de processar mÃºltiplas relaÃ§Ãµes simultaneamente\n",
        "\n",
        "### Analogia BiolÃ³gica\n",
        "\n",
        "**Sistema de AtenÃ§Ã£o Humano:**\n",
        "- **AtenÃ§Ã£o humana**: Foco seletivo em estÃ­mulos relevantes\n",
        "- **Processamento visual**: ConcentraÃ§Ã£o em caracterÃ­sticas importantes\n",
        "- **MemÃ³ria de trabalho**: ManutenÃ§Ã£o de informaÃ§Ãµes relevantes\n",
        "- **DecisÃ£o**: IntegraÃ§Ã£o de mÃºltiplas fontes de informaÃ§Ã£o\n",
        "\n",
        "**Analogia com Redes Neurais:**\n",
        "| Sistema BiolÃ³gico | Rede Neural | FunÃ§Ã£o |\n",
        "|-------------------|-------------|--------|\n",
        "| **AtenÃ§Ã£o seletiva** | Attention weights | Foco em features relevantes |\n",
        "| **Processamento paralelo** | Multi-head attention | MÃºltiplas representaÃ§Ãµes |\n",
        "| **Contexto global** | Self-attention | RelaÃ§Ãµes entre todos os elementos |\n",
        "| **MemÃ³ria de trabalho** | Hidden states | ManutenÃ§Ã£o de informaÃ§Ã£o |\n",
        "\n",
        "### Tipos de AtenÃ§Ã£o\n",
        "\n",
        "![Tipos de AtenÃ§Ã£o](https://github.com/rfapo/visao-computacional/blob/main/images/modulo8/tipos_atencao.png)\n",
        "\n",
        "#### **1. Self-Attention (AtenÃ§Ã£o PrÃ³pria)**\n",
        "\n",
        "**CaracterÃ­sticas:**\n",
        "- **RelaÃ§Ãµes internas**: Cada elemento interage com todos os outros\n",
        "- **Pesos computados**: ImportÃ¢ncia calculada dinamicamente\n",
        "- **Contexto completo**: ConsideraÃ§Ã£o de toda a sequÃªncia\n",
        "- **ParalelizaÃ§Ã£o**: Processamento simultÃ¢neo\n",
        "\n",
        "**FÃ³rmula:**\n",
        "```\n",
        "Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V\n",
        "```\n",
        "\n",
        "**AplicaÃ§Ãµes:**\n",
        "- **Processamento de sequÃªncias**: NLP, time series\n",
        "- **AnÃ¡lise de imagens**: RelaÃ§Ãµes entre pixels\n",
        "- **RecomendaÃ§Ã£o**: RelaÃ§Ãµes entre usuÃ¡rios e itens\n",
        "\n",
        "#### **2. Multi-Head Attention**\n",
        "\n",
        "**CaracterÃ­sticas:**\n",
        "- **MÃºltiplas cabeÃ§as**: Diferentes representaÃ§Ãµes de atenÃ§Ã£o\n",
        "- **Diversidade**: Captura diferentes tipos de relaÃ§Ãµes\n",
        "- **ConcatenaÃ§Ã£o**: CombinaÃ§Ã£o de mÃºltiplas cabeÃ§as\n",
        "- **Flexibilidade**: Maior capacidade expressiva\n",
        "\n",
        "**FÃ³rmula:**\n",
        "```\n",
        "MultiHead(Q,K,V) = Concat(head_1, ..., head_h)W^O\n",
        "head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\n",
        "```\n",
        "\n",
        "**Vantagens:**\n",
        "- âœ… **Diversidade**: Diferentes tipos de atenÃ§Ã£o\n",
        "- âœ… **Capacidade**: Maior poder expressivo\n",
        "- âœ… **Robustez**: Menos sensÃ­vel a falhas\n",
        "- âœ… **Flexibilidade**: AdaptaÃ§Ã£o a diferentes tarefas\n",
        "\n",
        "#### **3. Cross-Attention**\n",
        "\n",
        "**CaracterÃ­sticas:**\n",
        "- **InteraÃ§Ã£o**: Entre sequÃªncias diferentes\n",
        "- **Query**: De uma sequÃªncia\n",
        "- **Key/Value**: De outra sequÃªncia\n",
        "- **AplicaÃ§Ã£o**: TraduÃ§Ã£o, geraÃ§Ã£o, multimodal\n",
        "\n",
        "**FÃ³rmula:**\n",
        "```\n",
        "CrossAttention(Q,K,V) = softmax(QK^T/âˆšd_k)V\n",
        "onde Q vem de uma sequÃªncia e K,V de outra\n",
        "```\n",
        "\n",
        "**AplicaÃ§Ãµes:**\n",
        "- **TraduÃ§Ã£o**: RelaÃ§Ãµes entre idiomas\n",
        "- **GeraÃ§Ã£o**: RelaÃ§Ãµes entre texto e imagem\n",
        "- **Multimodal**: IntegraÃ§Ã£o de diferentes modalidades\n",
        "\n",
        "### EvoluÃ§Ã£o dos Mecanismos de AtenÃ§Ã£o\n",
        "\n",
        "![EvoluÃ§Ã£o Mecanismos AtenÃ§Ã£o](https://github.com/rfapo/visao-computacional/blob/main/images/modulo8/evolucao_mecanismos_atencao.png)\n",
        "\n",
        "#### **ProgressÃ£o HistÃ³rica:**\n",
        "\n",
        "| Ano | Marco | ContribuiÃ§Ã£o |\n",
        "|-----|-------|--------------|\n",
        "| **2014** | Attention mechanism | IntroduÃ§Ã£o para traduÃ§Ã£o |\n",
        "| **2017** | Transformer | Self-attention puro |\n",
        "| **2018** | BERT | AtenÃ§Ã£o bidirecional |\n",
        "| **2020** | Vision Transformer | AtenÃ§Ã£o para imagens |\n",
        "| **2021** | CLIP | AtenÃ§Ã£o multimodal |\n",
        "| **2022** | DALL-E | AtenÃ§Ã£o para geraÃ§Ã£o |\n",
        "\n",
        "#### **Marcos Importantes:**\n",
        "- **2014**: Neural Machine Translation by Jointly Learning to Align and Translate - Bahdanau et al.\n",
        "- **2017**: Attention Is All You Need - Vaswani et al.\n",
        "- **2020**: An Image is Worth 16x16 Words: Transformers for Image Recognition - Dosovitskiy et al.\n",
        "- **2021**: CLIP: Learning Transferable Visual Representations - Radford et al.\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ–¼ï¸ 8.2 Vision Transformers (ViT)\n",
        "\n",
        "### Conceito Fundamental\n",
        "\n",
        "**Vision Transformers** sÃ£o arquiteturas que aplicam o mecanismo de atenÃ§Ã£o dos Transformers para processar imagens, tratando patches de imagem como \"tokens\" em uma sequÃªncia.\n",
        "\n",
        "![Arquitetura Vision Transformer](https://github.com/rfapo/visao-computacional/blob/main/images/modulo8/arquitetura_vision_transformer.png)\n",
        "\n",
        "### Componentes Principais\n",
        "\n",
        "#### **1. Patch Embedding**\n",
        "\n",
        "**FunÃ§Ã£o:**\n",
        "- **DivisÃ£o**: Imagem dividida em patches\n",
        "- **Embedding**: Cada patch convertido em vetor\n",
        "- **LinearizaÃ§Ã£o**: Patches tratados como sequÃªncia\n",
        "- **Dimensionalidade**: ReduÃ§Ã£o para dimensÃ£o fixa\n",
        "\n",
        "**Processo:**\n",
        "```\n",
        "Imagem (HÃ—WÃ—C) â†’ Patches (NÃ—PÂ²Ã—C) â†’ Embedding (NÃ—D)\n",
        "```\n",
        "\n",
        "**ParÃ¢metros:**\n",
        "- **Patch size**: Tamanho de cada patch (ex: 16Ã—16)\n",
        "- **Embedding dimension**: DimensÃ£o do vetor (ex: 768)\n",
        "- **Number of patches**: N = (HÃ—W)/(PÂ²)\n",
        "\n",
        "#### **2. Positional Embedding**\n",
        "\n",
        "**FunÃ§Ã£o:**\n",
        "- **PosiÃ§Ã£o**: InformaÃ§Ã£o sobre localizaÃ§Ã£o dos patches\n",
        "- **Soma**: Adicionada ao patch embedding\n",
        "- **Aprendizado**: ParÃ¢metros aprendÃ­veis\n",
        "- **InvariÃ¢ncia**: Preserva informaÃ§Ã£o espacial\n",
        "\n",
        "**Tipos:**\n",
        "- **1D**: PosiÃ§Ã£o linear na sequÃªncia\n",
        "- **2D**: PosiÃ§Ã£o (x,y) na imagem\n",
        "- **AprendÃ­vel**: ParÃ¢metros otimizÃ¡veis\n",
        "- **Sinusoidal**: FunÃ§Ãµes seno/cosseno\n",
        "\n",
        "#### **3. Transformer Encoder**\n",
        "\n",
        "**Estrutura:**\n",
        "```\n",
        "Multi-Head Self-Attention â†’ Add & Norm â†’ Feed Forward â†’ Add & Norm\n",
        "```\n",
        "\n",
        "**Componentes:**\n",
        "- **Multi-Head Attention**: MÃºltiplas cabeÃ§as de atenÃ§Ã£o\n",
        "- **Feed Forward**: Rede feed-forward\n",
        "- **Add & Norm**: Residual connection + layer normalization\n",
        "- **Layers**: MÃºltiplas camadas empilhadas\n",
        "\n",
        "#### **4. Classification Head**\n",
        "\n",
        "**FunÃ§Ã£o:**\n",
        "- **CLS token**: Token especial para classificaÃ§Ã£o\n",
        "- **Pooling**: AgregaÃ§Ã£o de informaÃ§Ãµes\n",
        "- **Linear**: Camada linear final\n",
        "- **Softmax**: Probabilidades das classes\n",
        "\n",
        "**Processo:**\n",
        "```\n",
        "CLS token â†’ Transformer â†’ CLS output â†’ Linear â†’ Softmax\n",
        "```\n",
        "\n",
        "### Vantagens dos Vision Transformers\n",
        "\n",
        "#### **1. Escalabilidade**\n",
        "- **Dados**: Performance melhora com mais dados\n",
        "- **Modelo**: Arquitetura escalÃ¡vel\n",
        "- **ComputaÃ§Ã£o**: ParalelizaÃ§Ã£o eficiente\n",
        "- **Treinamento**: EstÃ¡vel com grandes datasets\n",
        "\n",
        "#### **2. Interpretabilidade**\n",
        "- **Attention maps**: VisualizaÃ§Ã£o de atenÃ§Ã£o\n",
        "- **Patch importance**: ImportÃ¢ncia de cada patch\n",
        "- **Global context**: Contexto global visÃ­vel\n",
        "- **Debugging**: Facilita debugging\n",
        "\n",
        "#### **3. Flexibilidade**\n",
        "- **Arquitetura**: FÃ¡cil modificaÃ§Ã£o\n",
        "- **Tarefas**: AdaptÃ¡vel a diferentes tarefas\n",
        "- **Modalidades**: ExtensÃ­vel a outras modalidades\n",
        "- **IntegraÃ§Ã£o**: CombinaÃ§Ã£o com outras arquiteturas\n",
        "\n",
        "### LimitaÃ§Ãµes dos Vision Transformers\n",
        "\n",
        "#### **1. Dados**\n",
        "- **Requisito**: Necessita grandes datasets\n",
        "- **Overfitting**: Risco com datasets pequenos\n",
        "- **Transfer learning**: DependÃªncia de modelos prÃ©-treinados\n",
        "- **Custo**: Treinamento caro\n",
        "\n",
        "#### **2. ComputaÃ§Ã£o**\n",
        "- **Complexidade**: O(nÂ²) com nÃºmero de patches\n",
        "- **MemÃ³ria**: Alto uso de memÃ³ria\n",
        "- **InferÃªncia**: Pode ser lenta\n",
        "- **Recursos**: Requer recursos computacionais\n",
        "\n",
        "#### **3. InduÃ§Ã£o**\n",
        "- **Bias**: ViÃ©s para padrÃµes globais\n",
        "- **Locais**: Menos eficiente para padrÃµes locais\n",
        "- **CNNs**: CNNs ainda melhores para algumas tarefas\n",
        "- **HÃ­brido**: CombinaÃ§Ã£o com CNNs pode ser melhor\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ” 8.3 DemonstraÃ§Ã£o PrÃ¡tica: Mecanismos de AtenÃ§Ã£o\n",
        "\n",
        "Vamos implementar e visualizar diferentes tipos de atenÃ§Ã£o:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "class AttentionMechanismsDemo:\n",
        "    \"\"\"DemonstraÃ§Ã£o de diferentes mecanismos de atenÃ§Ã£o\"\"\"\n",
        "    \n",
        "    def __init__(self, seq_len=10, d_model=64):\n",
        "        self.seq_len = seq_len\n",
        "        self.d_model = d_model\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "    def create_sample_data(self):\n",
        "        \"\"\"Cria dados de exemplo para demonstraÃ§Ã£o\"\"\"\n",
        "        \n",
        "        # Criar sequÃªncia de entrada\n",
        "        x = torch.randn(1, self.seq_len, self.d_model)\n",
        "        \n",
        "        # Adicionar padrÃµes especÃ­ficos\n",
        "        # PadrÃ£o 1: Valores altos no meio\n",
        "        x[0, 3:7, :] += 2.0\n",
        "        \n",
        "        # PadrÃ£o 2: Valores baixos no inÃ­cio\n",
        "        x[0, 0:2, :] -= 1.5\n",
        "        \n",
        "        # PadrÃ£o 3: Valores mÃ©dios no final\n",
        "        x[0, 8:, :] += 0.5\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def self_attention(self, x):\n",
        "        \"\"\"Implementa self-attention\"\"\"\n",
        "        \n",
        "        # Linear transformations\n",
        "        W_q = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        W_k = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        W_v = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        \n",
        "        # Compute Q, K, V\n",
        "        Q = W_q(x)\n",
        "        K = W_k(x)\n",
        "        V = W_v(x)\n",
        "        \n",
        "        # Compute attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_model)\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        \n",
        "        # Apply attention to values\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        \n",
        "        return output, attention_weights\n",
        "    \n",
        "    def multi_head_attention(self, x, num_heads=8):\n",
        "        \"\"\"Implementa multi-head attention\"\"\"\n",
        "        \n",
        "        class MultiHeadAttention(nn.Module):\n",
        "            def __init__(self, d_model, num_heads):\n",
        "                super(MultiHeadAttention, self).__init__()\n",
        "                self.d_model = d_model\n",
        "                self.num_heads = num_heads\n",
        "                self.d_k = d_model // num_heads\n",
        "                \n",
        "                self.W_q = nn.Linear(d_model, d_model)\n",
        "                self.W_k = nn.Linear(d_model, d_model)\n",
        "                self.W_v = nn.Linear(d_model, d_model)\n",
        "                self.W_o = nn.Linear(d_model, d_model)\n",
        "                \n",
        "            def forward(self, x):\n",
        "                batch_size, seq_len, d_model = x.size()\n",
        "                \n",
        "                # Linear transformations\n",
        "                Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "                K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "                V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "                \n",
        "                # Compute attention scores\n",
        "                scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
        "                attention_weights = F.softmax(scores, dim=-1)\n",
        "                \n",
        "                # Apply attention to values\n",
        "                attended_values = torch.matmul(attention_weights, V)\n",
        "                \n",
        "                # Concatenate heads\n",
        "                attended_values = attended_values.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
        "                \n",
        "                # Output projection\n",
        "                output = self.W_o(attended_values)\n",
        "                \n",
        "                return output, attention_weights\n",
        "        \n",
        "        mha = MultiHeadAttention(self.d_model, num_heads)\n",
        "        output, attention_weights = mha(x)\n",
        "        \n",
        "        return output, attention_weights\n",
        "    \n",
        "    def cross_attention(self, x1, x2):\n",
        "        \"\"\"Implementa cross-attention\"\"\"\n",
        "        \n",
        "        # Linear transformations\n",
        "        W_q = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        W_k = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        W_v = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        \n",
        "        # Compute Q from x1, K and V from x2\n",
        "        Q = W_q(x1)\n",
        "        K = W_k(x2)\n",
        "        V = W_v(x2)\n",
        "        \n",
        "        # Compute attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_model)\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        \n",
        "        # Apply attention to values\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        \n",
        "        return output, attention_weights\n",
        "    \n",
        "    def visualize_attention_weights(self, attention_weights, title=\"Attention Weights\"):\n",
        "        \"\"\"Visualiza pesos de atenÃ§Ã£o\"\"\"\n",
        "        \n",
        "        # Converter para numpy\n",
        "        if isinstance(attention_weights, torch.Tensor):\n",
        "            attention_weights = attention_weights.detach().numpy()\n",
        "        \n",
        "        # Remover dimensÃµes de batch se necessÃ¡rio\n",
        "        if attention_weights.ndim == 4:  # Multi-head attention\n",
        "            attention_weights = attention_weights[0]  # Primeiro batch\n",
        "        elif attention_weights.ndim == 3:\n",
        "            attention_weights = attention_weights[0]  # Primeiro batch\n",
        "        \n",
        "        # Visualizar\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        \n",
        "        if attention_weights.ndim == 3:  # Multi-head\n",
        "            # Visualizar cada cabeÃ§a\n",
        "            num_heads = attention_weights.shape[0]\n",
        "            cols = min(4, num_heads)\n",
        "            rows = (num_heads + cols - 1) // cols\n",
        "            \n",
        "            for i in range(num_heads):\n",
        "                plt.subplot(rows, cols, i + 1)\n",
        "                sns.heatmap(attention_weights[i], cmap='Blues', cbar=True)\n",
        "                plt.title(f'Head {i + 1}')\n",
        "                plt.xlabel('Key Position')\n",
        "                plt.ylabel('Query Position')\n",
        "        else:  # Single head\n",
        "            sns.heatmap(attention_weights, cmap='Blues', cbar=True)\n",
        "            plt.title(title)\n",
        "            plt.xlabel('Key Position')\n",
        "            plt.ylabel('Query Position')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def demonstrate_attention_mechanisms(self):\n",
        "        \"\"\"Demonstra diferentes mecanismos de atenÃ§Ã£o\"\"\"\n",
        "        \n",
        "        # Criar dados de exemplo\n",
        "        x = self.create_sample_data()\n",
        "        \n",
        "        print(\"=== DEMONSTRAÃ‡ÃƒO: MECANISMOS DE ATENÃ‡ÃƒO ===\")\n",
        "        print(f\"SequÃªncia de entrada: {x.shape}\")\n",
        "        print(f\"DimensÃ£o do modelo: {self.d_model}\")\n",
        "        print(f\"Comprimento da sequÃªncia: {self.seq_len}\")\n",
        "        \n",
        "        # Self-attention\n",
        "        print(\"\\n1. Self-Attention:\")\n",
        "        sa_output, sa_weights = self.self_attention(x)\n",
        "        print(f\"   Output shape: {sa_output.shape}\")\n",
        "        print(f\"   Attention weights shape: {sa_weights.shape}\")\n",
        "        \n",
        "        # Multi-head attention\n",
        "        print(\"\\n2. Multi-Head Attention:\")\n",
        "        mha_output, mha_weights = self.multi_head_attention(x, num_heads=8)\n",
        "        print(f\"   Output shape: {mha_output.shape}\")\n",
        "        print(f\"   Attention weights shape: {mha_weights.shape}\")\n",
        "        \n",
        "        # Cross-attention\n",
        "        print(\"\\n3. Cross-Attention:\")\n",
        "        x2 = torch.randn(1, self.seq_len, self.d_model)  # Segunda sequÃªncia\n",
        "        ca_output, ca_weights = self.cross_attention(x, x2)\n",
        "        print(f\"   Output shape: {ca_output.shape}\")\n",
        "        print(f\"   Attention weights shape: {ca_weights.shape}\")\n",
        "        \n",
        "        # Visualizar pesos de atenÃ§Ã£o\n",
        "        print(\"\\n=== VISUALIZAÃ‡ÃƒO DOS PESOS DE ATENÃ‡ÃƒO ===\")\n",
        "        \n",
        "        # Self-attention\n",
        "        self.visualize_attention_weights(sa_weights, \"Self-Attention Weights\")\n",
        "        \n",
        "        # Multi-head attention\n",
        "        self.visualize_attention_weights(mha_weights, \"Multi-Head Attention Weights\")\n",
        "        \n",
        "        # Cross-attention\n",
        "        self.visualize_attention_weights(ca_weights, \"Cross-Attention Weights\")\n",
        "        \n",
        "        # AnÃ¡lise quantitativa\n",
        "        print(\"\\n=== ANÃLISE QUANTITATIVA ===\")\n",
        "        \n",
        "        # Self-attention\n",
        "        sa_weights_np = sa_weights.detach().numpy()[0]\n",
        "        print(f\"\\nSelf-Attention:\")\n",
        "        print(f\"  - Peso mÃ¡ximo: {np.max(sa_weights_np):.4f}\")\n",
        "        print(f\"  - Peso mÃ­nimo: {np.min(sa_weights_np):.4f}\")\n",
        "        print(f\"  - Peso mÃ©dio: {np.mean(sa_weights_np):.4f}\")\n",
        "        print(f\"  - Entropia: {np.sum(-sa_weights_np * np.log(sa_weights_np + 1e-8)):.4f}\")\n",
        "        \n",
        "        # Multi-head attention\n",
        "        mha_weights_np = mha_weights.detach().numpy()[0]\n",
        "        print(f\"\\nMulti-Head Attention:\")\n",
        "        print(f\"  - Peso mÃ¡ximo: {np.max(mha_weights_np):.4f}\")\n",
        "        print(f\"  - Peso mÃ­nimo: {np.min(mha_weights_np):.4f}\")\n",
        "        print(f\"  - Peso mÃ©dio: {np.mean(mha_weights_np):.4f}\")\n",
        "        print(f\"  - Entropia mÃ©dia: {np.mean([np.sum(-head * np.log(head + 1e-8)) for head in mha_weights_np]):.4f}\")\n",
        "        \n",
        "        # Cross-attention\n",
        "        ca_weights_np = ca_weights.detach().numpy()[0]\n",
        "        print(f\"\\nCross-Attention:\")\n",
        "        print(f\"  - Peso mÃ¡ximo: {np.max(ca_weights_np):.4f}\")\n",
        "        print(f\"  - Peso mÃ­nimo: {np.min(ca_weights_np):.4f}\")\n",
        "        print(f\"  - Peso mÃ©dio: {np.mean(ca_weights_np):.4f}\")\n",
        "        print(f\"  - Entropia: {np.sum(-ca_weights_np * np.log(ca_weights_np + 1e-8)):.4f}\")\n",
        "        \n",
        "        return {\n",
        "            'self_attention': (sa_output, sa_weights),\n",
        "            'multi_head_attention': (mha_output, mha_weights),\n",
        "            'cross_attention': (ca_output, ca_weights)\n",
        "        }\n",
        "\n",
        "# Executar demonstraÃ§Ã£o\n",
        "print(\"=== DEMONSTRAÃ‡ÃƒO: MECANISMOS DE ATENÃ‡ÃƒO ===\")\n",
        "attention_demo = AttentionMechanismsDemo(seq_len=10, d_model=64)\n",
        "results = attention_demo.demonstrate_attention_mechanisms()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AnÃ¡lise dos Resultados\n",
        "\n",
        "**ObservaÃ§Ãµes Importantes:**\n",
        "\n",
        "1. **Self-Attention**:\n",
        "   - **Pesos**: DistribuiÃ§Ã£o de atenÃ§Ã£o entre elementos\n",
        "   - **PadrÃµes**: IdentificaÃ§Ã£o de relaÃ§Ãµes importantes\n",
        "   - **Contexto**: ConsideraÃ§Ã£o de toda a sequÃªncia\n",
        "\n",
        "2. **Multi-Head Attention**:\n",
        "   - **Diversidade**: Diferentes cabeÃ§as capturam diferentes padrÃµes\n",
        "   - **Robustez**: Menos sensÃ­vel a falhas\n",
        "   - **Capacidade**: Maior poder expressivo\n",
        "\n",
        "3. **Cross-Attention**:\n",
        "   - **InteraÃ§Ã£o**: RelaÃ§Ãµes entre sequÃªncias diferentes\n",
        "   - **Flexibilidade**: AdaptaÃ§Ã£o a diferentes modalidades\n",
        "   - **AplicaÃ§Ã£o**: Ãštil para tarefas multimodais\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ–¼ï¸ 8.4 DemonstraÃ§Ã£o PrÃ¡tica: Vision Transformer Simples\n",
        "\n",
        "Vamos implementar e visualizar um Vision Transformer simples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "class SimpleVisionTransformer:\n",
        "    \"\"\"ImplementaÃ§Ã£o de um Vision Transformer simples\"\"\"\n",
        "    \n",
        "    def __init__(self, img_size=32, patch_size=8, num_classes=10, d_model=128, num_heads=8, num_layers=6):\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_classes = num_classes\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "        # Calcular nÃºmero de patches\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        \n",
        "        # Inicializar modelo\n",
        "        self.vit = self._build_vit()\n",
        "        \n",
        "    def _build_vit(self):\n",
        "        \"\"\"ConstrÃ³i o Vision Transformer\"\"\"\n",
        "        \n",
        "        class PatchEmbedding(nn.Module):\n",
        "            def __init__(self, img_size, patch_size, d_model):\n",
        "                super(PatchEmbedding, self).__init__()\n",
        "                self.img_size = img_size\n",
        "                self.patch_size = patch_size\n",
        "                self.num_patches = (img_size // patch_size) ** 2\n",
        "                \n",
        "                # Patch embedding\n",
        "                self.patch_embed = nn.Conv2d(3, d_model, kernel_size=patch_size, stride=patch_size)\n",
        "                \n",
        "                # Positional embedding\n",
        "                self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, d_model))\n",
        "                \n",
        "                # CLS token\n",
        "                self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
        "                \n",
        "            def forward(self, x):\n",
        "                B = x.shape[0]\n",
        "                \n",
        "                # Patch embedding\n",
        "                x = self.patch_embed(x)  # (B, d_model, H/patch_size, W/patch_size)\n",
        "                x = x.flatten(2).transpose(1, 2)  # (B, num_patches, d_model)\n",
        "                \n",
        "                # Adicionar CLS token\n",
        "                cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "                x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches + 1, d_model)\n",
        "                \n",
        "                # Adicionar positional embedding\n",
        "                x = x + self.pos_embed\n",
        "                \n",
        "                return x\n",
        "        \n",
        "        class TransformerBlock(nn.Module):\n",
        "            def __init__(self, d_model, num_heads):\n",
        "                super(TransformerBlock, self).__init__()\n",
        "                \n",
        "                # Multi-head attention\n",
        "                self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
        "                \n",
        "                # Feed forward\n",
        "                self.feed_forward = nn.Sequential(\n",
        "                    nn.Linear(d_model, d_model * 4),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(d_model * 4, d_model)\n",
        "                )\n",
        "                \n",
        "                # Layer normalization\n",
        "                self.norm1 = nn.LayerNorm(d_model)\n",
        "                self.norm2 = nn.LayerNorm(d_model)\n",
        "                \n",
        "            def forward(self, x):\n",
        "                # Self-attention\n",
        "                attn_output, attn_weights = self.attention(x, x, x)\n",
        "                x = x + attn_output\n",
        "                x = self.norm1(x)\n",
        "                \n",
        "                # Feed forward\n",
        "                ff_output = self.feed_forward(x)\n",
        "                x = x + ff_output\n",
        "                x = self.norm2(x)\n",
        "                \n",
        "                return x, attn_weights\n",
        "        \n",
        "        class VisionTransformer(nn.Module):\n",
        "            def __init__(self, img_size, patch_size, num_classes, d_model, num_heads, num_layers):\n",
        "                super(VisionTransformer, self).__init__()\n",
        "                \n",
        "                # Patch embedding\n",
        "                self.patch_embedding = PatchEmbedding(img_size, patch_size, d_model)\n",
        "                \n",
        "                # Transformer blocks\n",
        "                self.transformer_blocks = nn.ModuleList([\n",
        "                    TransformerBlock(d_model, num_heads) for _ in range(num_layers)\n",
        "                ])\n",
        "                \n",
        "                # Classification head\n",
        "                self.classifier = nn.Linear(d_model, num_classes)\n",
        "                \n",
        "            def forward(self, x):\n",
        "                # Patch embedding\n",
        "                x = self.patch_embedding(x)\n",
        "                \n",
        "                # Transformer blocks\n",
        "                attention_weights = []\n",
        "                for transformer_block in self.transformer_blocks:\n",
        "                    x, attn_weights = transformer_block(x)\n",
        "                    attention_weights.append(attn_weights)\n",
        "                \n",
        "                # Classification\n",
        "                cls_output = x[:, 0]  # CLS token\n",
        "                logits = self.classifier(cls_output)\n",
        "                \n",
        "                return logits, attention_weights\n",
        "        \n",
        "        return VisionTransformer(\n",
        "            self.img_size, self.patch_size, self.num_classes,\n",
        "            self.d_model, self.num_heads, self.num_layers\n",
        "        ).to(self.device)\n",
        "    \n",
        "    def create_sample_images(self, num_samples=16):\n",
        "        \"\"\"Cria imagens de exemplo para demonstraÃ§Ã£o\"\"\"\n",
        "        \n",
        "        images = []\n",
        "        \n",
        "        for _ in range(num_samples):\n",
        "            # Criar imagem com padrÃµes simples\n",
        "            img = np.random.rand(3, self.img_size, self.img_size) * 2 - 1  # Normalizar para [-1, 1]\n",
        "            \n",
        "            # Adicionar padrÃµes\n",
        "            if np.random.random() > 0.5:\n",
        "                # PadrÃ£o circular\n",
        "                center_x, center_y = np.random.randint(8, self.img_size-8, 2)\n",
        "                radius = np.random.randint(4, 8)\n",
        "                \n",
        "                y, x = np.ogrid[:self.img_size, :self.img_size]\n",
        "                mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2\n",
        "                \n",
        "                img[0, mask] = 1.0  # Red\n",
        "                img[1, mask] = 0.0  # Green\n",
        "                img[2, mask] = 0.0  # Blue\n",
        "            else:\n",
        "                # PadrÃ£o retangular\n",
        "                x1, y1 = np.random.randint(0, self.img_size-12, 2)\n",
        "                x2, y2 = x1 + np.random.randint(8, 16), y1 + np.random.randint(8, 16)\n",
        "                \n",
        "                img[0, y1:y2, x1:x2] = 0.0  # Red\n",
        "                img[1, y1:y2, x1:x2] = 1.0  # Green\n",
        "                img[2, y1:y2, x1:x2] = 0.0  # Blue\n",
        "            \n",
        "            images.append(img)\n",
        "        \n",
        "        return torch.FloatTensor(images)\n",
        "    \n",
        "    def visualize_patches(self, img, patch_size):\n",
        "        \"\"\"Visualiza patches de uma imagem\"\"\"\n",
        "        \n",
        "        # Converter para numpy\n",
        "        if isinstance(img, torch.Tensor):\n",
        "            img = img.detach().numpy()\n",
        "        \n",
        "        # Normalizar para [0, 1]\n",
        "        img = (img + 1) / 2\n",
        "        img = np.clip(img, 0, 1)\n",
        "        \n",
        "        # Converter para formato de visualizaÃ§Ã£o\n",
        "        img_vis = img.transpose(1, 2, 0)\n",
        "        \n",
        "        # Criar grid de patches\n",
        "        h, w = img_vis.shape[:2]\n",
        "        num_patches_h = h // patch_size\n",
        "        num_patches_w = w // patch_size\n",
        "        \n",
        "        fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "        ax.imshow(img_vis)\n",
        "        \n",
        "        # Desenhar linhas dos patches\n",
        "        for i in range(num_patches_h + 1):\n",
        "            ax.axhline(y=i * patch_size, color='red', linewidth=1)\n",
        "        for j in range(num_patches_w + 1):\n",
        "            ax.axvline(x=j * patch_size, color='red', linewidth=1)\n",
        "        \n",
        "        ax.set_title(f'Imagem com Patches ({patch_size}Ã—{patch_size})')\n",
        "        ax.axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return num_patches_h * num_patches_w\n",
        "    \n",
        "    def visualize_attention_maps(self, attention_weights, img_size, patch_size):\n",
        "        \"\"\"Visualiza mapas de atenÃ§Ã£o\"\"\"\n",
        "        \n",
        "        # Converter para numpy\n",
        "        if isinstance(attention_weights, torch.Tensor):\n",
        "            attention_weights = attention_weights.detach().numpy()\n",
        "        \n",
        "        # Remover dimensÃµes de batch\n",
        "        if attention_weights.ndim == 4:  # Multi-head attention\n",
        "            attention_weights = attention_weights[0]  # Primeiro batch\n",
        "        elif attention_weights.ndim == 3:\n",
        "            attention_weights = attention_weights[0]  # Primeiro batch\n",
        "        \n",
        "        # Calcular nÃºmero de patches\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        \n",
        "        # Visualizar\n",
        "        if attention_weights.ndim == 3:  # Multi-head\n",
        "            num_heads = attention_weights.shape[0]\n",
        "            cols = min(4, num_heads)\n",
        "            rows = (num_heads + cols - 1) // cols\n",
        "            \n",
        "            fig, axes = plt.subplots(rows, cols, figsize=(15, 4 * rows))\n",
        "            if rows == 1:\n",
        "                axes = [axes] if cols == 1 else axes\n",
        "            else:\n",
        "                axes = axes.flatten()\n",
        "            \n",
        "            for i in range(num_heads):\n",
        "                # CLS token attention (primeira linha)\n",
        "                cls_attention = attention_weights[i, 0, 1:]  # Remover CLS token\n",
        "                \n",
        "                # Reshape para imagem\n",
        "                patch_size_h = img_size // patch_size\n",
        "                attention_map = cls_attention.reshape(patch_size_h, patch_size_h)\n",
        "                \n",
        "                # Visualizar\n",
        "                im = axes[i].imshow(attention_map, cmap='hot', interpolation='nearest')\n",
        "                axes[i].set_title(f'Head {i + 1} - CLS Attention')\n",
        "                axes[i].axis('off')\n",
        "                plt.colorbar(im, ax=axes[i])\n",
        "            \n",
        "            # Ocultar eixos extras\n",
        "            for i in range(num_heads, len(axes)):\n",
        "                axes[i].axis('off')\n",
        "        else:  # Single head\n",
        "            # CLS token attention\n",
        "            cls_attention = attention_weights[0, 1:]  # Remover CLS token\n",
        "            \n",
        "            # Reshape para imagem\n",
        "            patch_size_h = img_size // patch_size\n",
        "            attention_map = cls_attention.reshape(patch_size_h, patch_size_h)\n",
        "            \n",
        "            # Visualizar\n",
        "            plt.figure(figsize=(8, 8))\n",
        "            plt.imshow(attention_map, cmap='hot', interpolation='nearest')\n",
        "            plt.title('CLS Token Attention Map')\n",
        "            plt.colorbar()\n",
        "            plt.axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def demonstrate_vit(self):\n",
        "        \"\"\"Demonstra o Vision Transformer\"\"\"\n",
        "        \n",
        "        print(\"=== DEMONSTRAÃ‡ÃƒO: VISION TRANSFORMER ===\")\n",
        "        print(f\"Tamanho da imagem: {self.img_size}Ã—{self.img_size}\")\n",
        "        print(f\"Tamanho do patch: {self.patch_size}Ã—{self.patch_size}\")\n",
        "        print(f\"NÃºmero de patches: {self.num_patches}\")\n",
        "        print(f\"DimensÃ£o do modelo: {self.d_model}\")\n",
        "        print(f\"NÃºmero de cabeÃ§as: {self.num_heads}\")\n",
        "        print(f\"NÃºmero de camadas: {self.num_layers}\")\n",
        "        \n",
        "        # Criar imagens de exemplo\n",
        "        images = self.create_sample_images(16)\n",
        "        \n",
        "        # Visualizar patches\n",
        "        print(\"\\n=== VISUALIZAÃ‡ÃƒO DE PATCHES ===\")\n",
        "        num_patches = self.visualize_patches(images[0], self.patch_size)\n",
        "        print(f\"NÃºmero de patches por imagem: {num_patches}\")\n",
        "        \n",
        "        # Processar com ViT\n",
        "        print(\"\\n=== PROCESSAMENTO COM VIT ===\")\n",
        "        self.vit.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # Processar primeira imagem\n",
        "            img = images[0:1].to(self.device)\n",
        "            logits, attention_weights = self.vit(img)\n",
        "            \n",
        "            print(f\"Logits shape: {logits.shape}\")\n",
        "            print(f\"NÃºmero de camadas de atenÃ§Ã£o: {len(attention_weights)}\")\n",
        "            print(f\"Shape dos pesos de atenÃ§Ã£o: {attention_weights[0].shape}\")\n",
        "            \n",
        "            # PrediÃ§Ã£o\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            predicted_class = torch.argmax(probs, dim=1).item()\n",
        "            confidence = probs[0, predicted_class].item()\n",
        "            \n",
        "            print(f\"Classe predita: {predicted_class}\")\n",
        "            print(f\"ConfianÃ§a: {confidence:.4f}\")\n",
        "        \n",
        "        # Visualizar mapas de atenÃ§Ã£o\n",
        "        print(\"\\n=== VISUALIZAÃ‡ÃƒO DE MAPAS DE ATENÃ‡ÃƒO ===\")\n",
        "        \n",
        "        # Primeira camada\n",
        "        print(\"\\nPrimeira camada:\")\n",
        "        self.visualize_attention_maps(attention_weights[0], self.img_size, self.patch_size)\n",
        "        \n",
        "        # Ãšltima camada\n",
        "        print(\"\\nÃšltima camada:\")\n",
        "        self.visualize_attention_maps(attention_weights[-1], self.img_size, self.patch_size)\n",
        "        \n",
        "        # AnÃ¡lise quantitativa\n",
        "        print(\"\\n=== ANÃLISE QUANTITATIVA ===\")\n",
        "        \n",
        "        # Analisar evoluÃ§Ã£o da atenÃ§Ã£o\n",
        "        first_layer_attention = attention_weights[0].detach().numpy()[0]  # Primeira cabeÃ§a\n",
        "        last_layer_attention = attention_weights[-1].detach().numpy()[0]  # Primeira cabeÃ§a\n",
        "        \n",
        "        print(f\"\\nEvoluÃ§Ã£o da AtenÃ§Ã£o:\")\n",
        "        print(f\"  - Primeira camada - Entropia: {np.sum(-first_layer_attention * np.log(first_layer_attention + 1e-8)):.4f}\")\n",
        "        print(f\"  - Ãšltima camada - Entropia: {np.sum(-last_layer_attention * np.log(last_layer_attention + 1e-8)):.4f}\")\n",
        "        print(f\"  - DiferenÃ§a de entropia: {np.sum(-last_layer_attention * np.log(last_layer_attention + 1e-8)) - np.sum(-first_layer_attention * np.log(first_layer_attention + 1e-8)):.4f}\")\n",
        "        \n",
        "        # Analisar concentraÃ§Ã£o da atenÃ§Ã£o\n",
        "        first_cls_attention = first_layer_attention[0, 1:]  # CLS token\n",
        "        last_cls_attention = last_layer_attention[0, 1:]  # CLS token\n",
        "        \n",
        "        print(f\"\\nConcentraÃ§Ã£o da AtenÃ§Ã£o (CLS Token):\")\n",
        "        print(f\"  - Primeira camada - Max: {np.max(first_cls_attention):.4f}\")\n",
        "        print(f\"  - Primeira camada - Min: {np.min(first_cls_attention):.4f}\")\n",
        "        print(f\"  - Ãšltima camada - Max: {np.max(last_cls_attention):.4f}\")\n",
        "        print(f\"  - Ãšltima camada - Min: {np.min(last_cls_attention):.4f}\")\n",
        "        \n",
        "        return {\n",
        "            'logits': logits,\n",
        "            'attention_weights': attention_weights,\n",
        "            'predicted_class': predicted_class,\n",
        "            'confidence': confidence\n",
        "        }\n",
        "\n",
        "# Executar demonstraÃ§Ã£o\n",
        "print(\"=== DEMONSTRAÃ‡ÃƒO: VISION TRANSFORMER ===\")\n",
        "vit_demo = SimpleVisionTransformer(\n",
        "    img_size=32, patch_size=8, num_classes=10,\n",
        "    d_model=128, num_heads=8, num_layers=6\n",
        ")\n",
        "results = vit_demo.demonstrate_vit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### AnÃ¡lise dos Resultados\n",
        "\n",
        "**ObservaÃ§Ãµes Importantes:**\n",
        "\n",
        "1. **Patches**:\n",
        "   - **DivisÃ£o**: Imagem dividida em patches regulares\n",
        "   - **Embedding**: Cada patch convertido em vetor\n",
        "   - **PosiÃ§Ã£o**: InformaÃ§Ã£o de posiÃ§Ã£o preservada\n",
        "\n",
        "2. **Mapas de AtenÃ§Ã£o**:\n",
        "   - **EvoluÃ§Ã£o**: AtenÃ§Ã£o muda entre camadas\n",
        "   - **ConcentraÃ§Ã£o**: Ãšltima camada mais concentrada\n",
        "   - **Interpretabilidade**: VisualizaÃ§Ã£o de foco\n",
        "\n",
        "3. **ClassificaÃ§Ã£o**:\n",
        "   - **CLS Token**: Agrega informaÃ§Ã£o global\n",
        "   - **ConfianÃ§a**: Medida de certeza da prediÃ§Ã£o\n",
        "   - **Performance**: ViT funciona bem para classificaÃ§Ã£o\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“Š 8.5 ComparaÃ§Ã£o: CNNs vs Vision Transformers\n",
        "\n",
        "### AnÃ¡lise Comparativa\n",
        "\n",
        "![ComparaÃ§Ã£o CNNs vs ViT](https://github.com/rfapo/visao-computacional/blob/main/images/modulo8/comparacao_cnns_vit.png)\n",
        "\n",
        "#### **Arquitetura**\n",
        "\n",
        "| Aspecto | CNNs | Vision Transformers |\n",
        "|---------|------|---------------------|\n",
        "| **InduÃ§Ã£o** | Local | Global |\n",
        "| **ParalelizaÃ§Ã£o** | Limitada | Completa |\n",
        "| **Escalabilidade** | Moderada | Alta |\n",
        "| **Interpretabilidade** | Baixa | Alta |\n",
        "\n",
        "#### **Performance**\n",
        "\n",
        "| Aspecto | CNNs | Vision Transformers |\n",
        "|---------|------|---------------------|\n",
        "| **Dados pequenos** | Boa | Limitada |\n",
        "| **Dados grandes** | Boa | Excelente |\n",
        "| **Treinamento** | EstÃ¡vel | EstÃ¡vel |\n",
        "| **InferÃªncia** | RÃ¡pida | Moderada |\n",
        "\n",
        "#### **AplicaÃ§Ãµes**\n",
        "\n",
        "| Aspecto | CNNs | Vision Transformers |\n",
        "|---------|------|---------------------|\n",
        "| **ClassificaÃ§Ã£o** | Excelente | Excelente |\n",
        "| **DetecÃ§Ã£o** | Excelente | Boa |\n",
        "| **SegmentaÃ§Ã£o** | Excelente | Boa |\n",
        "| **Transfer Learning** | Boa | Excelente |\n",
        "\n",
        "### Quando Usar Cada Um\n",
        "\n",
        "#### **Use CNNs quando:**\n",
        "- âœ… **Dados limitados** estÃ£o disponÃ­veis\n",
        "- âœ… **PadrÃµes locais** sÃ£o importantes\n",
        "- âœ… **Velocidade** Ã© prioritÃ¡ria\n",
        "- âœ… **Recursos limitados** estÃ£o disponÃ­veis\n",
        "\n",
        "#### **Use Vision Transformers quando:**\n",
        "- âœ… **Grandes datasets** estÃ£o disponÃ­veis\n",
        "- âœ… **PadrÃµes globais** sÃ£o importantes\n",
        "- âœ… **Interpretabilidade** Ã© necessÃ¡ria\n",
        "- âœ… **Transfer learning** Ã© prioritÃ¡rio\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“ Resumo do MÃ³dulo 8\n",
        "\n",
        "### Principais Conceitos Abordados\n",
        "\n",
        "1. **Fundamentos**: Mecanismos de atenÃ§Ã£o\n",
        "2. **Tipos**: Self-attention, Multi-head, Cross-attention\n",
        "3. **Vision Transformers**: Arquitetura para imagens\n",
        "4. **ImplementaÃ§Ã£o**: DemonstraÃ§Ãµes prÃ¡ticas\n",
        "5. **ComparaÃ§Ã£o**: CNNs vs Vision Transformers\n",
        "\n",
        "### DemonstraÃ§Ãµes PrÃ¡ticas\n",
        "\n",
        "**1. Mecanismos de AtenÃ§Ã£o:**\n",
        "   - ImplementaÃ§Ã£o de diferentes tipos de atenÃ§Ã£o\n",
        "   - VisualizaÃ§Ã£o de pesos de atenÃ§Ã£o\n",
        "   - AnÃ¡lise quantitativa\n",
        "\n",
        "**2. Vision Transformer:**\n",
        "   - ImplementaÃ§Ã£o de ViT simples\n",
        "   - VisualizaÃ§Ã£o de patches e mapas de atenÃ§Ã£o\n",
        "   - AnÃ¡lise de evoluÃ§Ã£o da atenÃ§Ã£o\n",
        "\n",
        "### PrÃ³ximos Passos\n",
        "\n",
        "No **MÃ³dulo 9**, exploraremos **Foundation Models** para visÃ£o computacional, incluindo CLIP, DALL-E e GPT-4V.\n",
        "\n",
        "### ReferÃªncias Principais\n",
        "\n",
        "- [Attention Is All You Need - Vaswani et al.](https://arxiv.org/abs/1706.03762)\n",
        "- [An Image is Worth 16x16 Words: Transformers for Image Recognition - Dosovitskiy et al.](https://arxiv.org/abs/2010.11929)\n",
        "\n",
        "---\n",
        "\n",
        "**PrÃ³ximo MÃ³dulo**: Foundation Models para VisÃ£o Computacional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ¯ ConexÃ£o com o PrÃ³ximo MÃ³dulo\n",
        "\n",
        "Agora que dominamos **Vision Transformers e Mecanismos de AtenÃ§Ã£o**, estamos preparados para explorar **Foundation Models** para visÃ£o computacional.\n",
        "\n",
        "No **MÃ³dulo 9**, veremos como:\n",
        "\n",
        "### ğŸ”— **ConexÃµes Diretas:**\n",
        "\n",
        "1. **AtenÃ§Ã£o** â†’ **Foundation Models**\n",
        "   - Vision Transformers usam atenÃ§Ã£o\n",
        "   - Foundation Models sÃ£o baseados em Transformers\n",
        "\n",
        "2. **Arquiteturas EscalÃ¡veis** â†’ **Modelos Massivos**\n",
        "   - ViT Ã© escalÃ¡vel\n",
        "   - Foundation Models sÃ£o modelos massivos\n",
        "\n",
        "3. **Transfer Learning** â†’ **Zero-shot Learning**\n",
        "   - ViT Ã© bom para transfer learning\n",
        "   - Foundation Models permitem zero-shot learning\n",
        "\n",
        "4. **Interpretabilidade** â†’ **Capacidades Emergentes**\n",
        "   - ViT Ã© interpretÃ¡vel\n",
        "   - Foundation Models tÃªm capacidades emergentes\n",
        "\n",
        "### ğŸš€ **EvoluÃ§Ã£o Natural:**\n",
        "\n",
        "- **AtenÃ§Ã£o** â†’ **Foundation Models**\n",
        "- **Arquiteturas** â†’ **Modelos Massivos**\n",
        "- **Transfer Learning** â†’ **Zero-shot Learning**\n",
        "- **Interpretabilidade** â†’ **Capacidades Emergentes**\n",
        "\n",
        "Esta transiÃ§Ã£o marca o inÃ­cio da **era dos Foundation Models** em visÃ£o computacional!"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}