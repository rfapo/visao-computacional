{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# M√≥dulo 10: Atividade Final Pr√°tica",
        "",
        "## Objetivos de Aprendizagem",
        "- Aplicar todos os conceitos aprendidos em um projeto completo",
        "- Implementar uma solu√ß√£o de vis√£o computacional do zero",
        "- Integrar diferentes t√©cnicas e abordagens",
        "- Desenvolver habilidades pr√°ticas de implementa√ß√£o",
        "- Apresentar resultados e an√°lises t√©cnicas",
        "",
        "---",
        "",
        "## 10.1 Projeto Final: Sistema de An√°lise de Imagens Multimodal",
        "",
        "### Descri√ß√£o do Projeto",
        "",
        "**Objetivo:** Desenvolver um sistema completo de an√°lise de imagens que integre diferentes t√©cnicas de vis√£o computacional aprendidas ao longo do curso.",
        "",
        "![Descri√ß√£o Projeto Final](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo10/descricao_projeto_final.png)",
        "",
        "**Funcionalidades Principais:**",
        "1. **Classifica√ß√£o de Imagens** usando CNNs tradicionais",
        "2. **Detec√ß√£o de Objetos** com YOLO simplificado",
        "3. **Segmenta√ß√£o de Imagens** usando U-Net",
        "4. **OCR e Extra√ß√£o de Texto** com Tesseract/EasyOCR",
        "5. **An√°lise Multimodal** com Foundation Models (CLIP/GPT-4V)",
        "6. **Interface de Usu√°rio** para intera√ß√£o",
        "",
        "### Arquitetura do Sistema",
        "",
        "**Estrutura Modular:**",
        "",
        "![Arquitetura Sistema](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo10/arquitetura_sistema.png)",
        "",
        "**Componentes:**",
        "- **Input Module**: Entrada de imagens",
        "- **Preprocessing Module**: Pr√©-processamento",
        "- **Analysis Module**: An√°lise multimodal",
        "- **Output Module**: Resultados e visualiza√ß√£o",
        "",
        "### Tecnologias Utilizadas",
        "",
        "**Frameworks:**",
        "- **PyTorch**: Deep learning",
        "- **OpenCV**: Processamento de imagem",
        "- **Transformers**: Modelos pr√©-treinados",
        "- **Streamlit**: Interface web",
        "",
        "**Modelos:**",
        "- **ResNet**: Classifica√ß√£o",
        "- **YOLO**: Detec√ß√£o",
        "- **U-Net**: Segmenta√ß√£o",
        "- **CLIP**: An√°lise multimodal",
        "",
        "![Tecnologias Utilizadas](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo10/tecnologias_utilizadas.png)",
        "",
        "**Refer√™ncias:**",
        "- [Deep Residual Learning for Image Recognition - He et al.](https://arxiv.org/abs/1512.03385)",
        "- [You Only Look Once: Unified, Real-Time Object Detection - Redmon et al.](https://arxiv.org/abs/1506.02640)",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.2 Demonstra√ß√£o Pr√°tica: Sistema Completo\n",
        "\n",
        "Vamos implementar e demonstrar o sistema completo:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "class MultimodalImageAnalysisSystem:\n",
        "    \"\"\"Sistema completo de an√°lise de imagens multimodal\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.results = {}\n",
        "        \n",
        "        # Inicializar m√≥dulos\n",
        "        self.classifier = self._build_classifier()\n",
        "        self.detector = self._build_detector()\n",
        "        self.segmenter = self._build_segmenter()\n",
        "        self.ocr_engine = self._build_ocr_engine()\n",
        "        self.multimodal_analyzer = self._build_multimodal_analyzer()\n",
        "        \n",
        "        # Transforma√ß√µes\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    \n",
        "    def _build_classifier(self):\n",
        "        \"\"\"Constr√≥i o classificador de imagens\"\"\"\n",
        "        \n",
        "        class SimpleCNN(nn.Module):\n",
        "            def __init__(self, num_classes=10):\n",
        "                super(SimpleCNN, self).__init__()\n",
        "                \n",
        "                self.features = nn.Sequential(\n",
        "                    nn.Conv2d(3, 32, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    nn.Conv2d(32, 64, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    nn.Conv2d(64, 128, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    nn.Conv2d(128, 256, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.AdaptiveAvgPool2d((1, 1))\n",
        "                )\n",
        "                \n",
        "                self.classifier = nn.Sequential(\n",
        "                    nn.Dropout(0.5),\n",
        "                    nn.Linear(256, 128),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(0.5),\n",
        "                    nn.Linear(128, num_classes)\n",
        "                )\n",
        "                \n",
        "            def forward(self, x):\n",
        "                x = self.features(x)\n",
        "                x = x.view(x.size(0), -1)\n",
        "                x = self.classifier(x)\n",
        "                return x\n",
        "        \n",
        "        return SimpleCNN(num_classes=10).to(self.device)\n",
        "    \n",
        "    def _build_detector(self):\n",
        "        \"\"\"Constr√≥i o detector de objetos\"\"\"\n",
        "        \n",
        "        class SimpleYOLO(nn.Module):\n",
        "            def __init__(self, num_classes=5):\n",
        "                super(SimpleYOLO, self).__init__()\n",
        "                \n",
        "                self.backbone = nn.Sequential(\n",
        "                    nn.Conv2d(3, 32, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    nn.Conv2d(32, 64, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    nn.Conv2d(64, 128, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2)\n",
        "                )\n",
        "                \n",
        "                # Output: [batch, 5 + num_classes, H, W]\n",
        "                # 5 = [x, y, w, h, confidence]\n",
        "                self.detection_head = nn.Conv2d(128, 5 + num_classes, 1)\n",
        "                \n",
        "            def forward(self, x):\n",
        "                features = self.backbone(x)\n",
        "                detections = self.detection_head(features)\n",
        "                return detections\n",
        "        \n",
        "        return SimpleYOLO(num_classes=5).to(self.device)\n",
        "    \n",
        "    def _build_segmenter(self):\n",
        "        \"\"\"Constr√≥i o segmentador de imagens\"\"\"\n",
        "        \n",
        "        class SimpleUNet(nn.Module):\n",
        "            def __init__(self, num_classes=3):\n",
        "                super(SimpleUNet, self).__init__()\n",
        "                \n",
        "                # Encoder\n",
        "                self.enc1 = nn.Sequential(\n",
        "                    nn.Conv2d(3, 64, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(64, 64, 3, padding=1),\n",
        "                    nn.ReLU()\n",
        "                )\n",
        "                \n",
        "                self.enc2 = nn.Sequential(\n",
        "                    nn.MaxPool2d(2),\n",
        "                    nn.Conv2d(64, 128, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(128, 128, 3, padding=1),\n",
        "                    nn.ReLU()\n",
        "                )\n",
        "                \n",
        "                # Decoder\n",
        "                self.dec1 = nn.Sequential(\n",
        "                    nn.ConvTranspose2d(128, 64, 2, stride=2),\n",
        "                    nn.Conv2d(128, 64, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(64, 64, 3, padding=1),\n",
        "                    nn.ReLU()\n",
        "                )\n",
        "                \n",
        "                self.final = nn.Conv2d(64, num_classes, 1)\n",
        "                \n",
        "            def forward(self, x):\n",
        "                # Encoder\n",
        "                enc1 = self.enc1(x)\n",
        "                enc2 = self.enc2(enc1)\n",
        "                \n",
        "                # Decoder\n",
        "                dec1 = self.dec1(enc2)\n",
        "                \n",
        "                # Skip connection\n",
        "                dec1 = torch.cat([dec1, enc1], dim=1)\n",
        "                \n",
        "                # Final output\n",
        "                output = self.final(dec1)\n",
        "                \n",
        "                return output\n",
        "        \n",
        "        return SimpleUNet(num_classes=3).to(self.device)\n",
        "    \n",
        "    def _build_ocr_engine(self):\n",
        "        \"\"\"Constr√≥i o motor OCR\"\"\"\n",
        "        \n",
        "        class SimpleOCR:\n",
        "            def __init__(self):\n",
        "                self.text_regions = []\n",
        "                \n",
        "            def preprocess_image(self, image):\n",
        "                \"\"\"Pr√©-processa imagem para OCR\"\"\"\n",
        "                # Converter para escala de cinza\n",
        "                if len(image.shape) == 3:\n",
        "                    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "                else:\n",
        "                    gray = image\n",
        "                \n",
        "                # Aplicar threshold\n",
        "                _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "                \n",
        "                # Reduzir ru√≠do\n",
        "                kernel = np.ones((3, 3), np.uint8)\n",
        "                cleaned = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "                \n",
        "                return cleaned\n",
        "            \n",
        "            def detect_text_regions(self, image):\n",
        "                \"\"\"Detecta regi√µes de texto\"\"\"\n",
        "                # Usar contours para detectar regi√µes\n",
        "                contours, _ = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "                \n",
        "                text_regions = []\n",
        "                for contour in contours:\n",
        "                    x, y, w, h = cv2.boundingRect(contour)\n",
        "                    if w > 20 and h > 10:  # Filtrar regi√µes muito pequenas\n",
        "                        text_regions.append((x, y, w, h))\n",
        "                \n",
        "                return text_regions\n",
        "            \n",
        "            def extract_text(self, image):\n",
        "                \"\"\"Extrai texto da imagem\"\"\"\n",
        "                # Simular extra√ß√£o de texto\n",
        "                # Em implementa√ß√£o real, usar Tesseract ou EasyOCR\n",
        "                \n",
        "                preprocessed = self.preprocess_image(image)\n",
        "                text_regions = self.detect_text_regions(preprocessed)\n",
        "                \n",
        "                # Simular texto extra√≠do\n",
        "                extracted_text = f\"Texto detectado em {len(text_regions)} regi√µes\"\n",
        "                \n",
        "                return extracted_text, text_regions\n",
        "        \n",
        "        return SimpleOCR()\n",
        "    \n",
        "    def _build_multimodal_analyzer(self):\n",
        "        \"\"\"Constr√≥i o analisador multimodal\"\"\"\n",
        "        \n",
        "        class MultimodalAnalyzer:\n",
        "            def __init__(self):\n",
        "                self.text_encoder = nn.Sequential(\n",
        "                    nn.Linear(100, 128),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(128, 128)\n",
        "                )\n",
        "                \n",
        "                self.image_encoder = nn.Sequential(\n",
        "                    nn.Linear(224*224*3, 512),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(512, 128)\n",
        "                )\n",
        "                \n",
        "            def analyze_multimodal(self, image, text):\n",
        "                \"\"\"Analisa conte√∫do multimodal\"\"\"\n",
        "                # Simular an√°lise multimodal\n",
        "                \n",
        "                # An√°lise de imagem\n",
        "                image_features = self.image_encoder(image.flatten())\n",
        "                \n",
        "                # An√°lise de texto\n",
        "                text_features = self.text_encoder(torch.randn(100))\n",
        "                \n",
        "                # Similaridade\n",
        "                similarity = F.cosine_similarity(image_features, text_features, dim=0)\n",
        "                \n",
        "                # An√°lise sem√¢ntica\n",
        "                analysis = {\n",
        "                    'similarity': similarity.item(),\n",
        "                    'image_objects': ['objeto1', 'objeto2', 'objeto3'],\n",
        "                    'text_entities': ['entidade1', 'entidade2'],\n",
        "                    'semantic_match': similarity.item() > 0.5\n",
        "                }\n",
        "                \n",
        "                return analysis\n",
        "        \n",
        "        return MultimodalAnalyzer().to(self.device)\n",
        "    \n",
        "    def create_sample_image(self, width=224, height=224):\n",
        "        \"\"\"Cria uma imagem de exemplo para demonstra√ß√£o\"\"\"\n",
        "        \n",
        "        # Criar imagem base\n",
        "        image = np.zeros((height, width, 3), dtype=np.uint8)\n",
        "        \n",
        "        # Adicionar formas geom√©tricas\n",
        "        # C√≠rculo vermelho\n",
        "        cv2.circle(image, (80, 80), 30, (255, 0, 0), -1)\n",
        "        \n",
        "        # Ret√¢ngulo azul\n",
        "        cv2.rectangle(image, (150, 50), (200, 100), (0, 255, 0), -1)\n",
        "        \n",
        "        # Tri√¢ngulo amarelo\n",
        "        pts = np.array([[100, 150], [80, 200], [120, 200]], np.int32)\n",
        "        cv2.fillPoly(image, [pts], (0, 0, 255))\n",
        "        \n",
        "        # Adicionar texto simulado\n",
        "        cv2.putText(image, 'SAMPLE', (50, 220), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
        "        \n",
        "        return image\n",
        "    \n",
        "    def analyze_image(self, image):\n",
        "        \"\"\"Analisa uma imagem usando todos os m√≥dulos\"\"\"\n",
        "        \n",
        "        results = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'image_shape': image.shape,\n",
        "            'analysis': {}\n",
        "        }\n",
        "        \n",
        "        # 1. Classifica√ß√£o\n",
        "        print(\"Executando classifica√ß√£o...\")\n",
        "        classification_result = self._classify_image(image)\n",
        "        results['analysis']['classification'] = classification_result\n",
        "        \n",
        "        # 2. Detec√ß√£o de objetos\n",
        "        print(\"Executando detec√ß√£o de objetos...\")\n",
        "        detection_result = self._detect_objects(image)\n",
        "        results['analysis']['detection'] = detection_result\n",
        "        \n",
        "        # 3. Segmenta√ß√£o\n",
        "        print(\"Executando segmenta√ß√£o...\")\n",
        "        segmentation_result = self._segment_image(image)\n",
        "        results['analysis']['segmentation'] = segmentation_result\n",
        "        \n",
        "        # 4. OCR\n",
        "        print(\"Executando OCR...\")\n",
        "        ocr_result = self._extract_text(image)\n",
        "        results['analysis']['ocr'] = ocr_result\n",
        "        \n",
        "        # 5. An√°lise multimodal\n",
        "        print(\"Executando an√°lise multimodal...\")\n",
        "        multimodal_result = self._analyze_multimodal(image, ocr_result['text'])\n",
        "        results['analysis']['multimodal'] = multimodal_result\n",
        "        \n",
        "        self.results = results\n",
        "        return results\n",
        "    \n",
        "    def _classify_image(self, image):\n",
        "        \"\"\"Classifica a imagem\"\"\"\n",
        "        # Converter para tensor\n",
        "        image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
        "        \n",
        "        # Classificar\n",
        "        self.classifier.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.classifier(image_tensor)\n",
        "            probabilities = F.softmax(outputs, dim=1)\n",
        "            predicted_class = torch.argmax(probabilities, dim=1).item()\n",
        "            confidence = probabilities[0, predicted_class].item()\n",
        "        \n",
        "        return {\n",
        "            'predicted_class': predicted_class,\n",
        "            'confidence': confidence,\n",
        "            'probabilities': probabilities[0].cpu().numpy().tolist()\n",
        "        }\n",
        "    \n",
        "    def _detect_objects(self, image):\n",
        "        \"\"\"Detecta objetos na imagem\"\"\"\n",
        "        # Converter para tensor\n",
        "        image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
        "        \n",
        "        # Detectar\n",
        "        self.detector.eval()\n",
        "        with torch.no_grad():\n",
        "            detections = self.detector(image_tensor)\n",
        "        \n",
        "        # Simular detec√ß√µes\n",
        "        objects = [\n",
        "            {'class': 'circle', 'confidence': 0.95, 'bbox': [50, 50, 60, 60]},\n",
        "            {'class': 'rectangle', 'confidence': 0.87, 'bbox': [140, 40, 60, 60]},\n",
        "            {'class': 'triangle', 'confidence': 0.92, 'bbox': [80, 140, 40, 60]}\n",
        "        ]\n",
        "        \n",
        "        return {\n",
        "            'num_objects': len(objects),\n",
        "            'objects': objects\n",
        "        }\n",
        "    \n",
        "    def _segment_image(self, image):\n",
        "        \"\"\"Segmenta a imagem\"\"\"\n",
        "        # Converter para tensor\n",
        "        image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
        "        \n",
        "        # Segmentar\n",
        "        self.segmenter.eval()\n",
        "        with torch.no_grad():\n",
        "            segmentation = self.segmenter(image_tensor)\n",
        "            segmentation = F.softmax(segmentation, dim=1)\n",
        "            predicted_segments = torch.argmax(segmentation, dim=1)\n",
        "        \n",
        "        return {\n",
        "            'num_segments': 3,\n",
        "            'segmentation_map': predicted_segments[0].cpu().numpy().tolist()\n",
        "        }\n",
        "    \n",
        "    def _extract_text(self, image):\n",
        "        \"\"\"Extrai texto da imagem\"\"\"\n",
        "        text, regions = self.ocr_engine.extract_text(image)\n",
        "        \n",
        "        return {\n",
        "            'text': text,\n",
        "            'num_regions': len(regions),\n",
        "            'regions': regions\n",
        "        }\n",
        "    \n",
        "    def _analyze_multimodal(self, image, text):\n",
        "        \"\"\"Analisa conte√∫do multimodal\"\"\"\n",
        "        # Converter imagem para tensor\n",
        "        image_tensor = torch.FloatTensor(image).flatten().to(self.device)\n",
        "        \n",
        "        # An√°lise multimodal\n",
        "        analysis = self.multimodal_analyzer.analyze_multimodal(image_tensor, text)\n",
        "        \n",
        "        return analysis\n",
        "    \n",
        "    def visualize_results(self, image, results):\n",
        "        \"\"\"Visualiza os resultados da an√°lise\"\"\"\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        \n",
        "        # Imagem original\n",
        "        axes[0, 0].imshow(image)\n",
        "        axes[0, 0].set_title('Imagem Original')\n",
        "        axes[0, 0].axis('off')\n",
        "        \n",
        "        # Classifica√ß√£o\n",
        "        classification = results['analysis']['classification']\n",
        "        axes[0, 1].bar(range(len(classification['probabilities'])), classification['probabilities'])\n",
        "        axes[0, 1].set_title(f'Classifica√ß√£o\\nClasse: {classification[\"predicted_class\"]}, Conf: {classification[\"confidence\"]:.2f}')\n",
        "        axes[0, 1].set_xlabel('Classe')\n",
        "        axes[0, 1].set_ylabel('Probabilidade')\n",
        "        \n",
        "        # Detec√ß√£o de objetos\n",
        "        detection = results['analysis']['detection']\n",
        "        detection_image = image.copy()\n",
        "        for obj in detection['objects']:\n",
        "            x, y, w, h = obj['bbox']\n",
        "            cv2.rectangle(detection_image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "            cv2.putText(detection_image, f\"{obj['class']}: {obj['confidence']:.2f}\", \n",
        "                        (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "        \n",
        "        axes[0, 2].imshow(detection_image)\n",
        "        axes[0, 2].set_title(f'Detec√ß√£o de Objetos\\n{detection[\"num_objects\"]} objetos detectados')\n",
        "        axes[0, 2].axis('off')\n",
        "        \n",
        "        # Segmenta√ß√£o\n",
        "        segmentation = results['analysis']['segmentation']\n",
        "        seg_map = np.array(segmentation['segmentation_map'])\n",
        "        axes[1, 0].imshow(seg_map, cmap='viridis')\n",
        "        axes[1, 0].set_title(f'Segmenta√ß√£o\\n{segmentation[\"num_segments\"]} segmentos')\n",
        "        axes[1, 0].axis('off')\n",
        "        \n",
        "        # OCR\n",
        "        ocr = results['analysis']['ocr']\n",
        "        ocr_image = image.copy()\n",
        "        for region in ocr['regions']:\n",
        "            x, y, w, h = region\n",
        "            cv2.rectangle(ocr_image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "        \n",
        "        axes[1, 1].imshow(ocr_image)\n",
        "        axes[1, 1].set_title(f'OCR\\n{ocr[\"num_regions\"]} regi√µes de texto')\n",
        "        axes[1, 1].axis('off')\n",
        "        \n",
        "        # An√°lise multimodal\n",
        "        multimodal = results['analysis']['multimodal']\n",
        "        axes[1, 2].text(0.1, 0.8, f'Similaridade: {multimodal[\"similarity\"]:.3f}', transform=axes[1, 2].transAxes)\n",
        "        axes[1, 2].text(0.1, 0.6, f'Match Sem√¢ntico: {multimodal[\"semantic_match\"]}', transform=axes[1, 2].transAxes)\n",
        "        axes[1, 2].text(0.1, 0.4, f'Objetos: {len(multimodal[\"image_objects\"])}', transform=axes[1, 2].transAxes)\n",
        "        axes[1, 2].text(0.1, 0.2, f'Entidades: {len(multimodal[\"text_entities\"])}', transform=axes[1, 2].transAxes)\n",
        "        axes[1, 2].set_title('An√°lise Multimodal')\n",
        "        axes[1, 2].axis('off')\n",
        "        \n",
        "        plt.suptitle('Sistema de An√°lise de Imagens Multimodal', fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return fig\n",
        "    \n",
        "    def generate_report(self, results):\n",
        "        \"\"\"Gera relat√≥rio da an√°lise\"\"\"\n",
        "        \n",
        "        report = {\n",
        "            'summary': {\n",
        "                'timestamp': results['timestamp'],\n",
        "                'image_shape': results['image_shape'],\n",
        "                'total_analyses': len(results['analysis'])\n",
        "            },\n",
        "            'classification': {\n",
        "                'predicted_class': results['analysis']['classification']['predicted_class'],\n",
        "                'confidence': results['analysis']['classification']['confidence']\n",
        "            },\n",
        "            'detection': {\n",
        "                'num_objects': results['analysis']['detection']['num_objects'],\n",
        "                'objects': results['analysis']['detection']['objects']\n",
        "            },\n",
        "            'segmentation': {\n",
        "                'num_segments': results['analysis']['segmentation']['num_segments']\n",
        "            },\n",
        "            'ocr': {\n",
        "                'text': results['analysis']['ocr']['text'],\n",
        "                'num_regions': results['analysis']['ocr']['num_regions']\n",
        "            },\n",
        "            'multimodal': {\n",
        "                'similarity': results['analysis']['multimodal']['similarity'],\n",
        "                'semantic_match': results['analysis']['multimodal']['semantic_match']\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        return report\n",
        "\n",
        "def demonstrate_complete_system():\n",
        "    \"\"\"Demonstra o sistema completo\"\"\"\n",
        "    \n",
        "    print(\"=== Sistema de An√°lise de Imagens Multimodal ===\")\n",
        "    print(\"\\nNota: Esta demonstra√ß√£o usa modelos simplificados para fins educacionais.\")\n",
        "    print(\"Em aplica√ß√µes reais, use modelos pr√©-treinados e otimizados.\")\n",
        "    \n",
        "    # Criar inst√¢ncia do sistema\n",
        "    system = MultimodalImageAnalysisSystem()\n",
        "    \n",
        "    # Criar imagem de exemplo\n",
        "    print(\"\\nCriando imagem de exemplo...\")\n",
        "    sample_image = system.create_sample_image()\n",
        "    \n",
        "    # Analisar imagem\n",
        "    print(\"\\nAnalisando imagem...\")\n",
        "    results = system.analyze_image(sample_image)\n",
        "    \n",
        "    # Visualizar resultados\n",
        "    print(\"\\nVisualizando resultados...\")\n",
        "    fig = system.visualize_results(sample_image, results)\n",
        "    \n",
        "    # Gerar relat√≥rio\n",
        "    print(\"\\nGerando relat√≥rio...\")\n",
        "    report = system.generate_report(results)\n",
        "    \n",
        "    # Exibir resumo\n",
        "    print(\"\\n=== RESUMO DA AN√ÅLISE ===\")\n",
        "    print(f\"Timestamp: {report['summary']['timestamp']}\")\n",
        "    print(f\"Formato da imagem: {report['summary']['image_shape']}\")\n",
        "    print(f\"An√°lises realizadas: {report['summary']['total_analyses']}\")\n",
        "    print(f\"\\nClassifica√ß√£o: Classe {report['classification']['predicted_class']} (Conf: {report['classification']['confidence']:.2f})\")\n",
        "    print(f\"Detec√ß√£o: {report['detection']['num_objects']} objetos\")\n",
        "    print(f\"Segmenta√ß√£o: {report['segmentation']['num_segments']} segmentos\")\n",
        "    print(f\"OCR: {report['ocr']['num_regions']} regi√µes de texto\")\n",
        "    print(f\"Multimodal: Similaridade {report['multimodal']['similarity']:.3f}\")\n",
        "    \n",
        "    return system, sample_image, results, report\n",
        "\n",
        "# Executar demonstra√ß√£o\n",
        "complete_system, demo_image, analysis_results, final_report = demonstrate_complete_system()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### An√°lise dos Resultados\n",
        "\n",
        "**Sistema Completo Observado:**\n",
        "\n",
        "1. **Classifica√ß√£o**: CNN para classifica√ß√£o de imagens\n",
        "2. **Detec√ß√£o**: YOLO simplificado para detec√ß√£o de objetos\n",
        "3. **Segmenta√ß√£o**: U-Net para segmenta√ß√£o sem√¢ntica\n",
        "4. **OCR**: Extra√ß√£o de texto com detec√ß√£o de regi√µes\n",
        "5. **Multimodal**: An√°lise combinada de imagem e texto\n",
        "6. **Visualiza√ß√£o**: Resultados integrados em dashboard\n",
        "\n",
        "**Insights Importantes:**\n",
        "- **Integra√ß√£o**: M√∫ltiplas t√©cnicas trabalhando juntas\n",
        "- **Pipeline**: Fluxo completo de an√°lise\n",
        "- **Visualiza√ß√£o**: Resultados claros e interpret√°veis\n",
        "- **Relat√≥rio**: Documenta√ß√£o completa da an√°lise\n",
        "\n",
        "**Refer√™ncias:**\n",
        "- [Deep Residual Learning for Image Recognition - He et al.](https://arxiv.org/abs/1512.03385)\n",
        "- [You Only Look Once: Unified, Real-Time Object Detection - Redmon et al.](https://arxiv.org/abs/1506.02640)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.3 Implementa√ß√£o de Interface Web\n",
        "\n",
        "### Streamlit Dashboard\n",
        "\n",
        "Vamos criar uma interface web para o sistema:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from io import BytesIO\n",
        "import base64\n",
        "\n",
        "class WebInterface:\n",
        "    \"\"\"Interface web para o sistema de an√°lise\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.system = MultimodalImageAnalysisSystem()\n",
        "        \n",
        "    def create_dashboard(self):\n",
        "        \"\"\"Cria o dashboard principal\"\"\"\n",
        "        \n",
        "        st.set_page_config(\n",
        "            page_title=\"Sistema de An√°lise de Imagens Multimodal\",\n",
        "            page_icon=\"üîç\",\n",
        "            layout=\"wide\"\n",
        "        )\n",
        "        \n",
        "        st.title(\"üîç Sistema de An√°lise de Imagens Multimodal\")\n",
        "        st.markdown(\"---\")\n",
        "        \n",
        "        # Sidebar\n",
        "        st.sidebar.title(\"Configura√ß√µes\")\n",
        "        \n",
        "        # Upload de imagem\n",
        "        uploaded_file = st.sidebar.file_uploader(\n",
        "            \"Upload de Imagem\",\n",
        "            type=['png', 'jpg', 'jpeg'],\n",
        "            help=\"Fa√ßa upload de uma imagem para an√°lise\"\n",
        "        )\n",
        "        \n",
        "        # Op√ß√µes de an√°lise\n",
        "        st.sidebar.subheader(\"M√≥dulos de An√°lise\")\n",
        "        enable_classification = st.sidebar.checkbox(\"Classifica√ß√£o\", value=True)\n",
        "        enable_detection = st.sidebar.checkbox(\"Detec√ß√£o de Objetos\", value=True)\n",
        "        enable_segmentation = st.sidebar.checkbox(\"Segmenta√ß√£o\", value=True)\n",
        "        enable_ocr = st.sidebar.checkbox(\"OCR\", value=True)\n",
        "        enable_multimodal = st.sidebar.checkbox(\"An√°lise Multimodal\", value=True)\n",
        "        \n",
        "        # Bot√£o de an√°lise\n",
        "        if st.sidebar.button(\"Analisar Imagem\", type=\"primary\"):\n",
        "            if uploaded_file is not None:\n",
        "                self._analyze_uploaded_image(uploaded_file, {\n",
        "                    'classification': enable_classification,\n",
        "                    'detection': enable_detection,\n",
        "                    'segmentation': enable_segmentation,\n",
        "                    'ocr': enable_ocr,\n",
        "                    'multimodal': enable_multimodal\n",
        "                })\n",
        "            else:\n",
        "                st.error(\"Por favor, fa√ßa upload de uma imagem primeiro.\")\n",
        "        \n",
        "        # Demo com imagem de exemplo\n",
        "        if st.sidebar.button(\"Usar Imagem de Exemplo\"):\n",
        "            self._demo_with_sample_image()\n",
        "        \n",
        "        # Exibir imagem atual\n",
        "        if 'current_image' in st.session_state:\n",
        "            st.subheader(\"Imagem Atual\")\n",
        "            st.image(st.session_state['current_image'], caption=\"Imagem para an√°lise\", use_column_width=True)\n",
        "        \n",
        "        # Exibir resultados\n",
        "        if 'analysis_results' in st.session_state:\n",
        "            self._display_results(st.session_state['analysis_results'])\n",
        "        \n",
        "        # Estat√≠sticas do sistema\n",
        "        self._display_system_stats()\n",
        "    \n",
        "    def _analyze_uploaded_image(self, uploaded_file, options):\n",
        "        \"\"\"Analisa imagem enviada\"\"\"\n",
        "        \n",
        "        # Converter para numpy array\n",
        "        image = np.array(bytearray(uploaded_file.read()), dtype=np.uint8)\n",
        "        image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # Redimensionar se necess√°rio\n",
        "        if image.shape[0] > 512 or image.shape[1] > 512:\n",
        "            image = cv2.resize(image, (512, 512))\n",
        "        \n",
        "        # Armazenar imagem\n",
        "        st.session_state['current_image'] = image\n",
        "        \n",
        "        # Analisar\n",
        "        with st.spinner('Analisando imagem...'):\n",
        "            results = self.system.analyze_image(image)\n",
        "            st.session_state['analysis_results'] = results\n",
        "        \n",
        "        st.success(\"An√°lise conclu√≠da!\")\n",
        "    \n",
        "    def _demo_with_sample_image(self):\n",
        "        \"\"\"Demonstra com imagem de exemplo\"\"\"\n",
        "        \n",
        "        # Criar imagem de exemplo\n",
        "        sample_image = self.system.create_sample_image()\n",
        "        \n",
        "        # Armazenar imagem\n",
        "        st.session_state['current_image'] = sample_image\n",
        "        \n",
        "        # Analisar\n",
        "        with st.spinner('Analisando imagem de exemplo...'):\n",
        "            results = self.system.analyze_image(sample_image)\n",
        "            st.session_state['analysis_results'] = results\n",
        "        \n",
        "        st.success(\"An√°lise da imagem de exemplo conclu√≠da!\")\n",
        "    \n",
        "    def _display_results(self, results):\n",
        "        \"\"\"Exibe os resultados da an√°lise\"\"\"\n",
        "        \n",
        "        st.subheader(\"üìä Resultados da An√°lise\")\n",
        "        \n",
        "        # M√©tricas gerais\n",
        "        col1, col2, col3, col4 = st.columns(4)\n",
        "        \n",
        "        with col1:\n",
        "            st.metric(\n",
        "                \"Classifica√ß√£o\",\n",
        "                f\"Classe {results['analysis']['classification']['predicted_class']}\",\n",
        "                f\"{results['analysis']['classification']['confidence']:.2f}\"\n",
        "            )\n",
        "        \n",
        "        with col2:\n",
        "            st.metric(\n",
        "                \"Objetos Detectados\",\n",
        "                results['analysis']['detection']['num_objects'],\n",
        "                \"objetos\"\n",
        "            )\n",
        "        \n",
        "        with col3:\n",
        "            st.metric(\n",
        "                \"Segmentos\",\n",
        "                results['analysis']['segmentation']['num_segments'],\n",
        "                \"segmentos\"\n",
        "            )\n",
        "        \n",
        "        with col4:\n",
        "            st.metric(\n",
        "                \"Regi√µes OCR\",\n",
        "                results['analysis']['ocr']['num_regions'],\n",
        "                \"regi√µes\"\n",
        "            )\n",
        "        \n",
        "        # Tabs para diferentes an√°lises\n",
        "        tab1, tab2, tab3, tab4, tab5 = st.tabs([\n",
        "            \"üéØ Classifica√ß√£o\",\n",
        "            \"üîç Detec√ß√£o\",\n",
        "            \"‚úÇÔ∏è Segmenta√ß√£o\",\n",
        "            \"üìù OCR\",\n",
        "            \"üîó Multimodal\"\n",
        "        ])\n",
        "        \n",
        "        with tab1:\n",
        "            self._display_classification_results(results['analysis']['classification'])\n",
        "        \n",
        "        with tab2:\n",
        "            self._display_detection_results(results['analysis']['detection'])\n",
        "        \n",
        "        with tab3:\n",
        "            self._display_segmentation_results(results['analysis']['segmentation'])\n",
        "        \n",
        "        with tab4:\n",
        "            self._display_ocr_results(results['analysis']['ocr'])\n",
        "        \n",
        "        with tab5:\n",
        "            self._display_multimodal_results(results['analysis']['multimodal'])\n",
        "    \n",
        "    def _display_classification_results(self, classification):\n",
        "        \"\"\"Exibe resultados de classifica√ß√£o\"\"\"\n",
        "        \n",
        "        # Gr√°fico de barras das probabilidades\n",
        "        fig = px.bar(\n",
        "            x=list(range(len(classification['probabilities']))),\n",
        "            y=classification['probabilities'],\n",
        "            title=\"Probabilidades de Classifica√ß√£o\",\n",
        "            labels={'x': 'Classe', 'y': 'Probabilidade'}\n",
        "        )\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "        \n",
        "        # Informa√ß√µes detalhadas\n",
        "        st.write(f\"**Classe Predita:** {classification['predicted_class']}\")\n",
        "        st.write(f\"**Confian√ßa:** {classification['confidence']:.3f}\")\n",
        "    \n",
        "    def _display_detection_results(self, detection):\n",
        "        \"\"\"Exibe resultados de detec√ß√£o\"\"\"\n",
        "        \n",
        "        # Tabela de objetos\n",
        "        if detection['objects']:\n",
        "            df = pd.DataFrame(detection['objects'])\n",
        "            st.dataframe(df, use_container_width=True)\n",
        "        \n",
        "        # Gr√°fico de confian√ßa\n",
        "        if detection['objects']:\n",
        "            confidences = [obj['confidence'] for obj in detection['objects']]\n",
        "            classes = [obj['class'] for obj in detection['objects']]\n",
        "            \n",
        "            fig = px.bar(\n",
        "                x=classes,\n",
        "                y=confidences,\n",
        "                title=\"Confian√ßa por Classe\",\n",
        "                labels={'x': 'Classe', 'y': 'Confian√ßa'}\n",
        "            )\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "    \n",
        "    def _display_segmentation_results(self, segmentation):\n",
        "        \"\"\"Exibe resultados de segmenta√ß√£o\"\"\"\n",
        "        \n",
        "        st.write(f\"**N√∫mero de Segmentos:** {segmentation['num_segments']}\")\n",
        "        \n",
        "        # Visualizar mapa de segmenta√ß√£o\n",
        "        seg_map = np.array(segmentation['segmentation_map'])\n",
        "        \n",
        "        fig = px.imshow(\n",
        "            seg_map,\n",
        "            title=\"Mapa de Segmenta√ß√£o\",\n",
        "            color_continuous_scale='viridis'\n",
        "        )\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "    \n",
        "    def _display_ocr_results(self, ocr):\n",
        "        \"\"\"Exibe resultados de OCR\"\"\"\n",
        "        \n",
        "        st.write(f\"**Texto Extra√≠do:** {ocr['text']}\")\n",
        "        st.write(f\"**N√∫mero de Regi√µes:** {ocr['num_regions']}\")\n",
        "        \n",
        "        # Tabela de regi√µes\n",
        "        if ocr['regions']:\n",
        "            df = pd.DataFrame(ocr['regions'], columns=['X', 'Y', 'Width', 'Height'])\n",
        "            st.dataframe(df, use_container_width=True)\n",
        "    \n",
        "    def _display_multimodal_results(self, multimodal):\n",
        "        \"\"\"Exibe resultados multimodais\"\"\"\n",
        "        \n",
        "        # M√©tricas\n",
        "        col1, col2 = st.columns(2)\n",
        "        \n",
        "        with col1:\n",
        "            st.metric(\n",
        "                \"Similaridade\",\n",
        "                f\"{multimodal['similarity']:.3f}\"\n",
        "            )\n",
        "        \n",
        "        with col2:\n",
        "            st.metric(\n",
        "                \"Match Sem√¢ntico\",\n",
        "                \"Sim\" if multimodal['semantic_match'] else \"N√£o\"\n",
        "            )\n",
        "        \n",
        "        # Lista de objetos e entidades\n",
        "        col1, col2 = st.columns(2)\n",
        "        \n",
        "        with col1:\n",
        "            st.write(\"**Objetos Detectados:**\")\n",
        "            for obj in multimodal['image_objects']:\n",
        "                st.write(f\"- {obj}\")\n",
        "        \n",
        "        with col2:\n",
        "            st.write(\"**Entidades de Texto:**\")\n",
        "            for entity in multimodal['text_entities']:\n",
        "                st.write(f\"- {entity}\")\n",
        "    \n",
        "    def _display_system_stats(self):\n",
        "        \"\"\"Exibe estat√≠sticas do sistema\"\"\"\n",
        "        \n",
        "        st.subheader(\"üìà Estat√≠sticas do Sistema\")\n",
        "        \n",
        "        col1, col2, col3 = st.columns(3)\n",
        "        \n",
        "        with col1:\n",
        "            st.metric(\"M√≥dulos Ativos\", \"5\", \"m√≥dulos\")\n",
        "        \n",
        "        with col2:\n",
        "            st.metric(\"An√°lises Realizadas\", \"1\", \"an√°lise\")\n",
        "        \n",
        "        with col3:\n",
        "            st.metric(\"Tempo M√©dio\", \"< 1s\", \"por an√°lise\")\n",
        "    \n",
        "    def run_app(self):\n",
        "        \"\"\"Executa a aplica√ß√£o\"\"\"\n",
        "        \n",
        "        self.create_dashboard()\n",
        "\n",
        "def create_streamlit_app():\n",
        "    \"\"\"Cria a aplica√ß√£o Streamlit\"\"\"\n",
        "    \n",
        "    print(\"=== Criando Interface Web com Streamlit ===\")\n",
        "    print(\"\\nPara executar a aplica√ß√£o, use o comando:\")\n",
        "    print(\"streamlit run app.py\")\n",
        "    \n",
        "    # Criar arquivo da aplica√ß√£o\n",
        "    app_code = '''\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from multimodal_system import MultimodalImageAnalysisSystem\n",
        "\n",
        "def main():\n",
        "    st.set_page_config(\n",
        "        page_title=\"Sistema de An√°lise de Imagens Multimodal\",\n",
        "        page_icon=\"üîç\",\n",
        "        layout=\"wide\"\n",
        "    )\n",
        "    \n",
        "    st.title(\"üîç Sistema de An√°lise de Imagens Multimodal\")\n",
        "    \n",
        "    # Inicializar sistema\n",
        "    if 'system' not in st.session_state:\n",
        "        st.session_state.system = MultimodalImageAnalysisSystem()\n",
        "    \n",
        "    # Upload de imagem\n",
        "    uploaded_file = st.file_uploader(\n",
        "        \"Fa√ßa upload de uma imagem\",\n",
        "        type=['png', 'jpg', 'jpeg']\n",
        "    )\n",
        "    \n",
        "    if uploaded_file is not None:\n",
        "        # Processar imagem\n",
        "        image = np.array(bytearray(uploaded_file.read()), dtype=np.uint8)\n",
        "        image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # Exibir imagem\n",
        "        st.image(image, caption=\"Imagem enviada\", use_column_width=True)\n",
        "        \n",
        "        # Analisar\n",
        "        if st.button(\"Analisar Imagem\"):\n",
        "            with st.spinner('Analisando...'):\n",
        "                results = st.session_state.system.analyze_image(image)\n",
        "            \n",
        "            # Exibir resultados\n",
        "            st.success(\"An√°lise conclu√≠da!\")\n",
        "            \n",
        "            # M√©tricas\n",
        "            col1, col2, col3, col4 = st.columns(4)\n",
        "            \n",
        "            with col1:\n",
        "                st.metric(\n",
        "                    \"Classifica√ß√£o\",\n",
        "                    f\"Classe {results['analysis']['classification']['predicted_class']}\",\n",
        "                    f\"{results['analysis']['classification']['confidence']:.2f}\"\n",
        "                )\n",
        "            \n",
        "            with col2:\n",
        "                st.metric(\n",
        "                    \"Objetos\",\n",
        "                    results['analysis']['detection']['num_objects']\n",
        "                )\n",
        "            \n",
        "            with col3:\n",
        "                st.metric(\n",
        "                    \"Segmentos\",\n",
        "                    results['analysis']['segmentation']['num_segments']\n",
        "                )\n",
        "            \n",
        "            with col4:\n",
        "                st.metric(\n",
        "                    \"Regi√µes OCR\",\n",
        "                    results['analysis']['ocr']['num_regions']\n",
        "                )\n",
        "    \n",
        "    # Demo com imagem de exemplo\n",
        "    if st.button(\"Usar Imagem de Exemplo\"):\n",
        "        sample_image = st.session_state.system.create_sample_image()\n",
        "        st.image(sample_image, caption=\"Imagem de exemplo\", use_column_width=True)\n",
        "        \n",
        "        with st.spinner('Analisando imagem de exemplo...'):\n",
        "            results = st.session_state.system.analyze_image(sample_image)\n",
        "        \n",
        "        st.success(\"An√°lise da imagem de exemplo conclu√≠da!\")\n",
        "        \n",
        "        # Exibir resultados\n",
        "        st.json(results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    '''\n",
        "    \n",
        "    with open('app.py', 'w') as f:\n",
        "        f.write(app_code)\n",
        "    \n",
        "    print(\"\\nArquivo 'app.py' criado com sucesso!\")\n",
        "    print(\"\\nPara executar a aplica√ß√£o:\")\n",
        "    print(\"1. Instale o Streamlit: pip install streamlit\")\n",
        "    print(\"2. Execute: streamlit run app.py\")\n",
        "    print(\"3. Acesse: http://localhost:8501\")\n",
        "    \n",
        "    return app_code\n",
        "\n",
        "# Criar aplica√ß√£o\n",
        "app_code = create_streamlit_app()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interface Web Implementada",
        "",
        "**Funcionalidades da Interface:**",
        "",
        "1. **Upload de Imagens**: Interface para envio de imagens",
        "2. **An√°lise Interativa**: Bot√µes para executar an√°lises",
        "3. **Visualiza√ß√£o**: Resultados em tempo real",
        "4. **Dashboard**: M√©tricas e gr√°ficos interativos",
        "5. **Relat√≥rios**: Exporta√ß√£o de resultados",
        "",
        "![Interface Web](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo10/interface_web.png)",
        "",
        "**Tecnologias Utilizadas:**",
        "- **Streamlit**: Framework web",
        "- **Plotly**: Gr√°ficos interativos",
        "- **Pandas**: Manipula√ß√£o de dados",
        "- **OpenCV**: Processamento de imagem",
        "",
        "**Refer√™ncias:**",
        "- [Streamlit Documentation](https://docs.streamlit.io/)",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.4 Casos de Uso e Aplica√ß√µes",
        "",
        "### Aplica√ß√µes Pr√°ticas",
        "",
        "**1. E-commerce:**",
        "- **Busca visual**: Busca por produtos usando imagens",
        "- **Categoriza√ß√£o**: Categoriza√ß√£o autom√°tica de produtos",
        "- **Modera√ß√£o**: Modera√ß√£o de conte√∫do visual",
        "- **Recomenda√ß√µes**: Recomenda√ß√µes baseadas em visual",
        "",
        "**2. Sa√∫de:**",
        "- **Diagn√≥stico**: Assist√™ncia em diagn√≥stico m√©dico",
        "- **An√°lise**: An√°lise de imagens m√©dicas",
        "- **Documenta√ß√£o**: Documenta√ß√£o autom√°tica",
        "- **Educa√ß√£o**: Educa√ß√£o m√©dica",
        "",
        "**3. Educa√ß√£o:**",
        "- **Acessibilidade**: Acessibilidade visual",
        "- **Tutoria**: Tutoria multimodal",
        "- **Conte√∫do**: Cria√ß√£o de conte√∫do educacional",
        "- **Avalia√ß√£o**: Avalia√ß√£o autom√°tica",
        "",
        "**4. Entretenimento:**",
        "- **Gera√ß√£o**: Gera√ß√£o de conte√∫do visual",
        "- **Edi√ß√£o**: Edi√ß√£o autom√°tica",
        "- **Personaliza√ß√£o**: Personaliza√ß√£o de conte√∫do",
        "- **Intera√ß√£o**: Intera√ß√£o multimodal",
        "",
        "![Casos de Uso](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo10/casos_uso.png)",
        "",
        "### Vantagens do Sistema",
        "",
        "**1. Integra√ß√£o:**",
        "- **M√∫ltiplas t√©cnicas**: Combina√ß√£o de diferentes abordagens",
        "- **Pipeline completo**: Fluxo de an√°lise end-to-end",
        "- **Flexibilidade**: Adapt√°vel a diferentes casos de uso",
        "- **Escalabilidade**: Arquitetura modular",
        "",
        "**2. Performance:**",
        "- **Efici√™ncia**: Otimiza√ß√£o para diferentes tarefas",
        "- **Precis√£o**: Alta precis√£o em m√∫ltiplas modalidades",
        "- **Velocidade**: Processamento em tempo real",
        "- **Robustez**: Funciona em diferentes condi√ß√µes",
        "",
        "**3. Usabilidade:**",
        "- **Interface intuitiva**: F√°cil de usar",
        "- **Visualiza√ß√£o clara**: Resultados interpret√°veis",
        "- **Relat√≥rios**: Documenta√ß√£o autom√°tica",
        "- **Integra√ß√£o**: F√°cil integra√ß√£o em sistemas existentes",
        "",
        "### Limita√ß√µes e Desafios",
        "",
        "**1. T√©cnicas:**",
        "- **Recursos computacionais**: Requer recursos significativos",
        "- **Dados**: Depende de dados de qualidade",
        "- **Complexidade**: Arquitetura complexa",
        "- **Manuten√ß√£o**: Requer manuten√ß√£o cont√≠nua",
        "",
        "**2. Pr√°ticas:**",
        "- **Deploy**: Desafios de deploy em produ√ß√£o",
        "- **Escalabilidade**: Escalabilidade horizontal",
        "- **Monitoramento**: Monitoramento de performance",
        "- **Atualiza√ß√£o**: Atualiza√ß√£o de modelos",
        "",
        "**Refer√™ncias:**",
        "- [Deep Learning for Computer Vision - Goodfellow et al.](https://www.deeplearningbook.org/)",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumo do M√≥dulo 10\n",
        "\n",
        "### Principais Conceitos Abordados\n",
        "\n",
        "1. **Sistema Completo**\n",
        "   - Arquitetura modular\n",
        "   - Integra√ß√£o de t√©cnicas\n",
        "   - Pipeline de an√°lise\n",
        "   - Visualiza√ß√£o de resultados\n",
        "\n",
        "2. **Implementa√ß√£o Pr√°tica**\n",
        "   - Classifica√ß√£o com CNN\n",
        "   - Detec√ß√£o com YOLO\n",
        "   - Segmenta√ß√£o com U-Net\n",
        "   - OCR com Tesseract\n",
        "   - An√°lise multimodal\n",
        "\n",
        "3. **Interface Web**\n",
        "   - Dashboard interativo\n",
        "   - Upload de imagens\n",
        "   - Visualiza√ß√£o em tempo real\n",
        "   - Relat√≥rios autom√°ticos\n",
        "\n",
        "4. **Aplica√ß√µes Pr√°ticas**\n",
        "   - Casos de uso reais\n",
        "   - Vantagens e limita√ß√µes\n",
        "   - Desafios de implementa√ß√£o\n",
        "   - Futuro da tecnologia\n",
        "\n",
        "### Demonstra√ß√µes Pr√°ticas\n",
        "\n",
        "**1. Sistema Completo:**\n",
        "   - An√°lise multimodal\n",
        "   - Visualiza√ß√£o integrada\n",
        "   - Relat√≥rios autom√°ticos\n",
        "   - M√©tricas de performance\n",
        "\n",
        "**2. Interface Web:**\n",
        "   - Dashboard Streamlit\n",
        "   - Upload interativo\n",
        "   - Visualiza√ß√£o em tempo real\n",
        "   - Exporta√ß√£o de resultados\n",
        "\n",
        "### Conclus√£o do Curso\n",
        "\n",
        "**Conceitos Aprendidos:**\n",
        "- **Fundamentos**: Processamento digital de imagem\n",
        "- **Deep Learning**: CNNs e arquiteturas modernas\n",
        "- **T√©cnicas Avan√ßadas**: Transfer learning, GANs, VAEs\n",
        "- **Transformers**: Vision Transformers e aten√ß√£o\n",
        "- **Foundation Models**: CLIP, DALL-E, GPT-4V\n",
        "- **Aplica√ß√µes**: Casos de uso pr√°ticos\n",
        "\n",
        "**Habilidades Desenvolvidas:**\n",
        "- **Implementa√ß√£o**: C√≥digo pr√°tico e funcional\n",
        "- **An√°lise**: Interpreta√ß√£o de resultados\n",
        "- **Integra√ß√£o**: Combina√ß√£o de t√©cnicas\n",
        "- **Visualiza√ß√£o**: Apresenta√ß√£o de dados\n",
        "- **Deploy**: Implementa√ß√£o em produ√ß√£o\n",
        "\n",
        "### Pr√≥ximos Passos\n",
        "\n",
        "**Para Continuar Aprendendo:**\n",
        "1. **Projetos Pr√°ticos**: Implementar projetos reais\n",
        "2. **Datasets**: Trabalhar com datasets p√∫blicos\n",
        "3. **Competi√ß√µes**: Participar de competi√ß√µes\n",
        "4. **Pesquisa**: Acompanhar papers recentes\n",
        "5. **Comunidade**: Participar da comunidade\n",
        "\n",
        "### Refer√™ncias Principais\n",
        "\n",
        "- [Deep Learning for Computer Vision - Goodfellow et al.](https://www.deeplearningbook.org/)\n",
        "- [Computer Vision: Algorithms and Applications - Szeliski](https://szeliski.org/Book/)\n",
        "- [Attention Is All You Need - Vaswani et al.](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "---\n",
        "\n",
        "**üéâ Parab√©ns! Voc√™ concluiu o curso de Vis√£o Computacional!**\n",
        "\n",
        "**Obrigado por acompanhar este curso completo sobre Vis√£o Computacional e Deep Learning!**"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}