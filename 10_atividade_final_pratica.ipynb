{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Módulo 10: Atividade Final Prática",
        "",
        "## Objetivos de Aprendizagem",
        "- Aplicar todos os conceitos aprendidos em um projeto completo",
        "- Implementar uma solução de visão computacional do zero",
        "- Integrar diferentes técnicas e abordagens",
        "- Desenvolver habilidades práticas de implementação",
        "- Apresentar resultados e análises técnicas",
        "",
        "---",
        "",
        "## 10.1 Projeto Final: Sistema de Análise de Imagens Multimodal",
        "",
        "### Descrição do Projeto",
        "",
        "**Objetivo:** Desenvolver um sistema completo de análise de imagens que integre diferentes técnicas de visão computacional aprendidas ao longo do curso.",
        "",
        "![Descrição Projeto Final](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo10/descricao_projeto_final.png)",
        "",
        "**Funcionalidades Principais:**",
        "1. **Classificação de Imagens** usando CNNs tradicionais",
        "2. **Detecção de Objetos** com YOLO simplificado",
        "3. **Segmentação de Imagens** usando U-Net",
        "4. **OCR e Extração de Texto** com Tesseract/EasyOCR",
        "5. **Análise Multimodal** com Foundation Models (CLIP/GPT-4V)",
        "6. **Interface de Usuário** para interação",
        "",
        "### Arquitetura do Sistema",
        "",
        "**Estrutura Modular:**",
        "",
        "![Arquitetura Sistema](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo10/arquitetura_sistema.png)",
        "",
        "**Componentes:**",
        "- **Input Module**: Entrada de imagens",
        "- **Preprocessing Module**: Pré-processamento",
        "- **Analysis Module**: Análise multimodal",
        "- **Output Module**: Resultados e visualização",
        "",
        "### Tecnologias Utilizadas",
        "",
        "**Frameworks:**",
        "- **PyTorch**: Deep learning",
        "- **OpenCV**: Processamento de imagem",
        "- **Transformers**: Modelos pré-treinados",
        "- **Streamlit**: Interface web",
        "",
        "**Modelos:**",
        "- **ResNet**: Classificação",
        "- **YOLO**: Detecção",
        "- **U-Net**: Segmentação",
        "- **CLIP**: Análise multimodal",
        "",
        "![Tecnologias Utilizadas](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo10/tecnologias_utilizadas.png)",
        "",
        "**Referências:**",
        "- [Deep Residual Learning for Image Recognition - He et al.](https://arxiv.org/abs/1512.03385)",
        "- [You Only Look Once: Unified, Real-Time Object Detection - Redmon et al.](https://arxiv.org/abs/1506.02640)",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.2 Demonstração Prática: Sistema Completo\n",
        "\n",
        "Vamos implementar e demonstrar o sistema completo:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "class MultimodalImageAnalysisSystem:\n",
        "    \"\"\"Sistema completo de análise de imagens multimodal\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.results = {}\n",
        "        \n",
        "        # Inicializar módulos\n",
        "        self.classifier = self._build_classifier()\n",
        "        self.detector = self._build_detector()\n",
        "        self.segmenter = self._build_segmenter()\n",
        "        self.ocr_engine = self._build_ocr_engine()\n",
        "        self.multimodal_analyzer = self._build_multimodal_analyzer()\n",
        "        \n",
        "        # Transformações\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    \n",
        "    def _build_classifier(self):\n",
        "        \"\"\"Constrói o classificador de imagens\"\"\"\n",
        "        \n",
        "        class SimpleCNN(nn.Module):\n",
        "            def __init__(self, num_classes=10):\n",
        "                super(SimpleCNN, self).__init__()\n",
        "                \n",
        "                self.features = nn.Sequential(\n",
        "                    nn.Conv2d(3, 32, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    nn.Conv2d(32, 64, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    nn.Conv2d(64, 128, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    nn.Conv2d(128, 256, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.AdaptiveAvgPool2d((1, 1))\n",
        "                )\n",
        "                \n",
        "                self.classifier = nn.Sequential(\n",
        "                    nn.Dropout(0.5),\n",
        "                    nn.Linear(256, 128),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(0.5),\n",
        "                    nn.Linear(128, num_classes)\n",
        "                )\n",
        "                \n",
        "            def forward(self, x):\n",
        "                x = self.features(x)\n",
        "                x = x.view(x.size(0), -1)\n",
        "                x = self.classifier(x)\n",
        "                return x\n",
        "        \n",
        "        return SimpleCNN(num_classes=10).to(self.device)\n",
        "    \n",
        "    def _build_detector(self):\n",
        "        \"\"\"Constrói o detector de objetos\"\"\"\n",
        "        \n",
        "        class SimpleYOLO(nn.Module):\n",
        "            def __init__(self, num_classes=5):\n",
        "                super(SimpleYOLO, self).__init__()\n",
        "                \n",
        "                self.backbone = nn.Sequential(\n",
        "                    nn.Conv2d(3, 32, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    nn.Conv2d(32, 64, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    nn.Conv2d(64, 128, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2)\n",
        "                )\n",
        "                \n",
        "                # Output: [batch, 5 + num_classes, H, W]\n",
        "                # 5 = [x, y, w, h, confidence]\n",
        "                self.detection_head = nn.Conv2d(128, 5 + num_classes, 1)\n",
        "                \n",
        "            def forward(self, x):\n",
        "                features = self.backbone(x)\n",
        "                detections = self.detection_head(features)\n",
        "                return detections\n",
        "        \n",
        "        return SimpleYOLO(num_classes=5).to(self.device)\n",
        "    \n",
        "    def _build_segmenter(self):\n",
        "        \"\"\"Constrói o segmentador de imagens\"\"\"\n",
        "        \n",
        "        class SimpleUNet(nn.Module):\n",
        "            def __init__(self, num_classes=3):\n",
        "                super(SimpleUNet, self).__init__()\n",
        "                \n",
        "                # Encoder\n",
        "                self.enc1 = nn.Sequential(\n",
        "                    nn.Conv2d(3, 64, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(64, 64, 3, padding=1),\n",
        "                    nn.ReLU()\n",
        "                )\n",
        "                \n",
        "                self.enc2 = nn.Sequential(\n",
        "                    nn.MaxPool2d(2),\n",
        "                    nn.Conv2d(64, 128, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(128, 128, 3, padding=1),\n",
        "                    nn.ReLU()\n",
        "                )\n",
        "                \n",
        "                # Decoder\n",
        "                self.dec1 = nn.Sequential(\n",
        "                    nn.ConvTranspose2d(128, 64, 2, stride=2),\n",
        "                    nn.Conv2d(128, 64, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(64, 64, 3, padding=1),\n",
        "                    nn.ReLU()\n",
        "                )\n",
        "                \n",
        "                self.final = nn.Conv2d(64, num_classes, 1)\n",
        "                \n",
        "            def forward(self, x):\n",
        "                # Encoder\n",
        "                enc1 = self.enc1(x)\n",
        "                enc2 = self.enc2(enc1)\n",
        "                \n",
        "                # Decoder\n",
        "                dec1 = self.dec1(enc2)\n",
        "                \n",
        "                # Skip connection\n",
        "                dec1 = torch.cat([dec1, enc1], dim=1)\n",
        "                \n",
        "                # Final output\n",
        "                output = self.final(dec1)\n",
        "                \n",
        "                return output\n",
        "        \n",
        "        return SimpleUNet(num_classes=3).to(self.device)\n",
        "    \n",
        "    def _build_ocr_engine(self):\n",
        "        \"\"\"Constrói o motor OCR\"\"\"\n",
        "        \n",
        "        class SimpleOCR:\n",
        "            def __init__(self):\n",
        "                self.text_regions = []\n",
        "                \n",
        "            def preprocess_image(self, image):\n",
        "                \"\"\"Pré-processa imagem para OCR\"\"\"\n",
        "                # Converter para escala de cinza\n",
        "                if len(image.shape) == 3:\n",
        "                    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "                else:\n",
        "                    gray = image\n",
        "                \n",
        "                # Aplicar threshold\n",
        "                _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "                \n",
        "                # Reduzir ruído\n",
        "                kernel = np.ones((3, 3), np.uint8)\n",
        "                cleaned = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)\n",
        "                \n",
        "                return cleaned\n",
        "            \n",
        "            def detect_text_regions(self, image):\n",
        "                \"\"\"Detecta regiões de texto\"\"\"\n",
        "                # Usar contours para detectar regiões\n",
        "                contours, _ = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "                \n",
        "                text_regions = []\n",
        "                for contour in contours:\n",
        "                    x, y, w, h = cv2.boundingRect(contour)\n",
        "                    if w > 20 and h > 10:  # Filtrar regiões muito pequenas\n",
        "                        text_regions.append((x, y, w, h))\n",
        "                \n",
        "                return text_regions\n",
        "            \n",
        "            def extract_text(self, image):\n",
        "                \"\"\"Extrai texto da imagem\"\"\"\n",
        "                # Simular extração de texto\n",
        "                # Em implementação real, usar Tesseract ou EasyOCR\n",
        "                \n",
        "                preprocessed = self.preprocess_image(image)\n",
        "                text_regions = self.detect_text_regions(preprocessed)\n",
        "                \n",
        "                # Simular texto extraído\n",
        "                extracted_text = f\"Texto detectado em {len(text_regions)} regiões\"\n",
        "                \n",
        "                return extracted_text, text_regions\n",
        "        \n",
        "        return SimpleOCR()\n",
        "    \n",
        "    def _build_multimodal_analyzer(self):\n",
        "        \"\"\"Constrói o analisador multimodal\"\"\"\n",
        "        \n",
        "        class MultimodalAnalyzer:\n",
        "            def __init__(self):\n",
        "                self.text_encoder = nn.Sequential(\n",
        "                    nn.Linear(100, 128),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(128, 128)\n",
        "                )\n",
        "                \n",
        "                self.image_encoder = nn.Sequential(\n",
        "                    nn.Linear(224*224*3, 512),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(512, 128)\n",
        "                )\n",
        "                \n",
        "            def analyze_multimodal(self, image, text):\n",
        "                \"\"\"Analisa conteúdo multimodal\"\"\"\n",
        "                # Simular análise multimodal\n",
        "                \n",
        "                # Análise de imagem\n",
        "                image_features = self.image_encoder(image.flatten())\n",
        "                \n",
        "                # Análise de texto\n",
        "                text_features = self.text_encoder(torch.randn(100))\n",
        "                \n",
        "                # Similaridade\n",
        "                similarity = F.cosine_similarity(image_features, text_features, dim=0)\n",
        "                \n",
        "                # Análise semântica\n",
        "                analysis = {\n",
        "                    'similarity': similarity.item(),\n",
        "                    'image_objects': ['objeto1', 'objeto2', 'objeto3'],\n",
        "                    'text_entities': ['entidade1', 'entidade2'],\n",
        "                    'semantic_match': similarity.item() > 0.5\n",
        "                }\n",
        "                \n",
        "                return analysis\n",
        "        \n",
        "        return MultimodalAnalyzer().to(self.device)\n",
        "    \n",
        "    def create_sample_image(self, width=224, height=224):\n",
        "        \"\"\"Cria uma imagem de exemplo para demonstração\"\"\"\n",
        "        \n",
        "        # Criar imagem base\n",
        "        image = np.zeros((height, width, 3), dtype=np.uint8)\n",
        "        \n",
        "        # Adicionar formas geométricas\n",
        "        # Círculo vermelho\n",
        "        cv2.circle(image, (80, 80), 30, (255, 0, 0), -1)\n",
        "        \n",
        "        # Retângulo azul\n",
        "        cv2.rectangle(image, (150, 50), (200, 100), (0, 255, 0), -1)\n",
        "        \n",
        "        # Triângulo amarelo\n",
        "        pts = np.array([[100, 150], [80, 200], [120, 200]], np.int32)\n",
        "        cv2.fillPoly(image, [pts], (0, 0, 255))\n",
        "        \n",
        "        # Adicionar texto simulado\n",
        "        cv2.putText(image, 'SAMPLE', (50, 220), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
        "        \n",
        "        return image\n",
        "    \n",
        "    def analyze_image(self, image):\n",
        "        \"\"\"Analisa uma imagem usando todos os módulos\"\"\"\n",
        "        \n",
        "        results = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'image_shape': image.shape,\n",
        "            'analysis': {}\n",
        "        }\n",
        "        \n",
        "        # 1. Classificação\n",
        "        print(\"Executando classificação...\")\n",
        "        classification_result = self._classify_image(image)\n",
        "        results['analysis']['classification'] = classification_result\n",
        "        \n",
        "        # 2. Detecção de objetos\n",
        "        print(\"Executando detecção de objetos...\")\n",
        "        detection_result = self._detect_objects(image)\n",
        "        results['analysis']['detection'] = detection_result\n",
        "        \n",
        "        # 3. Segmentação\n",
        "        print(\"Executando segmentação...\")\n",
        "        segmentation_result = self._segment_image(image)\n",
        "        results['analysis']['segmentation'] = segmentation_result\n",
        "        \n",
        "        # 4. OCR\n",
        "        print(\"Executando OCR...\")\n",
        "        ocr_result = self._extract_text(image)\n",
        "        results['analysis']['ocr'] = ocr_result\n",
        "        \n",
        "        # 5. Análise multimodal\n",
        "        print(\"Executando análise multimodal...\")\n",
        "        multimodal_result = self._analyze_multimodal(image, ocr_result['text'])\n",
        "        results['analysis']['multimodal'] = multimodal_result\n",
        "        \n",
        "        self.results = results\n",
        "        return results\n",
        "    \n",
        "    def _classify_image(self, image):\n",
        "        \"\"\"Classifica a imagem\"\"\"\n",
        "        # Converter para tensor\n",
        "        image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
        "        \n",
        "        # Classificar\n",
        "        self.classifier.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.classifier(image_tensor)\n",
        "            probabilities = F.softmax(outputs, dim=1)\n",
        "            predicted_class = torch.argmax(probabilities, dim=1).item()\n",
        "            confidence = probabilities[0, predicted_class].item()\n",
        "        \n",
        "        return {\n",
        "            'predicted_class': predicted_class,\n",
        "            'confidence': confidence,\n",
        "            'probabilities': probabilities[0].cpu().numpy().tolist()\n",
        "        }\n",
        "    \n",
        "    def _detect_objects(self, image):\n",
        "        \"\"\"Detecta objetos na imagem\"\"\"\n",
        "        # Converter para tensor\n",
        "        image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
        "        \n",
        "        # Detectar\n",
        "        self.detector.eval()\n",
        "        with torch.no_grad():\n",
        "            detections = self.detector(image_tensor)\n",
        "        \n",
        "        # Simular detecções\n",
        "        objects = [\n",
        "            {'class': 'circle', 'confidence': 0.95, 'bbox': [50, 50, 60, 60]},\n",
        "            {'class': 'rectangle', 'confidence': 0.87, 'bbox': [140, 40, 60, 60]},\n",
        "            {'class': 'triangle', 'confidence': 0.92, 'bbox': [80, 140, 40, 60]}\n",
        "        ]\n",
        "        \n",
        "        return {\n",
        "            'num_objects': len(objects),\n",
        "            'objects': objects\n",
        "        }\n",
        "    \n",
        "    def _segment_image(self, image):\n",
        "        \"\"\"Segmenta a imagem\"\"\"\n",
        "        # Converter para tensor\n",
        "        image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
        "        \n",
        "        # Segmentar\n",
        "        self.segmenter.eval()\n",
        "        with torch.no_grad():\n",
        "            segmentation = self.segmenter(image_tensor)\n",
        "            segmentation = F.softmax(segmentation, dim=1)\n",
        "            predicted_segments = torch.argmax(segmentation, dim=1)\n",
        "        \n",
        "        return {\n",
        "            'num_segments': 3,\n",
        "            'segmentation_map': predicted_segments[0].cpu().numpy().tolist()\n",
        "        }\n",
        "    \n",
        "    def _extract_text(self, image):\n",
        "        \"\"\"Extrai texto da imagem\"\"\"\n",
        "        text, regions = self.ocr_engine.extract_text(image)\n",
        "        \n",
        "        return {\n",
        "            'text': text,\n",
        "            'num_regions': len(regions),\n",
        "            'regions': regions\n",
        "        }\n",
        "    \n",
        "    def _analyze_multimodal(self, image, text):\n",
        "        \"\"\"Analisa conteúdo multimodal\"\"\"\n",
        "        # Converter imagem para tensor\n",
        "        image_tensor = torch.FloatTensor(image).flatten().to(self.device)\n",
        "        \n",
        "        # Análise multimodal\n",
        "        analysis = self.multimodal_analyzer.analyze_multimodal(image_tensor, text)\n",
        "        \n",
        "        return analysis\n",
        "    \n",
        "    def visualize_results(self, image, results):\n",
        "        \"\"\"Visualiza os resultados da análise\"\"\"\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        \n",
        "        # Imagem original\n",
        "        axes[0, 0].imshow(image)\n",
        "        axes[0, 0].set_title('Imagem Original')\n",
        "        axes[0, 0].axis('off')\n",
        "        \n",
        "        # Classificação\n",
        "        classification = results['analysis']['classification']\n",
        "        axes[0, 1].bar(range(len(classification['probabilities'])), classification['probabilities'])\n",
        "        axes[0, 1].set_title(f'Classificação\\nClasse: {classification[\"predicted_class\"]}, Conf: {classification[\"confidence\"]:.2f}')\n",
        "        axes[0, 1].set_xlabel('Classe')\n",
        "        axes[0, 1].set_ylabel('Probabilidade')\n",
        "        \n",
        "        # Detecção de objetos\n",
        "        detection = results['analysis']['detection']\n",
        "        detection_image = image.copy()\n",
        "        for obj in detection['objects']:\n",
        "            x, y, w, h = obj['bbox']\n",
        "            cv2.rectangle(detection_image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "            cv2.putText(detection_image, f\"{obj['class']}: {obj['confidence']:.2f}\", \n",
        "                        (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "        \n",
        "        axes[0, 2].imshow(detection_image)\n",
        "        axes[0, 2].set_title(f'Detecção de Objetos\\n{detection[\"num_objects\"]} objetos detectados')\n",
        "        axes[0, 2].axis('off')\n",
        "        \n",
        "        # Segmentação\n",
        "        segmentation = results['analysis']['segmentation']\n",
        "        seg_map = np.array(segmentation['segmentation_map'])\n",
        "        axes[1, 0].imshow(seg_map, cmap='viridis')\n",
        "        axes[1, 0].set_title(f'Segmentação\\n{segmentation[\"num_segments\"]} segmentos')\n",
        "        axes[1, 0].axis('off')\n",
        "        \n",
        "        # OCR\n",
        "        ocr = results['analysis']['ocr']\n",
        "        ocr_image = image.copy()\n",
        "        for region in ocr['regions']:\n",
        "            x, y, w, h = region\n",
        "            cv2.rectangle(ocr_image, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
        "        \n",
        "        axes[1, 1].imshow(ocr_image)\n",
        "        axes[1, 1].set_title(f'OCR\\n{ocr[\"num_regions\"]} regiões de texto')\n",
        "        axes[1, 1].axis('off')\n",
        "        \n",
        "        # Análise multimodal\n",
        "        multimodal = results['analysis']['multimodal']\n",
        "        axes[1, 2].text(0.1, 0.8, f'Similaridade: {multimodal[\"similarity\"]:.3f}', transform=axes[1, 2].transAxes)\n",
        "        axes[1, 2].text(0.1, 0.6, f'Match Semântico: {multimodal[\"semantic_match\"]}', transform=axes[1, 2].transAxes)\n",
        "        axes[1, 2].text(0.1, 0.4, f'Objetos: {len(multimodal[\"image_objects\"])}', transform=axes[1, 2].transAxes)\n",
        "        axes[1, 2].text(0.1, 0.2, f'Entidades: {len(multimodal[\"text_entities\"])}', transform=axes[1, 2].transAxes)\n",
        "        axes[1, 2].set_title('Análise Multimodal')\n",
        "        axes[1, 2].axis('off')\n",
        "        \n",
        "        plt.suptitle('Sistema de Análise de Imagens Multimodal', fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return fig\n",
        "    \n",
        "    def generate_report(self, results):\n",
        "        \"\"\"Gera relatório da análise\"\"\"\n",
        "        \n",
        "        report = {\n",
        "            'summary': {\n",
        "                'timestamp': results['timestamp'],\n",
        "                'image_shape': results['image_shape'],\n",
        "                'total_analyses': len(results['analysis'])\n",
        "            },\n",
        "            'classification': {\n",
        "                'predicted_class': results['analysis']['classification']['predicted_class'],\n",
        "                'confidence': results['analysis']['classification']['confidence']\n",
        "            },\n",
        "            'detection': {\n",
        "                'num_objects': results['analysis']['detection']['num_objects'],\n",
        "                'objects': results['analysis']['detection']['objects']\n",
        "            },\n",
        "            'segmentation': {\n",
        "                'num_segments': results['analysis']['segmentation']['num_segments']\n",
        "            },\n",
        "            'ocr': {\n",
        "                'text': results['analysis']['ocr']['text'],\n",
        "                'num_regions': results['analysis']['ocr']['num_regions']\n",
        "            },\n",
        "            'multimodal': {\n",
        "                'similarity': results['analysis']['multimodal']['similarity'],\n",
        "                'semantic_match': results['analysis']['multimodal']['semantic_match']\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        return report\n",
        "\n",
        "def demonstrate_complete_system():\n",
        "    \"\"\"Demonstra o sistema completo\"\"\"\n",
        "    \n",
        "    print(\"=== Sistema de Análise de Imagens Multimodal ===\")\n",
        "    print(\"\\nNota: Esta demonstração usa modelos simplificados para fins educacionais.\")\n",
        "    print(\"Em aplicações reais, use modelos pré-treinados e otimizados.\")\n",
        "    \n",
        "    # Criar instância do sistema\n",
        "    system = MultimodalImageAnalysisSystem()\n",
        "    \n",
        "    # Criar imagem de exemplo\n",
        "    print(\"\\nCriando imagem de exemplo...\")\n",
        "    sample_image = system.create_sample_image()\n",
        "    \n",
        "    # Analisar imagem\n",
        "    print(\"\\nAnalisando imagem...\")\n",
        "    results = system.analyze_image(sample_image)\n",
        "    \n",
        "    # Visualizar resultados\n",
        "    print(\"\\nVisualizando resultados...\")\n",
        "    fig = system.visualize_results(sample_image, results)\n",
        "    \n",
        "    # Gerar relatório\n",
        "    print(\"\\nGerando relatório...\")\n",
        "    report = system.generate_report(results)\n",
        "    \n",
        "    # Exibir resumo\n",
        "    print(\"\\n=== RESUMO DA ANÁLISE ===\")\n",
        "    print(f\"Timestamp: {report['summary']['timestamp']}\")\n",
        "    print(f\"Formato da imagem: {report['summary']['image_shape']}\")\n",
        "    print(f\"Análises realizadas: {report['summary']['total_analyses']}\")\n",
        "    print(f\"\\nClassificação: Classe {report['classification']['predicted_class']} (Conf: {report['classification']['confidence']:.2f})\")\n",
        "    print(f\"Detecção: {report['detection']['num_objects']} objetos\")\n",
        "    print(f\"Segmentação: {report['segmentation']['num_segments']} segmentos\")\n",
        "    print(f\"OCR: {report['ocr']['num_regions']} regiões de texto\")\n",
        "    print(f\"Multimodal: Similaridade {report['multimodal']['similarity']:.3f}\")\n",
        "    \n",
        "    return system, sample_image, results, report\n",
        "\n",
        "# Executar demonstração\n",
        "complete_system, demo_image, analysis_results, final_report = demonstrate_complete_system()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análise dos Resultados\n",
        "\n",
        "**Sistema Completo Observado:**\n",
        "\n",
        "1. **Classificação**: CNN para classificação de imagens\n",
        "2. **Detecção**: YOLO simplificado para detecção de objetos\n",
        "3. **Segmentação**: U-Net para segmentação semântica\n",
        "4. **OCR**: Extração de texto com detecção de regiões\n",
        "5. **Multimodal**: Análise combinada de imagem e texto\n",
        "6. **Visualização**: Resultados integrados em dashboard\n",
        "\n",
        "**Insights Importantes:**\n",
        "- **Integração**: Múltiplas técnicas trabalhando juntas\n",
        "- **Pipeline**: Fluxo completo de análise\n",
        "- **Visualização**: Resultados claros e interpretáveis\n",
        "- **Relatório**: Documentação completa da análise\n",
        "\n",
        "**Referências:**\n",
        "- [Deep Residual Learning for Image Recognition - He et al.](https://arxiv.org/abs/1512.03385)\n",
        "- [You Only Look Once: Unified, Real-Time Object Detection - Redmon et al.](https://arxiv.org/abs/1506.02640)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.3 Implementação de Interface Web\n",
        "\n",
        "### Streamlit Dashboard\n",
        "\n",
        "Vamos criar uma interface web para o sistema:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from io import BytesIO\n",
        "import base64\n",
        "\n",
        "class WebInterface:\n",
        "    \"\"\"Interface web para o sistema de análise\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.system = MultimodalImageAnalysisSystem()\n",
        "        \n",
        "    def create_dashboard(self):\n",
        "        \"\"\"Cria o dashboard principal\"\"\"\n",
        "        \n",
        "        st.set_page_config(\n",
        "            page_title=\"Sistema de Análise de Imagens Multimodal\",\n",
        "            page_icon=\"🔍\",\n",
        "            layout=\"wide\"\n",
        "        )\n",
        "        \n",
        "        st.title(\"🔍 Sistema de Análise de Imagens Multimodal\")\n",
        "        st.markdown(\"---\")\n",
        "        \n",
        "        # Sidebar\n",
        "        st.sidebar.title(\"Configurações\")\n",
        "        \n",
        "        # Upload de imagem\n",
        "        uploaded_file = st.sidebar.file_uploader(\n",
        "            \"Upload de Imagem\",\n",
        "            type=['png', 'jpg', 'jpeg'],\n",
        "            help=\"Faça upload de uma imagem para análise\"\n",
        "        )\n",
        "        \n",
        "        # Opções de análise\n",
        "        st.sidebar.subheader(\"Módulos de Análise\")\n",
        "        enable_classification = st.sidebar.checkbox(\"Classificação\", value=True)\n",
        "        enable_detection = st.sidebar.checkbox(\"Detecção de Objetos\", value=True)\n",
        "        enable_segmentation = st.sidebar.checkbox(\"Segmentação\", value=True)\n",
        "        enable_ocr = st.sidebar.checkbox(\"OCR\", value=True)\n",
        "        enable_multimodal = st.sidebar.checkbox(\"Análise Multimodal\", value=True)\n",
        "        \n",
        "        # Botão de análise\n",
        "        if st.sidebar.button(\"Analisar Imagem\", type=\"primary\"):\n",
        "            if uploaded_file is not None:\n",
        "                self._analyze_uploaded_image(uploaded_file, {\n",
        "                    'classification': enable_classification,\n",
        "                    'detection': enable_detection,\n",
        "                    'segmentation': enable_segmentation,\n",
        "                    'ocr': enable_ocr,\n",
        "                    'multimodal': enable_multimodal\n",
        "                })\n",
        "            else:\n",
        "                st.error(\"Por favor, faça upload de uma imagem primeiro.\")\n",
        "        \n",
        "        # Demo com imagem de exemplo\n",
        "        if st.sidebar.button(\"Usar Imagem de Exemplo\"):\n",
        "            self._demo_with_sample_image()\n",
        "        \n",
        "        # Exibir imagem atual\n",
        "        if 'current_image' in st.session_state:\n",
        "            st.subheader(\"Imagem Atual\")\n",
        "            st.image(st.session_state['current_image'], caption=\"Imagem para análise\", use_column_width=True)\n",
        "        \n",
        "        # Exibir resultados\n",
        "        if 'analysis_results' in st.session_state:\n",
        "            self._display_results(st.session_state['analysis_results'])\n",
        "        \n",
        "        # Estatísticas do sistema\n",
        "        self._display_system_stats()\n",
        "    \n",
        "    def _analyze_uploaded_image(self, uploaded_file, options):\n",
        "        \"\"\"Analisa imagem enviada\"\"\"\n",
        "        \n",
        "        # Converter para numpy array\n",
        "        image = np.array(bytearray(uploaded_file.read()), dtype=np.uint8)\n",
        "        image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # Redimensionar se necessário\n",
        "        if image.shape[0] > 512 or image.shape[1] > 512:\n",
        "            image = cv2.resize(image, (512, 512))\n",
        "        \n",
        "        # Armazenar imagem\n",
        "        st.session_state['current_image'] = image\n",
        "        \n",
        "        # Analisar\n",
        "        with st.spinner('Analisando imagem...'):\n",
        "            results = self.system.analyze_image(image)\n",
        "            st.session_state['analysis_results'] = results\n",
        "        \n",
        "        st.success(\"Análise concluída!\")\n",
        "    \n",
        "    def _demo_with_sample_image(self):\n",
        "        \"\"\"Demonstra com imagem de exemplo\"\"\"\n",
        "        \n",
        "        # Criar imagem de exemplo\n",
        "        sample_image = self.system.create_sample_image()\n",
        "        \n",
        "        # Armazenar imagem\n",
        "        st.session_state['current_image'] = sample_image\n",
        "        \n",
        "        # Analisar\n",
        "        with st.spinner('Analisando imagem de exemplo...'):\n",
        "            results = self.system.analyze_image(sample_image)\n",
        "            st.session_state['analysis_results'] = results\n",
        "        \n",
        "        st.success(\"Análise da imagem de exemplo concluída!\")\n",
        "    \n",
        "    def _display_results(self, results):\n",
        "        \"\"\"Exibe os resultados da análise\"\"\"\n",
        "        \n",
        "        st.subheader(\"📊 Resultados da Análise\")\n",
        "        \n",
        "        # Métricas gerais\n",
        "        col1, col2, col3, col4 = st.columns(4)\n",
        "        \n",
        "        with col1:\n",
        "            st.metric(\n",
        "                \"Classificação\",\n",
        "                f\"Classe {results['analysis']['classification']['predicted_class']}\",\n",
        "                f\"{results['analysis']['classification']['confidence']:.2f}\"\n",
        "            )\n",
        "        \n",
        "        with col2:\n",
        "            st.metric(\n",
        "                \"Objetos Detectados\",\n",
        "                results['analysis']['detection']['num_objects'],\n",
        "                \"objetos\"\n",
        "            )\n",
        "        \n",
        "        with col3:\n",
        "            st.metric(\n",
        "                \"Segmentos\",\n",
        "                results['analysis']['segmentation']['num_segments'],\n",
        "                \"segmentos\"\n",
        "            )\n",
        "        \n",
        "        with col4:\n",
        "            st.metric(\n",
        "                \"Regiões OCR\",\n",
        "                results['analysis']['ocr']['num_regions'],\n",
        "                \"regiões\"\n",
        "            )\n",
        "        \n",
        "        # Tabs para diferentes análises\n",
        "        tab1, tab2, tab3, tab4, tab5 = st.tabs([\n",
        "            \"🎯 Classificação\",\n",
        "            \"🔍 Detecção\",\n",
        "            \"✂️ Segmentação\",\n",
        "            \"📝 OCR\",\n",
        "            \"🔗 Multimodal\"\n",
        "        ])\n",
        "        \n",
        "        with tab1:\n",
        "            self._display_classification_results(results['analysis']['classification'])\n",
        "        \n",
        "        with tab2:\n",
        "            self._display_detection_results(results['analysis']['detection'])\n",
        "        \n",
        "        with tab3:\n",
        "            self._display_segmentation_results(results['analysis']['segmentation'])\n",
        "        \n",
        "        with tab4:\n",
        "            self._display_ocr_results(results['analysis']['ocr'])\n",
        "        \n",
        "        with tab5:\n",
        "            self._display_multimodal_results(results['analysis']['multimodal'])\n",
        "    \n",
        "    def _display_classification_results(self, classification):\n",
        "        \"\"\"Exibe resultados de classificação\"\"\"\n",
        "        \n",
        "        # Gráfico de barras das probabilidades\n",
        "        fig = px.bar(\n",
        "            x=list(range(len(classification['probabilities']))),\n",
        "            y=classification['probabilities'],\n",
        "            title=\"Probabilidades de Classificação\",\n",
        "            labels={'x': 'Classe', 'y': 'Probabilidade'}\n",
        "        )\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "        \n",
        "        # Informações detalhadas\n",
        "        st.write(f\"**Classe Predita:** {classification['predicted_class']}\")\n",
        "        st.write(f\"**Confiança:** {classification['confidence']:.3f}\")\n",
        "    \n",
        "    def _display_detection_results(self, detection):\n",
        "        \"\"\"Exibe resultados de detecção\"\"\"\n",
        "        \n",
        "        # Tabela de objetos\n",
        "        if detection['objects']:\n",
        "            df = pd.DataFrame(detection['objects'])\n",
        "            st.dataframe(df, use_container_width=True)\n",
        "        \n",
        "        # Gráfico de confiança\n",
        "        if detection['objects']:\n",
        "            confidences = [obj['confidence'] for obj in detection['objects']]\n",
        "            classes = [obj['class'] for obj in detection['objects']]\n",
        "            \n",
        "            fig = px.bar(\n",
        "                x=classes,\n",
        "                y=confidences,\n",
        "                title=\"Confiança por Classe\",\n",
        "                labels={'x': 'Classe', 'y': 'Confiança'}\n",
        "            )\n",
        "            st.plotly_chart(fig, use_container_width=True)\n",
        "    \n",
        "    def _display_segmentation_results(self, segmentation):\n",
        "        \"\"\"Exibe resultados de segmentação\"\"\"\n",
        "        \n",
        "        st.write(f\"**Número de Segmentos:** {segmentation['num_segments']}\")\n",
        "        \n",
        "        # Visualizar mapa de segmentação\n",
        "        seg_map = np.array(segmentation['segmentation_map'])\n",
        "        \n",
        "        fig = px.imshow(\n",
        "            seg_map,\n",
        "            title=\"Mapa de Segmentação\",\n",
        "            color_continuous_scale='viridis'\n",
        "        )\n",
        "        st.plotly_chart(fig, use_container_width=True)\n",
        "    \n",
        "    def _display_ocr_results(self, ocr):\n",
        "        \"\"\"Exibe resultados de OCR\"\"\"\n",
        "        \n",
        "        st.write(f\"**Texto Extraído:** {ocr['text']}\")\n",
        "        st.write(f\"**Número de Regiões:** {ocr['num_regions']}\")\n",
        "        \n",
        "        # Tabela de regiões\n",
        "        if ocr['regions']:\n",
        "            df = pd.DataFrame(ocr['regions'], columns=['X', 'Y', 'Width', 'Height'])\n",
        "            st.dataframe(df, use_container_width=True)\n",
        "    \n",
        "    def _display_multimodal_results(self, multimodal):\n",
        "        \"\"\"Exibe resultados multimodais\"\"\"\n",
        "        \n",
        "        # Métricas\n",
        "        col1, col2 = st.columns(2)\n",
        "        \n",
        "        with col1:\n",
        "            st.metric(\n",
        "                \"Similaridade\",\n",
        "                f\"{multimodal['similarity']:.3f}\"\n",
        "            )\n",
        "        \n",
        "        with col2:\n",
        "            st.metric(\n",
        "                \"Match Semântico\",\n",
        "                \"Sim\" if multimodal['semantic_match'] else \"Não\"\n",
        "            )\n",
        "        \n",
        "        # Lista de objetos e entidades\n",
        "        col1, col2 = st.columns(2)\n",
        "        \n",
        "        with col1:\n",
        "            st.write(\"**Objetos Detectados:**\")\n",
        "            for obj in multimodal['image_objects']:\n",
        "                st.write(f\"- {obj}\")\n",
        "        \n",
        "        with col2:\n",
        "            st.write(\"**Entidades de Texto:**\")\n",
        "            for entity in multimodal['text_entities']:\n",
        "                st.write(f\"- {entity}\")\n",
        "    \n",
        "    def _display_system_stats(self):\n",
        "        \"\"\"Exibe estatísticas do sistema\"\"\"\n",
        "        \n",
        "        st.subheader(\"📈 Estatísticas do Sistema\")\n",
        "        \n",
        "        col1, col2, col3 = st.columns(3)\n",
        "        \n",
        "        with col1:\n",
        "            st.metric(\"Módulos Ativos\", \"5\", \"módulos\")\n",
        "        \n",
        "        with col2:\n",
        "            st.metric(\"Análises Realizadas\", \"1\", \"análise\")\n",
        "        \n",
        "        with col3:\n",
        "            st.metric(\"Tempo Médio\", \"< 1s\", \"por análise\")\n",
        "    \n",
        "    def run_app(self):\n",
        "        \"\"\"Executa a aplicação\"\"\"\n",
        "        \n",
        "        self.create_dashboard()\n",
        "\n",
        "def create_streamlit_app():\n",
        "    \"\"\"Cria a aplicação Streamlit\"\"\"\n",
        "    \n",
        "    print(\"=== Criando Interface Web com Streamlit ===\")\n",
        "    print(\"\\nPara executar a aplicação, use o comando:\")\n",
        "    print(\"streamlit run app.py\")\n",
        "    \n",
        "    # Criar arquivo da aplicação\n",
        "    app_code = '''\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from multimodal_system import MultimodalImageAnalysisSystem\n",
        "\n",
        "def main():\n",
        "    st.set_page_config(\n",
        "        page_title=\"Sistema de Análise de Imagens Multimodal\",\n",
        "        page_icon=\"🔍\",\n",
        "        layout=\"wide\"\n",
        "    )\n",
        "    \n",
        "    st.title(\"🔍 Sistema de Análise de Imagens Multimodal\")\n",
        "    \n",
        "    # Inicializar sistema\n",
        "    if 'system' not in st.session_state:\n",
        "        st.session_state.system = MultimodalImageAnalysisSystem()\n",
        "    \n",
        "    # Upload de imagem\n",
        "    uploaded_file = st.file_uploader(\n",
        "        \"Faça upload de uma imagem\",\n",
        "        type=['png', 'jpg', 'jpeg']\n",
        "    )\n",
        "    \n",
        "    if uploaded_file is not None:\n",
        "        # Processar imagem\n",
        "        image = np.array(bytearray(uploaded_file.read()), dtype=np.uint8)\n",
        "        image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        # Exibir imagem\n",
        "        st.image(image, caption=\"Imagem enviada\", use_column_width=True)\n",
        "        \n",
        "        # Analisar\n",
        "        if st.button(\"Analisar Imagem\"):\n",
        "            with st.spinner('Analisando...'):\n",
        "                results = st.session_state.system.analyze_image(image)\n",
        "            \n",
        "            # Exibir resultados\n",
        "            st.success(\"Análise concluída!\")\n",
        "            \n",
        "            # Métricas\n",
        "            col1, col2, col3, col4 = st.columns(4)\n",
        "            \n",
        "            with col1:\n",
        "                st.metric(\n",
        "                    \"Classificação\",\n",
        "                    f\"Classe {results['analysis']['classification']['predicted_class']}\",\n",
        "                    f\"{results['analysis']['classification']['confidence']:.2f}\"\n",
        "                )\n",
        "            \n",
        "            with col2:\n",
        "                st.metric(\n",
        "                    \"Objetos\",\n",
        "                    results['analysis']['detection']['num_objects']\n",
        "                )\n",
        "            \n",
        "            with col3:\n",
        "                st.metric(\n",
        "                    \"Segmentos\",\n",
        "                    results['analysis']['segmentation']['num_segments']\n",
        "                )\n",
        "            \n",
        "            with col4:\n",
        "                st.metric(\n",
        "                    \"Regiões OCR\",\n",
        "                    results['analysis']['ocr']['num_regions']\n",
        "                )\n",
        "    \n",
        "    # Demo com imagem de exemplo\n",
        "    if st.button(\"Usar Imagem de Exemplo\"):\n",
        "        sample_image = st.session_state.system.create_sample_image()\n",
        "        st.image(sample_image, caption=\"Imagem de exemplo\", use_column_width=True)\n",
        "        \n",
        "        with st.spinner('Analisando imagem de exemplo...'):\n",
        "            results = st.session_state.system.analyze_image(sample_image)\n",
        "        \n",
        "        st.success(\"Análise da imagem de exemplo concluída!\")\n",
        "        \n",
        "        # Exibir resultados\n",
        "        st.json(results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    '''\n",
        "    \n",
        "    with open('app.py', 'w') as f:\n",
        "        f.write(app_code)\n",
        "    \n",
        "    print(\"\\nArquivo 'app.py' criado com sucesso!\")\n",
        "    print(\"\\nPara executar a aplicação:\")\n",
        "    print(\"1. Instale o Streamlit: pip install streamlit\")\n",
        "    print(\"2. Execute: streamlit run app.py\")\n",
        "    print(\"3. Acesse: http://localhost:8501\")\n",
        "    \n",
        "    return app_code\n",
        "\n",
        "# Criar aplicação\n",
        "app_code = create_streamlit_app()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interface Web Implementada",
        "",
        "**Funcionalidades da Interface:**",
        "",
        "1. **Upload de Imagens**: Interface para envio de imagens",
        "2. **Análise Interativa**: Botões para executar análises",
        "3. **Visualização**: Resultados em tempo real",
        "4. **Dashboard**: Métricas e gráficos interativos",
        "5. **Relatórios**: Exportação de resultados",
        "",
        "![Interface Web](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo10/interface_web.png)",
        "",
        "**Tecnologias Utilizadas:**",
        "- **Streamlit**: Framework web",
        "- **Plotly**: Gráficos interativos",
        "- **Pandas**: Manipulação de dados",
        "- **OpenCV**: Processamento de imagem",
        "",
        "**Referências:**",
        "- [Streamlit Documentation](https://docs.streamlit.io/)",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10.4 Casos de Uso e Aplicações",
        "",
        "### Aplicações Práticas",
        "",
        "**1. E-commerce:**",
        "- **Busca visual**: Busca por produtos usando imagens",
        "- **Categorização**: Categorização automática de produtos",
        "- **Moderação**: Moderação de conteúdo visual",
        "- **Recomendações**: Recomendações baseadas em visual",
        "",
        "**2. Saúde:**",
        "- **Diagnóstico**: Assistência em diagnóstico médico",
        "- **Análise**: Análise de imagens médicas",
        "- **Documentação**: Documentação automática",
        "- **Educação**: Educação médica",
        "",
        "**3. Educação:**",
        "- **Acessibilidade**: Acessibilidade visual",
        "- **Tutoria**: Tutoria multimodal",
        "- **Conteúdo**: Criação de conteúdo educacional",
        "- **Avaliação**: Avaliação automática",
        "",
        "**4. Entretenimento:**",
        "- **Geração**: Geração de conteúdo visual",
        "- **Edição**: Edição automática",
        "- **Personalização**: Personalização de conteúdo",
        "- **Interação**: Interação multimodal",
        "",
        "![Casos de Uso](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo10/casos_uso.png)",
        "",
        "### Vantagens do Sistema",
        "",
        "**1. Integração:**",
        "- **Múltiplas técnicas**: Combinação de diferentes abordagens",
        "- **Pipeline completo**: Fluxo de análise end-to-end",
        "- **Flexibilidade**: Adaptável a diferentes casos de uso",
        "- **Escalabilidade**: Arquitetura modular",
        "",
        "**2. Performance:**",
        "- **Eficiência**: Otimização para diferentes tarefas",
        "- **Precisão**: Alta precisão em múltiplas modalidades",
        "- **Velocidade**: Processamento em tempo real",
        "- **Robustez**: Funciona em diferentes condições",
        "",
        "**3. Usabilidade:**",
        "- **Interface intuitiva**: Fácil de usar",
        "- **Visualização clara**: Resultados interpretáveis",
        "- **Relatórios**: Documentação automática",
        "- **Integração**: Fácil integração em sistemas existentes",
        "",
        "### Limitações e Desafios",
        "",
        "**1. Técnicas:**",
        "- **Recursos computacionais**: Requer recursos significativos",
        "- **Dados**: Depende de dados de qualidade",
        "- **Complexidade**: Arquitetura complexa",
        "- **Manutenção**: Requer manutenção contínua",
        "",
        "**2. Práticas:**",
        "- **Deploy**: Desafios de deploy em produção",
        "- **Escalabilidade**: Escalabilidade horizontal",
        "- **Monitoramento**: Monitoramento de performance",
        "- **Atualização**: Atualização de modelos",
        "",
        "**Referências:**",
        "- [Deep Learning for Computer Vision - Goodfellow et al.](https://www.deeplearningbook.org/)",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumo do Módulo 10\n",
        "\n",
        "### Principais Conceitos Abordados\n",
        "\n",
        "1. **Sistema Completo**\n",
        "   - Arquitetura modular\n",
        "   - Integração de técnicas\n",
        "   - Pipeline de análise\n",
        "   - Visualização de resultados\n",
        "\n",
        "2. **Implementação Prática**\n",
        "   - Classificação com CNN\n",
        "   - Detecção com YOLO\n",
        "   - Segmentação com U-Net\n",
        "   - OCR com Tesseract\n",
        "   - Análise multimodal\n",
        "\n",
        "3. **Interface Web**\n",
        "   - Dashboard interativo\n",
        "   - Upload de imagens\n",
        "   - Visualização em tempo real\n",
        "   - Relatórios automáticos\n",
        "\n",
        "4. **Aplicações Práticas**\n",
        "   - Casos de uso reais\n",
        "   - Vantagens e limitações\n",
        "   - Desafios de implementação\n",
        "   - Futuro da tecnologia\n",
        "\n",
        "### Demonstrações Práticas\n",
        "\n",
        "**1. Sistema Completo:**\n",
        "   - Análise multimodal\n",
        "   - Visualização integrada\n",
        "   - Relatórios automáticos\n",
        "   - Métricas de performance\n",
        "\n",
        "**2. Interface Web:**\n",
        "   - Dashboard Streamlit\n",
        "   - Upload interativo\n",
        "   - Visualização em tempo real\n",
        "   - Exportação de resultados\n",
        "\n",
        "### Conclusão do Curso\n",
        "\n",
        "**Conceitos Aprendidos:**\n",
        "- **Fundamentos**: Processamento digital de imagem\n",
        "- **Deep Learning**: CNNs e arquiteturas modernas\n",
        "- **Técnicas Avançadas**: Transfer learning, GANs, VAEs\n",
        "- **Transformers**: Vision Transformers e atenção\n",
        "- **Foundation Models**: CLIP, DALL-E, GPT-4V\n",
        "- **Aplicações**: Casos de uso práticos\n",
        "\n",
        "**Habilidades Desenvolvidas:**\n",
        "- **Implementação**: Código prático e funcional\n",
        "- **Análise**: Interpretação de resultados\n",
        "- **Integração**: Combinação de técnicas\n",
        "- **Visualização**: Apresentação de dados\n",
        "- **Deploy**: Implementação em produção\n",
        "\n",
        "### Próximos Passos\n",
        "\n",
        "**Para Continuar Aprendendo:**\n",
        "1. **Projetos Práticos**: Implementar projetos reais\n",
        "2. **Datasets**: Trabalhar com datasets públicos\n",
        "3. **Competições**: Participar de competições\n",
        "4. **Pesquisa**: Acompanhar papers recentes\n",
        "5. **Comunidade**: Participar da comunidade\n",
        "\n",
        "### Referências Principais\n",
        "\n",
        "- [Deep Learning for Computer Vision - Goodfellow et al.](https://www.deeplearningbook.org/)\n",
        "- [Computer Vision: Algorithms and Applications - Szeliski](https://szeliski.org/Book/)\n",
        "- [Attention Is All You Need - Vaswani et al.](https://arxiv.org/abs/1706.03762)\n",
        "\n",
        "---\n",
        "\n",
        "**🎉 Parabéns! Você concluiu o curso de Visão Computacional!**\n",
        "\n",
        "**Obrigado por acompanhar este curso completo sobre Visão Computacional e Deep Learning!**"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}