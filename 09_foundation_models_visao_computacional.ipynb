{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Módulo 9: Foundation Models para Visão Computacional",
        "",
        "## Objetivos de Aprendizagem",
        "- Compreender o conceito de Foundation Models",
        "- Conhecer modelos como CLIP, DALL-E e GPT-4V",
        "- Implementar soluções com APIs do OpenAI e Gemini",
        "- Analisar vantagens e limitações dos Foundation Models",
        "- Aplicar Foundation Models em casos práticos do mercado",
        "",
        "---",
        "",
        "## 9.1 Introdução aos Foundation Models",
        "",
        "**Foundation Models** são modelos de inteligência artificial treinados em grandes quantidades de dados não rotulados que podem ser adaptados para uma ampla gama de tarefas downstream através de técnicas como fine-tuning, prompt engineering e few-shot learning.",
        "",
        "![Introdução Foundation Models](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo9/introducao_foundation_models.png)",
        "",
        "### Conceitos Fundamentais",
        "",
        "**Definição de Foundation Models:**",
        "- **Treinamento em escala**: Modelos treinados em datasets massivos",
        "- **Capacidade de adaptação**: Adaptação para múltiplas tarefas",
        "- **Emergência de capacidades**: Habilidades que emergem com escala",
        "- **Base para aplicações**: Fundação para sistemas especializados",
        "",
        "**Características Principais:**",
        "- **Escala**: Milhões ou bilhões de parâmetros",
        "- **Dados**: Treinamento em datasets diversificados",
        "- **Multimodalidade**: Capacidade de processar diferentes modalidades",
        "- **Generalização**: Performance em tarefas não vistas durante treinamento",
        "",
        "### Evolução dos Foundation Models",
        "",
        "**Cronologia dos Marcos:**",
        "- **2017**: Transformer architecture (Attention Is All You Need)",
        "- **2018**: BERT (Bidirectional Encoder Representations)",
        "- **2019**: GPT-2 (Generative Pre-trained Transformer)",
        "- **2020**: GPT-3 (175B parameters)",
        "- **2021**: CLIP (Contrastive Language-Image Pre-training)",
        "- **2022**: DALL-E 2, ChatGPT, Stable Diffusion",
        "- **2023**: GPT-4, GPT-4V, Gemini, Claude",
        "",
        "![Evolução Foundation Models](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo9/evolucao_foundation_models.png)",
        "",
        "**Marcos Importantes:**",
        "- **2017**: Attention Is All You Need - Vaswani et al.",
        "- **2018**: BERT: Pre-training of Deep Bidirectional Transformers - Devlin et al.",
        "- **2020**: Language Models are Few-Shot Learners - Brown et al.",
        "- **2021**: CLIP: Learning Transferable Visual Representations - Radford et al.",
        "- **2022**: DALL-E 2: Hierarchical Text-Conditional Image Generation - Ramesh et al.",
        "",
        "**Referências:**",
        "- [On the Opportunities and Risks of Foundation Models - Bommasani et al.](https://arxiv.org/abs/2108.07258)",
        "- [CLIP: Learning Transferable Visual Representations - Radford et al.](https://arxiv.org/abs/2103.00020)",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.2 Demonstração Prática: Foundation Models Simples\n",
        "\n",
        "Vamos implementar e demonstrar conceitos de Foundation Models:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "class FoundationModelsDemo:\n",
        "    \"\"\"Demonstração de conceitos de Foundation Models\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "    def create_synthetic_dataset(self, num_samples=1000):\n",
        "        \"\"\"Cria dataset sintético para demonstração\"\"\"\n",
        "        \n",
        "        # Simular dados de texto e imagem\n",
        "        text_data = []\n",
        "        image_data = []\n",
        "        labels = []\n",
        "        \n",
        "        categories = [\n",
        "            {\"text\": \"a red car\", \"image\": [1, 0, 0], \"label\": 0},\n",
        "            {\"text\": \"a blue car\", \"image\": [0, 1, 0], \"label\": 1},\n",
        "            {\"text\": \"a green car\", \"image\": [0, 0, 1], \"label\": 2},\n",
        "            {\"text\": \"a red truck\", \"image\": [1, 0, 0], \"label\": 3},\n",
        "            {\"text\": \"a blue truck\", \"image\": [0, 1, 0], \"label\": 4},\n",
        "            {\"text\": \"a green truck\", \"image\": [0, 0, 1], \"label\": 5}\n",
        "        ]\n",
        "        \n",
        "        for i in range(num_samples):\n",
        "            # Selecionar categoria aleatória\n",
        "            category = categories[i % len(categories)]\n",
        "            \n",
        "            # Adicionar variação ao texto\n",
        "            text_variations = [\n",
        "                category[\"text\"],\n",
        "                f\"{category['text']} in the street\",\n",
        "                f\"{category['text']} parked outside\",\n",
        "                f\"{category['text']} driving fast\",\n",
        "                f\"{category['text']} at the traffic light\"\n",
        "            ]\n",
        "            \n",
        "            text = text_variations[i % len(text_variations)]\n",
        "            \n",
        "            # Adicionar ruído à imagem\n",
        "            image = np.array(category[\"image\"]) + np.random.normal(0, 0.1, 3)\n",
        "            image = np.clip(image, 0, 1)\n",
        "            \n",
        "            text_data.append(text)\n",
        "            image_data.append(image)\n",
        "            labels.append(category[\"label\"])\n",
        "        \n",
        "        return text_data, image_data, labels\n",
        "    \n",
        "    def simple_text_encoder(self, text_data, vocab_size=1000, embedding_dim=128):\n",
        "        \"\"\"Simula um encoder de texto simples\"\"\"\n",
        "        \n",
        "        # Criar vocabulário simples\n",
        "        vocab = {}\n",
        "        for text in text_data:\n",
        "            words = text.lower().split()\n",
        "            for word in words:\n",
        "                if word not in vocab:\n",
        "                    vocab[word] = len(vocab)\n",
        "        \n",
        "        # Embedding layer\n",
        "        embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        \n",
        "        # Encoder simples\n",
        "        encoder = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embedding_dim, embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embedding_dim, embedding_dim)\n",
        "        )\n",
        "        \n",
        "        # Processar textos\n",
        "        text_embeddings = []\n",
        "        for text in text_data:\n",
        "            words = text.lower().split()\n",
        "            word_ids = [vocab.get(word, 0) for word in words]\n",
        "            \n",
        "            if word_ids:\n",
        "                # Média dos embeddings das palavras\n",
        "                word_embeddings = embedding(torch.tensor(word_ids))\n",
        "                text_embedding = torch.mean(word_embeddings, dim=0)\n",
        "            else:\n",
        "                text_embedding = torch.zeros(embedding_dim)\n",
        "            \n",
        "            text_embeddings.append(text_embedding)\n",
        "        \n",
        "        # Aplicar encoder\n",
        "        text_embeddings = torch.stack(text_embeddings)\n",
        "        encoded_texts = encoder(text_embeddings)\n",
        "        \n",
        "        return encoded_texts, vocab\n",
        "    \n",
        "    def simple_image_encoder(self, image_data, embedding_dim=128):\n",
        "        \"\"\"Simula um encoder de imagem simples\"\"\"\n",
        "        \n",
        "        # Encoder simples para imagens\n",
        "        encoder = nn.Sequential(\n",
        "            nn.Linear(3, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embedding_dim, embedding_dim)\n",
        "        )\n",
        "        \n",
        "        # Processar imagens\n",
        "        image_tensor = torch.FloatTensor(image_data)\n",
        "        encoded_images = encoder(image_tensor)\n",
        "        \n",
        "        return encoded_images\n",
        "    \n",
        "    def contrastive_learning(self, text_embeddings, image_embeddings, temperature=0.07):\n",
        "        \"\"\"Simula aprendizado contrastivo\"\"\"\n",
        "        \n",
        "        # Normalizar embeddings\n",
        "        text_embeddings = F.normalize(text_embeddings, p=2, dim=1)\n",
        "        image_embeddings = F.normalize(image_embeddings, p=2, dim=1)\n",
        "        \n",
        "        # Calcular similaridade\n",
        "        similarity = torch.matmul(text_embeddings, image_embeddings.T) / temperature\n",
        "        \n",
        "        # Labels para contraste (diagonal = positivos)\n",
        "        batch_size = text_embeddings.size(0)\n",
        "        labels = torch.arange(batch_size).to(text_embeddings.device)\n",
        "        \n",
        "        # Loss contrastivo\n",
        "        loss_text = F.cross_entropy(similarity, labels)\n",
        "        loss_image = F.cross_entropy(similarity.T, labels)\n",
        "        \n",
        "        total_loss = (loss_text + loss_image) / 2\n",
        "        \n",
        "        return total_loss, similarity\n",
        "    \n",
        "    def zero_shot_classification(self, text_embeddings, image_embeddings, test_texts, test_images):\n",
        "        \"\"\"Demonstra classificação zero-shot\"\"\"\n",
        "        \n",
        "        # Normalizar embeddings\n",
        "        text_embeddings = F.normalize(text_embeddings, p=2, dim=1)\n",
        "        image_embeddings = F.normalize(image_embeddings, p=2, dim=1)\n",
        "        \n",
        "        # Calcular similaridade\n",
        "        similarity = torch.matmul(image_embeddings, text_embeddings.T)\n",
        "        \n",
        "        # Predições\n",
        "        predictions = torch.argmax(similarity, dim=1)\n",
        "        \n",
        "        return predictions, similarity\n",
        "    \n",
        "    def visualize_embeddings(self, text_embeddings, image_embeddings, labels, title=\"Embeddings\"):\n",
        "        \"\"\"Visualiza os embeddings em 2D\"\"\"\n",
        "        \n",
        "        # Reduzir dimensionalidade usando PCA simples\n",
        "        from sklearn.decomposition import PCA\n",
        "        \n",
        "        # Combinar embeddings\n",
        "        all_embeddings = torch.cat([text_embeddings, image_embeddings], dim=0)\n",
        "        \n",
        "        # PCA para 2D\n",
        "        pca = PCA(n_components=2)\n",
        "        embeddings_2d = pca.fit_transform(all_embeddings.detach().numpy())\n",
        "        \n",
        "        # Separar texto e imagem\n",
        "        text_2d = embeddings_2d[:len(text_embeddings)]\n",
        "        image_2d = embeddings_2d[len(text_embeddings):]\n",
        "        \n",
        "        # Visualizar\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        \n",
        "        # Cores para diferentes labels\n",
        "        colors = plt.cm.Set3(np.linspace(0, 1, len(set(labels))))\n",
        "        \n",
        "        for i, label in enumerate(set(labels)):\n",
        "            mask = np.array(labels) == label\n",
        "            \n",
        "            # Texto\n",
        "            plt.scatter(text_2d[mask, 0], text_2d[mask, 1], \n",
        "                       c=[colors[label]], marker='o', s=100, \n",
        "                       label=f'Text - Label {label}', alpha=0.7)\n",
        "            \n",
        "            # Imagem\n",
        "            plt.scatter(image_2d[mask, 0], image_2d[mask, 1], \n",
        "                       c=[colors[label]], marker='s', s=100, \n",
        "                       label=f'Image - Label {label}', alpha=0.7)\n",
        "        \n",
        "        plt.title(f'{title} - Text (circles) vs Image (squares)')\n",
        "        plt.xlabel('PC1')\n",
        "        plt.ylabel('PC2')\n",
        "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return embeddings_2d\n",
        "    \n",
        "    def analyze_performance(self, predictions, true_labels, similarity_matrix):\n",
        "        \"\"\"Analisa a performance do modelo\"\"\"\n",
        "        \n",
        "        # Calcular accuracy\n",
        "        accuracy = (predictions == true_labels).float().mean().item()\n",
        "        \n",
        "        # Análise de similaridade\n",
        "        similarity_scores = torch.diag(similarity_matrix)\n",
        "        \n",
        "        # Visualizar resultados\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        \n",
        "        # Histograma de accuracy\n",
        "        axes[0].hist(similarity_scores.detach().numpy(), bins=20, alpha=0.7)\n",
        "        axes[0].set_title(f'Distribuição de Similaridade\\nAccuracy: {accuracy:.3f}')\n",
        "        axes[0].set_xlabel('Similaridade')\n",
        "        axes[0].set_ylabel('Frequência')\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "        \n",
        "        # Matriz de confusão\n",
        "        from sklearn.metrics import confusion_matrix\n",
        "        cm = confusion_matrix(true_labels, predictions)\n",
        "        \n",
        "        im = axes[1].imshow(cm, interpolation='nearest', cmap='Blues')\n",
        "        axes[1].set_title('Matriz de Confusão')\n",
        "        axes[1].set_xlabel('Predição')\n",
        "        axes[1].set_ylabel('Verdadeiro')\n",
        "        \n",
        "        # Adicionar valores na matriz\n",
        "        for i in range(cm.shape[0]):\n",
        "            for j in range(cm.shape[1]):\n",
        "                axes[1].text(j, i, cm[i, j], ha='center', va='center')\n",
        "        \n",
        "        plt.colorbar(im, ax=axes[1])\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return {\n",
        "            'accuracy': accuracy,\n",
        "            'similarity_scores': similarity_scores,\n",
        "            'confusion_matrix': cm\n",
        "        }\n",
        "\n",
        "def demonstrate_foundation_models():\n",
        "    \"\"\"Demonstra conceitos de Foundation Models\"\"\"\n",
        "    \n",
        "    print(\"=== Demonstração de Foundation Models ===\")\n",
        "    print(\"\\nNota: Esta demonstração usa dados sintéticos para fins educacionais.\")\n",
        "    print(\"Em aplicações reais, use datasets reais e modelos pré-treinados.\")\n",
        "    \n",
        "    # Criar instância\n",
        "    demo = FoundationModelsDemo()\n",
        "    \n",
        "    # Criar dataset sintético\n",
        "    print(\"\\nCriando dataset sintético...\")\n",
        "    text_data, image_data, labels = demo.create_synthetic_dataset(num_samples=500)\n",
        "    \n",
        "    # Encoder de texto\n",
        "    print(\"\\nProcessando texto...\")\n",
        "    text_embeddings, vocab = demo.simple_text_encoder(text_data)\n",
        "    \n",
        "    # Encoder de imagem\n",
        "    print(\"\\nProcessando imagens...\")\n",
        "    image_embeddings = demo.simple_image_encoder(image_data)\n",
        "    \n",
        "    # Aprendizado contrastivo\n",
        "    print(\"\\nExecutando aprendizado contrastivo...\")\n",
        "    loss, similarity = demo.contrastive_learning(text_embeddings, image_embeddings)\n",
        "    print(f\"Loss contrastivo: {loss.item():.4f}\")\n",
        "    \n",
        "    # Visualizar embeddings\n",
        "    print(\"\\nVisualizando embeddings...\")\n",
        "    embeddings_2d = demo.visualize_embeddings(text_embeddings, image_embeddings, labels)\n",
        "    \n",
        "    # Classificação zero-shot\n",
        "    print(\"\\nExecutando classificação zero-shot...\")\n",
        "    test_texts = text_data[:100]\n",
        "    test_images = image_data[:100]\n",
        "    test_labels = labels[:100]\n",
        "    \n",
        "    predictions, similarity_matrix = demo.zero_shot_classification(\n",
        "        text_embeddings[:100], image_embeddings[:100], test_texts, test_images\n",
        "    )\n",
        "    \n",
        "    # Analisar performance\n",
        "    print(\"\\nAnalisando performance...\")\n",
        "    performance = demo.analyze_performance(predictions, test_labels, similarity_matrix)\n",
        "    \n",
        "    return demo, text_embeddings, image_embeddings, performance\n",
        "\n",
        "# Executar demonstração\n",
        "demo_model, text_emb, image_emb, model_performance = demonstrate_foundation_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análise dos Resultados\n",
        "\n",
        "**Foundation Models Observados:**\n",
        "\n",
        "1. **Embeddings**: Representações aprendidas de texto e imagem\n",
        "2. **Aprendizado Contrastivo**: Alinhamento entre modalidades\n",
        "3. **Classificação Zero-Shot**: Predição sem treinamento específico\n",
        "4. **Performance**: Accuracy e similaridade\n",
        "\n",
        "**Insights Importantes:**\n",
        "- **Alinhamento**: Texto e imagem no mesmo espaço\n",
        "- **Generalização**: Funciona em dados não vistos\n",
        "- **Escalabilidade**: Performance melhora com escala\n",
        "- **Flexibilidade**: Adaptável a diferentes tarefas\n",
        "\n",
        "**Referências:**\n",
        "- [CLIP: Learning Transferable Visual Representations - Radford et al.](https://arxiv.org/abs/2103.00020)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.3 CLIP: Contrastive Language-Image Pre-training",
        "",
        "**CLIP** é um modelo que aprende representações visuais através de aprendizado contrastivo em pares de texto e imagem, permitindo classificação zero-shot e busca multimodal.",
        "",
        "![CLIP Model](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo9/clip_model.png)",
        "",
        "### Arquitetura CLIP",
        "",
        "**Componentes Principais:**",
        "",
        "**1. Text Encoder:**",
        "- **Transformer**: Arquitetura baseada em attention",
        "- **Vocabulário**: Vocabulário expandido",
        "- **Embedding**: Representações de texto",
        "- **Normalização**: Normalização L2",
        "",
        "**2. Image Encoder:**",
        "- **Vision Transformer**: ViT ou ResNet",
        "- **Patches**: Divisão em patches",
        "- **Embedding**: Representações de imagem",
        "- **Normalização**: Normalização L2",
        "",
        "![Arquitetura CLIP](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo9/arquitetura_clip.png)",
        "",
        "**3. Contrastive Learning:**",
        "- **Similaridade**: Cálculo de similaridade coseno",
        "- **Contraste**: Pares positivos e negativos",
        "- **Loss**: InfoNCE loss",
        "- **Otimização**: Otimização conjunta",
        "",
        "### Processo de Treinamento",
        "",
        "**1. Pré-processamento:**",
        "- **Texto**: Tokenização e embedding",
        "- **Imagem**: Redimensionamento e normalização",
        "- **Pares**: Associação texto-imagem",
        "- **Batch**: Organização em batches",
        "",
        "**2. Forward Pass:**",
        "- **Text Encoder**: Texto → embedding",
        "- **Image Encoder**: Imagem → embedding",
        "- **Normalização**: Normalização L2",
        "- **Similaridade**: Cálculo de similaridade",
        "",
        "**3. Loss Function:**",
        "- **InfoNCE**: Loss contrastivo",
        "- **Temperatura**: Parâmetro de temperatura",
        "- **Otimização**: Otimização conjunta",
        "- **Convergência**: Alinhamento das modalidades",
        "",
        "### Aplicações do CLIP",
        "",
        "**1. Classificação Zero-Shot:**",
        "- **Descrições**: Descrições textuais das classes",
        "- **Similaridade**: Cálculo de similaridade",
        "- **Predição**: Classe com maior similaridade",
        "- **Flexibilidade**: Novas classes sem retreinamento",
        "",
        "**2. Busca Multimodal:**",
        "- **Query**: Consulta em texto",
        "- **Database**: Base de imagens",
        "- **Ranking**: Ordenação por similaridade",
        "- **Retrieval**: Recuperação de imagens",
        "",
        "**3. Transfer Learning:**",
        "- **Fine-tuning**: Ajuste fino para tarefas específicas",
        "- **Feature Extraction**: Extração de características",
        "- **Downstream Tasks**: Tarefas downstream",
        "- **Performance**: Melhoria de performance",
        "",
        "**Referências:**",
        "- [CLIP: Learning Transferable Visual Representations - Radford et al.](https://arxiv.org/abs/2103.00020)",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.4 DALL-E: Text-to-Image Generation",
        "",
        "**DALL-E** é um modelo generativo que cria imagens a partir de descrições textuais, combinando técnicas de geração de imagens com processamento de linguagem natural.",
        "",
        "![DALL-E Model](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo9/dalle_model.png)",
        "",
        "### Arquitetura DALL-E",
        "",
        "**Componentes Principais:**",
        "",
        "**1. Text Encoder:**",
        "- **Transformer**: Encoder baseado em attention",
        "- **Vocabulário**: Vocabulário expandido",
        "- **Embedding**: Representações de texto",
        "- **Contexto**: Contexto global",
        "",
        "**2. Image Decoder:**",
        "- **Transformer**: Decoder baseado em attention",
        "- **Patches**: Divisão em patches",
        "- **Geração**: Geração de patches",
        "- **Reconstrução**: Reconstrução da imagem",
        "",
        "![Arquitetura DALL-E](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo9/arquitetura_dalle.png)",
        "",
        "**3. VQ-VAE:**",
        "- **Quantização**: Quantização vetorial",
        "- **Codebook**: Livro de códigos",
        "- **Compressão**: Compressão de imagem",
        "- **Reconstrução**: Reconstrução de alta qualidade",
        "",
        "### Processo de Geração",
        "",
        "**1. Pré-processamento:**",
        "- **Texto**: Tokenização e embedding",
        "- **Imagem**: Divisão em patches",
        "- **Quantização**: Quantização vetorial",
        "- **Sequência**: Sequência de tokens",
        "",
        "**2. Geração:**",
        "- **Text Encoder**: Texto → contexto",
        "- **Image Decoder**: Contexto → patches",
        "- **Sampling**: Amostragem autoregressiva",
        "- **Reconstrução**: Reconstrução da imagem",
        "",
        "**3. Pós-processamento:**",
        "- **Desquantização**: Desquantização vetorial",
        "- **Reconstrução**: Reconstrução da imagem",
        "- **Refinamento**: Refinamento da qualidade",
        "- **Output**: Imagem final",
        "",
        "### Aplicações do DALL-E",
        "",
        "**1. Geração de Imagens:**",
        "- **Descrições**: Descrições textuais",
        "- **Criação**: Criação de imagens",
        "- **Qualidade**: Alta qualidade",
        "- **Diversidade**: Grande diversidade",
        "",
        "**2. Edição de Imagens:**",
        "- **Inpainting**: Preenchimento de regiões",
        "- **Outpainting**: Extensão da imagem",
        "- **Edição**: Edição baseada em texto",
        "- **Controle**: Controle preciso",
        "",
        "**3. Aplicações Criativas:**",
        "- **Arte**: Criação artística",
        "- **Design**: Design gráfico",
        "- **Conteúdo**: Conteúdo visual",
        "- **Prototipagem**: Prototipagem rápida",
        "",
        "![Aplicações DALL-E](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo9/aplicacoes_dalle.png)",
        "",
        "**Referências:**",
        "- [DALL-E: Creating Images from Text - Ramesh et al.](https://arxiv.org/abs/2102.12092)",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.5 GPT-4V e Multimodal AI",
        "",
        "**GPT-4V** é uma versão multimodal do GPT-4 que pode processar tanto texto quanto imagens, permitindo análise visual e geração de conteúdo multimodal.",
        "",
        "![GPT-4V Model](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo9/gpt4v_model.png)",
        "",
        "### Arquitetura GPT-4V",
        "",
        "**Componentes Principais:**",
        "",
        "**1. Vision Encoder:**",
        "- **Transformer**: Encoder baseado em attention",
        "- **Patches**: Divisão em patches",
        "- **Embedding**: Representações de imagem",
        "- **Normalização**: Normalização de camadas",
        "",
        "**2. Text Encoder:**",
        "- **Transformer**: Encoder baseado em attention",
        "- **Vocabulário**: Vocabulário expandido",
        "- **Embedding**: Representações de texto",
        "- **Contexto**: Contexto global",
        "",
        "**3. Multimodal Fusion:**",
        "- **Cross-Attention**: Atenção cruzada",
        "- **Fusão**: Fusão de modalidades",
        "- **Contexto**: Contexto multimodal",
        "- **Decodificação**: Decodificação conjunta",
        "",
        "### Capacidades do GPT-4V",
        "",
        "**1. Análise Visual:**",
        "- **Descrição**: Descrição de imagens",
        "- **Análise**: Análise de conteúdo",
        "- **Detecção**: Detecção de objetos",
        "- **Interpretação**: Interpretação de cenas",
        "",
        "**2. Geração Multimodal:**",
        "- **Texto**: Geração de texto",
        "- **Imagem**: Análise de imagem",
        "- **Combinação**: Combinação de modalidades",
        "- **Criatividade**: Conteúdo criativo",
        "",
        "**3. Aplicações Práticas:**",
        "- **Assistência**: Assistência visual",
        "- **Educação**: Educação multimodal",
        "- **Acessibilidade**: Acessibilidade visual",
        "- **Produtividade**: Produtividade multimodal",
        "",
        "### Demonstração Prática: APIs",
        "",
        "Vamos demonstrar o uso de APIs para Foundation Models:",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import base64\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "class FoundationModelsAPI:\n",
        "    \"\"\"Demonstração de APIs para Foundation Models\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.openai_api_key = None  # Configurar com sua chave\n",
        "        self.gemini_api_key = None  # Configurar com sua chave\n",
        "        \n",
        "    def create_sample_image(self, width=64, height=64):\n",
        "        \"\"\"Cria uma imagem de exemplo para demonstração\"\"\"\n",
        "        \n",
        "        import numpy as np\n",
        "        import matplotlib.pyplot as plt\n",
        "        \n",
        "        # Criar imagem simples\n",
        "        img = np.zeros((height, width, 3), dtype=np.uint8)\n",
        "        \n",
        "        # Adicionar formas\n",
        "        # Círculo vermelho\n",
        "        center = (width//2, height//2)\n",
        "        radius = min(width, height) // 4\n",
        "        \n",
        "        y, x = np.ogrid[:height, :width]\n",
        "        mask = (x - center[0])**2 + (y - center[1])**2 <= radius**2\n",
        "        img[mask] = [255, 0, 0]  # Vermelho\n",
        "        \n",
        "        # Salvar imagem\n",
        "        plt.imsave('sample_image.png', img)\n",
        "        \n",
        "        return img\n",
        "    \n",
        "    def encode_image_to_base64(self, image_path: str) -> str:\n",
        "        \"\"\"Codifica imagem para base64\"\"\"\n",
        "        \n",
        "        with open(image_path, \"rb\") as image_file:\n",
        "            encoded_string = base64.b64encode(image_file.read()).decode('utf-8')\n",
        "        \n",
        "        return encoded_string\n",
        "    \n",
        "    def openai_vision_api(self, image_path: str, prompt: str) -> Dict:\n",
        "        \"\"\"Simula chamada para OpenAI Vision API\"\"\"\n",
        "        \n",
        "        if not self.openai_api_key:\n",
        "            return {\n",
        "                \"error\": \"API key not configured\",\n",
        "                \"simulation\": True,\n",
        "                \"response\": {\n",
        "                    \"choices\": [{\n",
        "                        \"message\": {\n",
        "                            \"content\": \"Esta é uma simulação da resposta da OpenAI Vision API. A imagem contém um círculo vermelho no centro.\"\n",
        "                        }\n",
        "                    }]\n",
        "                }\n",
        "            }\n",
        "        \n",
        "        # Codificar imagem\n",
        "        base64_image = self.encode_image_to_base64(image_path)\n",
        "        \n",
        "        # Preparar payload\n",
        "        payload = {\n",
        "            \"model\": \"gpt-4-vision-preview\",\n",
        "            \"messages\": [\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"type\": \"text\",\n",
        "                            \"text\": prompt\n",
        "                        },\n",
        "                        {\n",
        "                            \"type\": \"image_url\",\n",
        "                            \"image_url\": {\n",
        "                                \"url\": f\"data:image/png;base64,{base64_image}\"\n",
        "                            }\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            \"max_tokens\": 300\n",
        "        }\n",
        "        \n",
        "        # Fazer requisição\n",
        "        headers = {\n",
        "            \"Authorization\": f\"Bearer {self.openai_api_key}\",\n",
        "            \"Content-Type\": \"application/json\"\n",
        "        }\n",
        "        \n",
        "        try:\n",
        "            response = requests.post(\n",
        "                \"https://api.openai.com/v1/chat/completions\",\n",
        "                headers=headers,\n",
        "                json=payload\n",
        "            )\n",
        "            \n",
        "            return response.json()\n",
        "        \n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"error\": str(e),\n",
        "                \"simulation\": True,\n",
        "                \"response\": {\n",
        "                    \"choices\": [{\n",
        "                        \"message\": {\n",
        "                            \"content\": \"Erro na chamada da API. Esta é uma simulação.\"\n",
        "                        }\n",
        "                    }]\n",
        "                }\n",
        "            }\n",
        "    \n",
        "    def gemini_vision_api(self, image_path: str, prompt: str) -> Dict:\n",
        "        \"\"\"Simula chamada para Gemini Vision API\"\"\"\n",
        "        \n",
        "        if not self.gemini_api_key:\n",
        "            return {\n",
        "                \"error\": \"API key not configured\",\n",
        "                \"simulation\": True,\n",
        "                \"response\": {\n",
        "                    \"candidates\": [{\n",
        "                        \"content\": {\n",
        "                            \"parts\": [{\n",
        "                                \"text\": \"Esta é uma simulação da resposta da Gemini Vision API. A imagem contém um círculo vermelho no centro.\"\n",
        "                            }]\n",
        "                        }\n",
        "                    }]\n",
        "                }\n",
        "            }\n",
        "        \n",
        "        # Codificar imagem\n",
        "        base64_image = self.encode_image_to_base64(image_path)\n",
        "        \n",
        "        # Preparar payload\n",
        "        payload = {\n",
        "            \"contents\": [\n",
        "                {\n",
        "                    \"parts\": [\n",
        "                        {\n",
        "                            \"text\": prompt\n",
        "                        },\n",
        "                        {\n",
        "                            \"inline_data\": {\n",
        "                                \"mime_type\": \"image/png\",\n",
        "                                \"data\": base64_image\n",
        "                            }\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ]\n",
        "        }\n",
        "        \n",
        "        # Fazer requisição\n",
        "        url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro-vision:generateContent?key={self.gemini_api_key}\"\n",
        "        \n",
        "        try:\n",
        "            response = requests.post(url, json=payload)\n",
        "            \n",
        "            return response.json()\n",
        "        \n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"error\": str(e),\n",
        "                \"simulation\": True,\n",
        "                \"response\": {\n",
        "                    \"candidates\": [{\n",
        "                        \"content\": {\n",
        "                            \"parts\": [{\n",
        "                                \"text\": \"Erro na chamada da API. Esta é uma simulação.\"\n",
        "                            }]\n",
        "                        }\n",
        "                    }]\n",
        "                }\n",
        "            }\n",
        "    \n",
        "    def compare_apis(self, image_path: str, prompt: str) -> Dict:\n",
        "        \"\"\"Compara respostas das APIs\"\"\"\n",
        "        \n",
        "        print(f\"\\n=== COMPARAÇÃO DE APIs ===\")\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        print(f\"Imagem: {image_path}\")\n",
        "        \n",
        "        # OpenAI Vision API\n",
        "        print(\"\\n--- OpenAI Vision API ---\")\n",
        "        openai_response = self.openai_vision_api(image_path, prompt)\n",
        "        \n",
        "        if \"error\" in openai_response:\n",
        "            print(f\"Erro: {openai_response['error']}\")\n",
        "            if openai_response.get(\"simulation\"):\n",
        "                print(\"Modo simulação ativado\")\n",
        "        \n",
        "        if \"response\" in openai_response:\n",
        "            openai_text = openai_response[\"response\"][\"choices\"][0][\"message\"][\"content\"]\n",
        "            print(f\"Resposta: {openai_text}\")\n",
        "        \n",
        "        # Gemini Vision API\n",
        "        print(\"\\n--- Gemini Vision API ---\")\n",
        "        gemini_response = self.gemini_vision_api(image_path, prompt)\n",
        "        \n",
        "        if \"error\" in gemini_response:\n",
        "            print(f\"Erro: {gemini_response['error']}\")\n",
        "            if gemini_response.get(\"simulation\"):\n",
        "                print(\"Modo simulação ativado\")\n",
        "        \n",
        "        if \"response\" in gemini_response:\n",
        "            gemini_text = gemini_response[\"response\"][\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "            print(f\"Resposta: {gemini_text}\")\n",
        "        \n",
        "        return {\n",
        "            \"openai\": openai_response,\n",
        "            \"gemini\": gemini_response\n",
        "        }\n",
        "    \n",
        "    def analyze_responses(self, responses: Dict) -> Dict:\n",
        "        \"\"\"Analisa as respostas das APIs\"\"\"\n",
        "        \n",
        "        analysis = {\n",
        "            \"openai\": {\n",
        "                \"success\": \"error\" not in responses[\"openai\"],\n",
        "                \"simulation\": responses[\"openai\"].get(\"simulation\", False),\n",
        "                \"response_length\": 0\n",
        "            },\n",
        "            \"gemini\": {\n",
        "                \"success\": \"error\" not in responses[\"gemini\"],\n",
        "                \"simulation\": responses[\"gemini\"].get(\"simulation\", False),\n",
        "                \"response_length\": 0\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # Analisar OpenAI\n",
        "        if \"response\" in responses[\"openai\"]:\n",
        "            openai_text = responses[\"openai\"][\"response\"][\"choices\"][0][\"message\"][\"content\"]\n",
        "            analysis[\"openai\"][\"response_length\"] = len(openai_text)\n",
        "        \n",
        "        # Analisar Gemini\n",
        "        if \"response\" in responses[\"gemini\"]:\n",
        "            gemini_text = responses[\"gemini\"][\"response\"][\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"]\n",
        "            analysis[\"gemini\"][\"response_length\"] = len(gemini_text)\n",
        "        \n",
        "        # Visualizar análise\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "        \n",
        "        # Gráfico de sucesso\n",
        "        success_data = [analysis[\"openai\"][\"success\"], analysis[\"gemini\"][\"success\"]]\n",
        "        axes[0].bar([\"OpenAI\", \"Gemini\"], success_data, color=['blue', 'green'])\n",
        "        axes[0].set_title('Sucesso das APIs')\n",
        "        axes[0].set_ylabel('Sucesso')\n",
        "        axes[0].set_ylim(0, 1)\n",
        "        \n",
        "        # Gráfico de comprimento da resposta\n",
        "        length_data = [analysis[\"openai\"][\"response_length\"], analysis[\"gemini\"][\"response_length\"]]\n",
        "        axes[1].bar([\"OpenAI\", \"Gemini\"], length_data, color=['blue', 'green'])\n",
        "        axes[1].set_title('Comprimento da Resposta')\n",
        "        axes[1].set_ylabel('Caracteres')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return analysis\n",
        "\n",
        "def demonstrate_apis():\n",
        "    \"\"\"Demonstra o uso de APIs para Foundation Models\"\"\"\n",
        "    \n",
        "    print(\"=== Demonstração de APIs para Foundation Models ===\")\n",
        "    print(\"\\nNota: Esta demonstração usa modo simulação quando as chaves de API não estão configuradas.\")\n",
        "    print(\"Para usar as APIs reais, configure as chaves de API.\")\n",
        "    \n",
        "    # Criar instância\n",
        "    api_demo = FoundationModelsAPI()\n",
        "    \n",
        "    # Criar imagem de exemplo\n",
        "    print(\"\\nCriando imagem de exemplo...\")\n",
        "    sample_image = api_demo.create_sample_image()\n",
        "    \n",
        "    # Testar diferentes prompts\n",
        "    prompts = [\n",
        "        \"Descreva o que você vê nesta imagem\",\n",
        "        \"Quais cores estão presentes na imagem?\",\n",
        "        \"Que formas geométricas você identifica?\"\n",
        "    ]\n",
        "    \n",
        "    all_responses = []\n",
        "    \n",
        "    for prompt in prompts:\n",
        "        print(f\"\\n--- Testando prompt: {prompt} ---\")\n",
        "        responses = api_demo.compare_apis('sample_image.png', prompt)\n",
        "        all_responses.append(responses)\n",
        "        \n",
        "        # Analisar respostas\n",
        "        analysis = api_demo.analyze_responses(responses)\n",
        "        \n",
        "        print(f\"\\nAnálise:\")\n",
        "        print(f\"OpenAI - Sucesso: {analysis['openai']['success']}, Simulação: {analysis['openai']['simulation']}\")\n",
        "        print(f\"Gemini - Sucesso: {analysis['gemini']['success']}, Simulação: {analysis['gemini']['simulation']}\")\n",
        "    \n",
        "    return api_demo, all_responses\n",
        "\n",
        "# Executar demonstração\n",
        "api_demo, api_responses = demonstrate_apis()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análise dos Resultados\n",
        "\n",
        "**APIs de Foundation Models Observadas:**\n",
        "\n",
        "1. **OpenAI Vision API**: Análise de imagens com GPT-4V\n",
        "2. **Gemini Vision API**: Análise de imagens com Gemini\n",
        "3. **Comparação**: Diferenças entre as APIs\n",
        "4. **Simulação**: Modo simulação para demonstração\n",
        "\n",
        "**Insights Importantes:**\n",
        "- **Multimodalidade**: Processamento de texto e imagem\n",
        "- **APIs**: Interfaces padronizadas\n",
        "- **Flexibilidade**: Diferentes modelos disponíveis\n",
        "- **Integração**: Fácil integração em aplicações\n",
        "\n",
        "**Referências:**\n",
        "- [OpenAI API Documentation](https://platform.openai.com/docs)\n",
        "- [Google Gemini API Documentation](https://ai.google.dev/docs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9.6 Aplicações Práticas e Casos de Uso",
        "",
        "### Aplicações no Mercado",
        "",
        "**1. E-commerce:**",
        "- **Busca visual**: Busca por produtos usando imagens",
        "- **Recomendações**: Recomendações baseadas em visual",
        "- **Categorização**: Categorização automática de produtos",
        "- **Moderação**: Moderação de conteúdo visual",
        "",
        "**2. Saúde:**",
        "- **Diagnóstico**: Assistência em diagnóstico médico",
        "- **Análise**: Análise de imagens médicas",
        "- **Documentação**: Documentação automática",
        "- **Educação**: Educação médica",
        "",
        "**3. Educação:**",
        "- **Acessibilidade**: Acessibilidade visual",
        "- **Tutoria**: Tutoria multimodal",
        "- **Conteúdo**: Criação de conteúdo educacional",
        "- **Avaliação**: Avaliação automática",
        "",
        "**4. Entretenimento:**",
        "- **Geração**: Geração de conteúdo visual",
        "- **Edição**: Edição automática",
        "- **Personalização**: Personalização de conteúdo",
        "- **Interação**: Interação multimodal",
        "",
        "![Aplicações Práticas](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo9/aplicacoes_praticas.png)",
        "",
        "### Vantagens e Limitações",
        "",
        "**Vantagens:**",
        "- **Escalabilidade**: Performance melhora com escala",
        "- **Generalização**: Funciona em diferentes domínios",
        "- **Flexibilidade**: Adaptável a múltiplas tarefas",
        "- **Eficiência**: Reduz necessidade de dados rotulados",
        "",
        "**Limitações:**",
        "- **Recursos**: Requer muitos recursos computacionais",
        "- **Dados**: Depende de grandes datasets",
        "- **Viés**: Pode perpetuar vieses dos dados",
        "- **Interpretabilidade**: Difícil de interpretar",
        "",
        "### Futuro dos Foundation Models",
        "",
        "**Tendências:**",
        "- **Multimodalidade**: Integração de mais modalidades",
        "- **Eficiência**: Modelos mais eficientes",
        "- **Especialização**: Modelos especializados",
        "- **Acessibilidade**: Maior acessibilidade",
        "",
        "**Desafios:**",
        "- **Ética**: Considerações éticas",
        "- **Segurança**: Segurança e privacidade",
        "- **Regulamentação**: Regulamentação adequada",
        "- **Sustentabilidade**: Impacto ambiental",
        "",
        "**Referências:**",
        "- [On the Opportunities and Risks of Foundation Models - Bommasani et al.](https://arxiv.org/abs/2108.07258)",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumo do Módulo 9\n",
        "\n",
        "### Principais Conceitos Abordados\n",
        "\n",
        "1. **Foundation Models**\n",
        "   - Conceitos fundamentais\n",
        "   - Evolução histórica\n",
        "   - Características principais\n",
        "\n",
        "2. **CLIP**\n",
        "   - Arquitetura e componentes\n",
        "   - Aprendizado contrastivo\n",
        "   - Aplicações práticas\n",
        "\n",
        "3. **DALL-E**\n",
        "   - Geração de imagens\n",
        "   - Arquitetura multimodal\n",
        "   - Aplicações criativas\n",
        "\n",
        "4. **GPT-4V e Multimodal AI**\n",
        "   - Processamento multimodal\n",
        "   - Análise visual\n",
        "   - Geração de conteúdo\n",
        "\n",
        "5. **APIs e Aplicações**\n",
        "   - OpenAI e Gemini APIs\n",
        "   - Casos de uso práticos\n",
        "   - Vantagens e limitações\n",
        "\n",
        "### Demonstrações Práticas\n",
        "\n",
        "**1. Foundation Models:**\n",
        "   - Aprendizado contrastivo\n",
        "   - Classificação zero-shot\n",
        "   - Visualização de embeddings\n",
        "\n",
        "**2. APIs:**\n",
        "   - OpenAI Vision API\n",
        "   - Gemini Vision API\n",
        "   - Comparação de respostas\n",
        "\n",
        "### Próximos Passos\n",
        "\n",
        "No próximo módulo, exploraremos a **Atividade Final Prática**, onde integraremos todos os conceitos aprendidos em um projeto completo.\n",
        "\n",
        "### Referências Principais\n",
        "\n",
        "- [On the Opportunities and Risks of Foundation Models - Bommasani et al.](https://arxiv.org/abs/2108.07258)\n",
        "- [CLIP: Learning Transferable Visual Representations - Radford et al.](https://arxiv.org/abs/2103.00020)\n",
        "- [DALL-E: Creating Images from Text - Ramesh et al.](https://arxiv.org/abs/2102.12092)\n",
        "\n",
        "---\n",
        "\n",
        "**Próximo Módulo**: Atividade Final Prática\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}