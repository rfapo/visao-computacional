{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Módulo 7: GANs e VAEs - Geração Sintética de Imagens",
        "",
        "## Objetivos de Aprendizagem",
        "- Compreender os fundamentos de Generative Adversarial Networks (GANs)",
        "- Entender Variational Autoencoders (VAEs) para geração de imagens",
        "- Implementar modelos de geração sintética",
        "- Analisar aplicações práticas de geração de imagens",
        "- Comparar diferentes abordagens de geração",
        "",
        "---",
        "",
        "## 7.1 Introdução à Geração Sintética de Imagens",
        "",
        "**Geração Sintética de Imagens** é o processo de criar imagens artificialmente usando modelos de machine learning, permitindo criar conteúdo visual novo e realista.",
        "",
        "![Introdução Geração Sintética](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo7/introducao_geracao_sintetica.png)",
        "",
        "### Conceitos Fundamentais",
        "",
        "**Definição:**",
        "- **Geração**: Criação de novas imagens",
        "- **Sintética**: Produzida artificialmente",
        "- **Realista**: Visualmente convincente",
        "- **Controlável**: Parâmetros ajustáveis",
        "",
        "**Aplicações:**",
        "- **Arte digital**: Criação artística",
        "- **Data augmentation**: Aumento de datasets",
        "- **Design**: Prototipagem visual",
        "- **Entretenimento**: Conteúdo para jogos/filmes",
        "",
        "### Evolução da Geração Sintética",
        "",
        "**Progressão Histórica:**",
        "- **2014**: GANs introduzidos por Ian Goodfellow",
        "- **2015**: VAEs para geração de imagens",
        "- **2016**: DCGAN para imagens de alta qualidade",
        "- **2017**: Progressive GAN para resolução crescente",
        "- **2018**: StyleGAN para controle de estilo",
        "- **2020**: Diffusion Models revolucionam o campo",
        "- **2022**: DALL-E 2 e Midjourney popularizam",
        "",
        "![Evolução Geração Sintética](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo7/evolucao_geracao_sintetica.png)",
        "",
        "**Marcos Importantes:**",
        "- **2014**: Generative Adversarial Networks - Goodfellow et al.",
        "- **2015**: Auto-Encoding Variational Bayes - Kingma & Welling",
        "- **2016**: Deep Convolutional GANs - Radford et al.",
        "- **2017**: Progressive Growing of GANs - Karras et al.",
        "- **2019**: StyleGAN - Karras et al.",
        "- **2021**: DALL-E - Ramesh et al.",
        "",
        "**Referências:**",
        "- [Generative Adversarial Networks - Goodfellow et al.](https://arxiv.org/abs/1406.2661)",
        "- [Auto-Encoding Variational Bayes - Kingma & Welling](https://arxiv.org/abs/1312.6114)",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.2 Demonstração Prática: GANs Simples\n",
        "\n",
        "Vamos implementar e visualizar um GAN simples para geração de imagens:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class SimpleGAN:\n",
        "    \"\"\"Implementação de um GAN simples para demonstração\"\"\"\n",
        "    \n",
        "    def __init__(self, latent_dim=100, img_size=64):\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_size = img_size\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "        # Inicializar modelos\n",
        "        self.generator = self._build_generator()\n",
        "        self.discriminator = self._build_discriminator()\n",
        "        \n",
        "        # Otimizadores\n",
        "        self.g_optimizer = optim.Adam(self.generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "        self.d_optimizer = optim.Adam(self.discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
        "        \n",
        "        # Loss function\n",
        "        self.criterion = nn.BCELoss()\n",
        "        \n",
        "        # Histórico de treinamento\n",
        "        self.g_losses = []\n",
        "        self.d_losses = []\n",
        "        \n",
        "    def _build_generator(self):\n",
        "        \"\"\"Constrói o gerador\"\"\"\n",
        "        \n",
        "        class Generator(nn.Module):\n",
        "            def __init__(self, latent_dim, img_size):\n",
        "                super(Generator, self).__init__()\n",
        "                \n",
        "                # Camadas fully connected\n",
        "                self.fc1 = nn.Linear(latent_dim, 256)\n",
        "                self.fc2 = nn.Linear(256, 512)\n",
        "                self.fc3 = nn.Linear(512, 1024)\n",
        "                self.fc4 = nn.Linear(1024, img_size * img_size * 3)\n",
        "                \n",
        "                # Dropout\n",
        "                self.dropout = nn.Dropout(0.3)\n",
        "                \n",
        "            def forward(self, x):\n",
        "                x = F.relu(self.fc1(x))\n",
        "                x = self.dropout(x)\n",
        "                x = F.relu(self.fc2(x))\n",
        "                x = self.dropout(x)\n",
        "                x = F.relu(self.fc3(x))\n",
        "                x = self.dropout(x)\n",
        "                x = torch.tanh(self.fc4(x))\n",
        "                \n",
        "                # Reshape para imagem\n",
        "                x = x.view(x.size(0), 3, self.img_size, self.img_size)\n",
        "                return x\n",
        "        \n",
        "        return Generator(self.latent_dim, self.img_size).to(self.device)\n",
        "    \n",
        "    def _build_discriminator(self):\n",
        "        \"\"\"Constrói o discriminador\"\"\"\n",
        "        \n",
        "        class Discriminator(nn.Module):\n",
        "            def __init__(self, img_size):\n",
        "                super(Discriminator, self).__init__()\n",
        "                \n",
        "                # Camadas fully connected\n",
        "                self.fc1 = nn.Linear(img_size * img_size * 3, 1024)\n",
        "                self.fc2 = nn.Linear(1024, 512)\n",
        "                self.fc3 = nn.Linear(512, 256)\n",
        "                self.fc4 = nn.Linear(256, 1)\n",
        "                \n",
        "                # Dropout\n",
        "                self.dropout = nn.Dropout(0.3)\n",
        "                \n",
        "            def forward(self, x):\n",
        "                x = x.view(x.size(0), -1)\n",
        "                x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "                x = self.dropout(x)\n",
        "                x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "                x = self.dropout(x)\n",
        "                x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "                x = self.dropout(x)\n",
        "                x = torch.sigmoid(self.fc4(x))\n",
        "                return x\n",
        "        \n",
        "        return Discriminator(self.img_size).to(self.device)\n",
        "    \n",
        "    def create_synthetic_data(self, num_samples=1000):\n",
        "        \"\"\"Cria dados sintéticos para demonstração\"\"\"\n",
        "        \n",
        "        # Criar imagens sintéticas simples\n",
        "        images = []\n",
        "        \n",
        "        for i in range(num_samples):\n",
        "            # Criar imagem base\n",
        "            img = np.zeros((self.img_size, self.img_size, 3), dtype=np.float32)\n",
        "            \n",
        "            # Adicionar formas geométricas aleatórias\n",
        "            if np.random.random() < 0.5:\n",
        "                # Círculo\n",
        "                center = (np.random.randint(20, self.img_size-20), np.random.randint(20, self.img_size-20))\n",
        "                radius = np.random.randint(10, 25)\n",
        "                color = (np.random.random(), np.random.random(), np.random.random())\n",
        "                \n",
        "                # Desenhar círculo\n",
        "                y, x = np.ogrid[:self.img_size, :self.img_size]\n",
        "                mask = (x - center[0])**2 + (y - center[1])**2 <= radius**2\n",
        "                img[mask] = color\n",
        "            \n",
        "            else:\n",
        "                # Retângulo\n",
        "                x1 = np.random.randint(10, self.img_size-30)\n",
        "                y1 = np.random.randint(10, self.img_size-30)\n",
        "                x2 = x1 + np.random.randint(20, 40)\n",
        "                y2 = y1 + np.random.randint(20, 40)\n",
        "                color = (np.random.random(), np.random.random(), np.random.random())\n",
        "                \n",
        "                img[y1:y2, x1:x2] = color\n",
        "            \n",
        "            # Adicionar ruído\n",
        "            noise = np.random.normal(0, 0.1, img.shape)\n",
        "            img = np.clip(img + noise, 0, 1)\n",
        "            \n",
        "            images.append(img)\n",
        "        \n",
        "        return np.array(images)\n",
        "    \n",
        "    def train_step(self, real_images, batch_size):\n",
        "        \"\"\"Executa um passo de treinamento\"\"\"\n",
        "        \n",
        "        # Converter para tensor\n",
        "        real_images = torch.FloatTensor(real_images).to(self.device)\n",
        "        \n",
        "        # Labels\n",
        "        real_labels = torch.ones(batch_size, 1).to(self.device)\n",
        "        fake_labels = torch.zeros(batch_size, 1).to(self.device)\n",
        "        \n",
        "        # Treinar Discriminador\n",
        "        self.d_optimizer.zero_grad()\n",
        "        \n",
        "        # Imagens reais\n",
        "        real_output = self.discriminator(real_images)\n",
        "        d_real_loss = self.criterion(real_output, real_labels)\n",
        "        \n",
        "        # Imagens falsas\n",
        "        noise = torch.randn(batch_size, self.latent_dim).to(self.device)\n",
        "        fake_images = self.generator(noise)\n",
        "        fake_output = self.discriminator(fake_images.detach())\n",
        "        d_fake_loss = self.criterion(fake_output, fake_labels)\n",
        "        \n",
        "        # Loss total do discriminador\n",
        "        d_loss = d_real_loss + d_fake_loss\n",
        "        d_loss.backward()\n",
        "        self.d_optimizer.step()\n",
        "        \n",
        "        # Treinar Gerador\n",
        "        self.g_optimizer.zero_grad()\n",
        "        \n",
        "        # Gerar imagens falsas\n",
        "        noise = torch.randn(batch_size, self.latent_dim).to(self.device)\n",
        "        fake_images = self.generator(noise)\n",
        "        fake_output = self.discriminator(fake_images)\n",
        "        \n",
        "        # Loss do gerador\n",
        "        g_loss = self.criterion(fake_output, real_labels)\n",
        "        g_loss.backward()\n",
        "        self.g_optimizer.step()\n",
        "        \n",
        "        # Armazenar losses\n",
        "        self.g_losses.append(g_loss.item())\n",
        "        self.d_losses.append(d_loss.item())\n",
        "        \n",
        "        return g_loss.item(), d_loss.item()\n",
        "    \n",
        "    def train(self, real_images, epochs=100, batch_size=32):\n",
        "        \"\"\"Treina o GAN\"\"\"\n",
        "        \n",
        "        print(f\"Iniciando treinamento do GAN para {epochs} épocas...\")\n",
        "        \n",
        "        # Criar dataloader\n",
        "        dataset = TensorDataset(torch.FloatTensor(real_images))\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            epoch_g_loss = 0\n",
        "            epoch_d_loss = 0\n",
        "            num_batches = 0\n",
        "            \n",
        "            for batch_idx, (real_batch,) in enumerate(dataloader):\n",
        "                g_loss, d_loss = self.train_step(real_batch, batch_size)\n",
        "                epoch_g_loss += g_loss\n",
        "                epoch_d_loss += d_loss\n",
        "                num_batches += 1\n",
        "            \n",
        "            # Média das losses\n",
        "            avg_g_loss = epoch_g_loss / num_batches\n",
        "            avg_d_loss = epoch_d_loss / num_batches\n",
        "            \n",
        "            if epoch % 20 == 0:\n",
        "                print(f\"Época {epoch}: G Loss = {avg_g_loss:.4f}, D Loss = {avg_d_loss:.4f}\")\n",
        "        \n",
        "        print(\"Treinamento concluído!\")\n",
        "        \n",
        "        return self.g_losses, self.d_losses\n",
        "    \n",
        "    def generate_images(self, num_images=16):\n",
        "        \"\"\"Gera imagens usando o gerador treinado\"\"\"\n",
        "        \n",
        "        self.generator.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            noise = torch.randn(num_images, self.latent_dim).to(self.device)\n",
        "            generated_images = self.generator(noise)\n",
        "            \n",
        "            # Converter para numpy\n",
        "            generated_images = generated_images.cpu().numpy()\n",
        "            \n",
        "            # Normalizar para [0, 1]\n",
        "            generated_images = (generated_images + 1) / 2\n",
        "            generated_images = np.clip(generated_images, 0, 1)\n",
        "            \n",
        "        return generated_images\n",
        "    \n",
        "    def visualize_training(self, real_images, generated_images):\n",
        "        \"\"\"Visualiza o treinamento e resultados\"\"\"\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        \n",
        "        # Imagens reais\n",
        "        for i in range(6):\n",
        "            axes[0, i].imshow(real_images[i])\n",
        "            axes[0, i].set_title(f'Real {i+1}')\n",
        "            axes[0, i].axis('off')\n",
        "        \n",
        "        # Imagens geradas\n",
        "        for i in range(6):\n",
        "            axes[1, i].imshow(generated_images[i])\n",
        "            axes[1, i].set_title(f'Gerada {i+1}')\n",
        "            axes[1, i].axis('off')\n",
        "        \n",
        "        plt.suptitle('Comparação: Imagens Reais vs Geradas', fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Gráfico de losses\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(self.g_losses, label='Gerador Loss', alpha=0.7)\n",
        "        plt.plot(self.d_losses, label='Discriminador Loss', alpha=0.7)\n",
        "        plt.title('Evolução das Losses durante o Treinamento')\n",
        "        plt.xlabel('Época')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "        \n",
        "        return generated_images\n",
        "\n",
        "def demonstrate_simple_gan():\n",
        "    \"\"\"Demonstra um GAN simples\"\"\"\n",
        "    \n",
        "    print(\"=== Demonstração de GAN Simples ===\")\n",
        "    print(\"\\nNota: Esta demonstração usa dados sintéticos para fins educacionais.\")\n",
        "    print(\"Em aplicações reais, use datasets reais de imagens.\")\n",
        "    \n",
        "    # Criar instância do GAN\n",
        "    gan = SimpleGAN(latent_dim=100, img_size=64)\n",
        "    \n",
        "    # Criar dados sintéticos\n",
        "    print(\"\\nCriando dados sintéticos...\")\n",
        "    real_images = gan.create_synthetic_data(num_samples=1000)\n",
        "    \n",
        "    # Treinar GAN\n",
        "    print(\"\\nTreinando GAN...\")\n",
        "    g_losses, d_losses = gan.train(real_images, epochs=100, batch_size=32)\n",
        "    \n",
        "    # Gerar imagens\n",
        "    print(\"\\nGerando imagens...\")\n",
        "    generated_images = gan.generate_images(num_images=16)\n",
        "    \n",
        "    # Visualizar resultados\n",
        "    print(\"\\nVisualizando resultados...\")\n",
        "    gan.visualize_training(real_images, generated_images)\n",
        "    \n",
        "    return gan, real_images, generated_images\n",
        "\n",
        "# Executar demonstração\n",
        "gan_model, real_data, generated_data = demonstrate_simple_gan()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análise dos Resultados\n",
        "\n",
        "**GAN Observado:**\n",
        "\n",
        "1. **Treinamento**: Competição entre gerador e discriminador\n",
        "2. **Losses**: Evolução das perdas durante treinamento\n",
        "3. **Geração**: Imagens sintéticas criadas pelo gerador\n",
        "4. **Qualidade**: Comparação com imagens reais\n",
        "\n",
        "**Insights Importantes:**\n",
        "- **Competição**: Gerador vs Discriminador\n",
        "- **Equilíbrio**: Balanceamento das perdas\n",
        "- **Qualidade**: Melhoria gradual da geração\n",
        "- **Diversidade**: Variação nas imagens geradas\n",
        "\n",
        "**Referências:**\n",
        "- [Generative Adversarial Networks - Goodfellow et al.](https://arxiv.org/abs/1406.2661)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.3 Generative Adversarial Networks (GANs)",
        "",
        "**GANs** são arquiteturas que consistem em dois modelos neurais competindo: um gerador que cria imagens e um discriminador que tenta distinguir entre imagens reais e sintéticas.",
        "",
        "![GANs Conceito](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo7/gans_conceito.png)",
        "",
        "### Arquitetura GAN",
        "",
        "**Componentes Principais:**",
        "",
        "**1. Gerador (Generator):**",
        "- **Entrada**: Vetor de ruído aleatório (latent space)",
        "- **Saída**: Imagem sintética",
        "- **Objetivo**: Criar imagens que enganem o discriminador",
        "- **Arquitetura**: Rede neural com camadas convolucionais",
        "",
        "**2. Discriminador (Discriminator):**",
        "- **Entrada**: Imagem (real ou sintética)",
        "- **Saída**: Probabilidade de ser real",
        "- **Objetivo**: Distinguir entre imagens reais e sintéticas",
        "- **Arquitetura**: Rede neural classificadora",
        "",
        "![Arquitetura GAN](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo7/arquitetura_gan.png)",
        "",
        "### Processo de Treinamento",
        "",
        "**1. Treinamento do Discriminador:**",
        "- **Imagens reais**: Label = 1 (real)",
        "- **Imagens sintéticas**: Label = 0 (falso)",
        "- **Loss**: Binary Cross Entropy",
        "- **Objetivo**: Maximizar precisão de classificação",
        "",
        "**2. Treinamento do Gerador:**",
        "- **Entrada**: Ruído aleatório",
        "- **Saída**: Imagem sintética",
        "- **Loss**: Discriminador classifica como real",
        "- **Objetivo**: Minimizar perda do discriminador",
        "",
        "**3. Competição Adversarial:**",
        "- **Equilíbrio**: Gerador e discriminador competem",
        "- **Convergência**: Ambos melhoram simultaneamente",
        "- **Estabilidade**: Desafio principal dos GANs",
        "- **Resultado**: Imagens de alta qualidade",
        "",
        "### Tipos de GANs",
        "",
        "**1. DCGAN (Deep Convolutional GAN):**",
        "- **Arquitetura**: Convolucional para gerador e discriminador",
        "- **Inovações**: Batch normalization, ReLU, LeakyReLU",
        "- **Aplicação**: Imagens de alta qualidade",
        "- **Referência**: Radford et al. (2016)",
        "",
        "**2. Progressive GAN:**",
        "- **Conceito**: Treinamento progressivo de baixa para alta resolução",
        "- **Vantagem**: Estabilidade e qualidade",
        "- **Aplicação**: Imagens de alta resolução",
        "- **Referência**: Karras et al. (2017)",
        "",
        "**3. StyleGAN:**",
        "- **Inovação**: Controle de estilo e conteúdo",
        "- **Arquitetura**: Mapeamento de ruído para estilo",
        "- **Aplicação**: Geração controlada de faces",
        "- **Referência**: Karras et al. (2019)",
        "",
        "![Tipos de GANs](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo7/tipos_gans.png)",
        "",
        "**Referências:**",
        "- [Generative Adversarial Networks - Goodfellow et al.](https://arxiv.org/abs/1406.2661)",
        "- [Unsupervised Representation Learning with Deep Convolutional GANs - Radford et al.](https://arxiv.org/abs/1511.06434)",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.4 Variational Autoencoders (VAEs)",
        "",
        "**VAEs** são modelos generativos que aprendem a representar dados em um espaço latente de baixa dimensionalidade, permitindo geração de novas amostras.",
        "",
        "![VAEs Conceito](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo7/vaes_conceito.png)",
        "",
        "### Arquitetura VAE",
        "",
        "**Componentes Principais:**",
        "",
        "**1. Encoder:**",
        "- **Entrada**: Imagem real",
        "- **Saída**: Parâmetros da distribuição latente (μ, σ)",
        "- **Objetivo**: Comprimir imagem para espaço latente",
        "- **Arquitetura**: Rede neural com camadas convolucionais",
        "",
        "**2. Decoder:**",
        "- **Entrada**: Amostra do espaço latente",
        "- **Saída**: Imagem reconstruída",
        "- **Objetivo**: Reconstruir imagem a partir do espaço latente",
        "- **Arquitetura**: Rede neural com camadas transpostas",
        "",
        "![Arquitetura VAE](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo7/arquitetura_vae.png)",
        "",
        "### Processo de Treinamento",
        "",
        "**1. Forward Pass:**",
        "- **Encoder**: Imagem → μ, σ",
        "- **Sampling**: z ~ N(μ, σ²)",
        "- **Decoder**: z → Imagem reconstruída",
        "",
        "**2. Loss Function:**",
        "- **Reconstruction Loss**: Diferença entre imagem original e reconstruída",
        "- **KL Divergence**: Regularização do espaço latente",
        "- **Total Loss**: Reconstruction + β × KL Divergence",
        "",
        "**3. Regularização:**",
        "- **KL Divergence**: Força distribuição latente próxima a N(0,1)",
        "- **β-VAE**: Controle do trade-off entre reconstrução e regularização",
        "- **Resultado**: Espaço latente contínuo e interpretável",
        "",
        "### Demonstração Prática: VAE Simples",
        "",
        "Vamos implementar e visualizar um VAE simples:",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class SimpleVAE:\n",
        "    \"\"\"Implementação de um VAE simples para demonstração\"\"\"\n",
        "    \n",
        "    def __init__(self, latent_dim=2, img_size=64):\n",
        "        self.latent_dim = latent_dim\n",
        "        self.img_size = img_size\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "        # Inicializar modelo\n",
        "        self.model = self._build_vae()\n",
        "        \n",
        "        # Otimizador\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        \n",
        "        # Histórico de treinamento\n",
        "        self.losses = []\n",
        "        self.recon_losses = []\n",
        "        self.kl_losses = []\n",
        "        \n",
        "    def _build_vae(self):\n",
        "        \"\"\"Constrói o VAE\"\"\"\n",
        "        \n",
        "        class VAE(nn.Module):\n",
        "            def __init__(self, latent_dim, img_size):\n",
        "                super(VAE, self).__init__()\n",
        "                \n",
        "                self.latent_dim = latent_dim\n",
        "                self.img_size = img_size\n",
        "                \n",
        "                # Encoder\n",
        "                self.encoder = nn.Sequential(\n",
        "                    nn.Linear(img_size * img_size * 3, 512),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(512, 256),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(256, 128),\n",
        "                    nn.ReLU()\n",
        "                )\n",
        "                \n",
        "                # Mean e log variance\n",
        "                self.fc_mu = nn.Linear(128, latent_dim)\n",
        "                self.fc_logvar = nn.Linear(128, latent_dim)\n",
        "                \n",
        "                # Decoder\n",
        "                self.decoder = nn.Sequential(\n",
        "                    nn.Linear(latent_dim, 128),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(128, 256),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(256, 512),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(512, img_size * img_size * 3),\n",
        "                    nn.Sigmoid()\n",
        "                )\n",
        "                \n",
        "            def encode(self, x):\n",
        "                \"\"\"Codifica imagem para espaço latente\"\"\"\n",
        "                x = x.view(x.size(0), -1)\n",
        "                h = self.encoder(x)\n",
        "                mu = self.fc_mu(h)\n",
        "                logvar = self.fc_logvar(h)\n",
        "                return mu, logvar\n",
        "                \n",
        "            def reparameterize(self, mu, logvar):\n",
        "                \"\"\"Reparameterization trick\"\"\"\n",
        "                std = torch.exp(0.5 * logvar)\n",
        "                eps = torch.randn_like(std)\n",
        "                return mu + eps * std\n",
        "                \n",
        "            def decode(self, z):\n",
        "                \"\"\"Decodifica espaço latente para imagem\"\"\"\n",
        "                x = self.decoder(z)\n",
        "                return x.view(x.size(0), 3, self.img_size, self.img_size)\n",
        "                \n",
        "            def forward(self, x):\n",
        "                mu, logvar = self.encode(x)\n",
        "                z = self.reparameterize(mu, logvar)\n",
        "                recon_x = self.decode(z)\n",
        "                return recon_x, mu, logvar\n",
        "        \n",
        "        return VAE(self.latent_dim, self.img_size).to(self.device)\n",
        "    \n",
        "    def vae_loss(self, recon_x, x, mu, logvar):\n",
        "        \"\"\"Calcula a loss do VAE\"\"\"\n",
        "        \n",
        "        # Reconstruction loss (MSE)\n",
        "        recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
        "        \n",
        "        # KL divergence\n",
        "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        \n",
        "        # Total loss\n",
        "        total_loss = recon_loss + kl_loss\n",
        "        \n",
        "        return total_loss, recon_loss, kl_loss\n",
        "    \n",
        "    def train_step(self, batch):\n",
        "        \"\"\"Executa um passo de treinamento\"\"\"\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        # Forward pass\n",
        "        recon_batch, mu, logvar = self.model(batch)\n",
        "        \n",
        "        # Calcular loss\n",
        "        loss, recon_loss, kl_loss = self.vae_loss(recon_batch, batch, mu, logvar)\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return loss.item(), recon_loss.item(), kl_loss.item()\n",
        "    \n",
        "    def train(self, data, epochs=100, batch_size=32):\n",
        "        \"\"\"Treina o VAE\"\"\"\n",
        "        \n",
        "        print(f\"Iniciando treinamento do VAE para {epochs} épocas...\")\n",
        "        \n",
        "        # Criar dataloader\n",
        "        dataset = TensorDataset(torch.FloatTensor(data))\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "            epoch_recon_loss = 0\n",
        "            epoch_kl_loss = 0\n",
        "            num_batches = 0\n",
        "            \n",
        "            for batch_idx, (batch,) in enumerate(dataloader):\n",
        "                batch = batch.to(self.device)\n",
        "                loss, recon_loss, kl_loss = self.train_step(batch)\n",
        "                \n",
        "                epoch_loss += loss\n",
        "                epoch_recon_loss += recon_loss\n",
        "                epoch_kl_loss += kl_loss\n",
        "                num_batches += 1\n",
        "            \n",
        "            # Média das losses\n",
        "            avg_loss = epoch_loss / num_batches\n",
        "            avg_recon_loss = epoch_recon_loss / num_batches\n",
        "            avg_kl_loss = epoch_kl_loss / num_batches\n",
        "            \n",
        "            # Armazenar losses\n",
        "            self.losses.append(avg_loss)\n",
        "            self.recon_losses.append(avg_recon_loss)\n",
        "            self.kl_losses.append(avg_kl_loss)\n",
        "            \n",
        "            if epoch % 20 == 0:\n",
        "                print(f\"Época {epoch}: Loss = {avg_loss:.4f}, Recon = {avg_recon_loss:.4f}, KL = {avg_kl_loss:.4f}\")\n",
        "        \n",
        "        print(\"Treinamento concluído!\")\n",
        "        \n",
        "        return self.losses, self.recon_losses, self.kl_losses\n",
        "    \n",
        "    def generate_images(self, num_images=16):\n",
        "        \"\"\"Gera imagens usando o VAE treinado\"\"\"\n",
        "        \n",
        "        self.model.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # Amostrar do espaço latente\n",
        "            z = torch.randn(num_images, self.latent_dim).to(self.device)\n",
        "            generated_images = self.model.decode(z)\n",
        "            \n",
        "            # Converter para numpy\n",
        "            generated_images = generated_images.cpu().numpy()\n",
        "            \n",
        "        return generated_images\n",
        "    \n",
        "    def reconstruct_images(self, images):\n",
        "        \"\"\"Reconstrói imagens usando o VAE\"\"\"\n",
        "        \n",
        "        self.model.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            images_tensor = torch.FloatTensor(images).to(self.device)\n",
        "            recon_images, _, _ = self.model(images_tensor)\n",
        "            \n",
        "            # Converter para numpy\n",
        "            recon_images = recon_images.cpu().numpy()\n",
        "            \n",
        "        return recon_images\n",
        "    \n",
        "    def visualize_results(self, original_images, reconstructed_images, generated_images):\n",
        "        \"\"\"Visualiza resultados do VAE\"\"\"\n",
        "        \n",
        "        fig, axes = plt.subplots(3, 6, figsize=(18, 9))\n",
        "        \n",
        "        # Imagens originais\n",
        "        for i in range(6):\n",
        "            axes[0, i].imshow(original_images[i])\n",
        "            axes[0, i].set_title(f'Original {i+1}')\n",
        "            axes[0, i].axis('off')\n",
        "        \n",
        "        # Imagens reconstruídas\n",
        "        for i in range(6):\n",
        "            axes[1, i].imshow(reconstructed_images[i])\n",
        "            axes[1, i].set_title(f'Reconstruída {i+1}')\n",
        "            axes[1, i].axis('off')\n",
        "        \n",
        "        # Imagens geradas\n",
        "        for i in range(6):\n",
        "            axes[2, i].imshow(generated_images[i])\n",
        "            axes[2, i].set_title(f'Gerada {i+1}')\n",
        "            axes[2, i].axis('off')\n",
        "        \n",
        "        plt.suptitle('VAE: Originais, Reconstruídas e Geradas', fontsize=16)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Gráfico de losses\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(self.losses, label='Total Loss', alpha=0.7)\n",
        "        plt.plot(self.recon_losses, label='Reconstruction Loss', alpha=0.7)\n",
        "        plt.plot(self.kl_losses, label='KL Divergence Loss', alpha=0.7)\n",
        "        plt.title('Evolução das Losses durante o Treinamento')\n",
        "        plt.xlabel('Época')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.show()\n",
        "        \n",
        "        return generated_images\n",
        "\n",
        "def demonstrate_simple_vae():\n",
        "    \"\"\"Demonstra um VAE simples\"\"\"\n",
        "    \n",
        "    print(\"=== Demonstração de VAE Simples ===\")\n",
        "    print(\"\\nNota: Esta demonstração usa dados sintéticos para fins educacionais.\")\n",
        "    print(\"Em aplicações reais, use datasets reais de imagens.\")\n",
        "    \n",
        "    # Criar instância do VAE\n",
        "    vae = SimpleVAE(latent_dim=2, img_size=64)\n",
        "    \n",
        "    # Criar dados sintéticos\n",
        "    print(\"\\nCriando dados sintéticos...\")\n",
        "    real_images = vae.create_synthetic_data(num_samples=1000)\n",
        "    \n",
        "    # Treinar VAE\n",
        "    print(\"\\nTreinando VAE...\")\n",
        "    losses, recon_losses, kl_losses = vae.train(real_images, epochs=100, batch_size=32)\n",
        "    \n",
        "    # Reconstruir imagens\n",
        "    print(\"\\nReconstruindo imagens...\")\n",
        "    reconstructed_images = vae.reconstruct_images(real_images[:6])\n",
        "    \n",
        "    # Gerar imagens\n",
        "    print(\"\\nGerando imagens...\")\n",
        "    generated_images = vae.generate_images(num_images=16)\n",
        "    \n",
        "    # Visualizar resultados\n",
        "    print(\"\\nVisualizando resultados...\")\n",
        "    vae.visualize_results(real_images[:6], reconstructed_images, generated_images)\n",
        "    \n",
        "    return vae, real_images, reconstructed_images, generated_images\n",
        "\n",
        "# Executar demonstração\n",
        "vae_model, real_data, recon_data, generated_data = demonstrate_simple_vae()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análise dos Resultados\n",
        "\n",
        "**VAE Observado:**\n",
        "\n",
        "1. **Reconstrução**: Imagens reconstruídas a partir do espaço latente\n",
        "2. **Geração**: Novas imagens geradas do espaço latente\n",
        "3. **Losses**: Evolução das perdas durante treinamento\n",
        "4. **Qualidade**: Comparação entre originais e reconstruídas\n",
        "\n",
        "**Insights Importantes:**\n",
        "- **Reconstrução**: VAE pode reconstruir imagens\n",
        "- **Geração**: Espaço latente permite geração\n",
        "- **Regularização**: KL divergence controla o espaço latente\n",
        "- **Trade-off**: Reconstrução vs regularização\n",
        "\n",
        "**Referências:**\n",
        "- [Auto-Encoding Variational Bayes - Kingma & Welling](https://arxiv.org/abs/1312.6114)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.5 Comparação: GANs vs VAEs",
        "",
        "### Diferenças Fundamentais",
        "",
        "**GANs:**",
        "- **Objetivo**: Gerar imagens realistas",
        "- **Arquitetura**: Gerador + Discriminador",
        "- **Treinamento**: Competição adversarial",
        "- **Qualidade**: Imagens de alta qualidade",
        "- **Controle**: Limitado",
        "",
        "**VAEs:**",
        "- **Objetivo**: Aprender representação latente",
        "- **Arquitetura**: Encoder + Decoder",
        "- **Treinamento**: Minimização de loss",
        "- **Qualidade**: Imagens mais suaves",
        "- **Controle**: Espaço latente interpretável",
        "",
        "![Comparação GANs vs VAEs](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo7/comparacao_gans_vaes.png)",
        "",
        "### Vantagens e Desvantagens",
        "",
        "**GANs - Vantagens:**",
        "- **Qualidade**: Imagens muito realistas",
        "- **Diversidade**: Grande variedade de saídas",
        "- **Inovação**: Arquiteturas criativas",
        "- **Aplicação**: Arte, entretenimento",
        "",
        "**GANs - Desvantagens:**",
        "- **Estabilidade**: Difícil de treinar",
        "- **Controle**: Limitado controle sobre saída",
        "- **Modo Collapse**: Problema comum",
        "- **Recursos**: Requer muitos recursos",
        "",
        "**VAEs - Vantagens:**",
        "- **Estabilidade**: Treinamento estável",
        "- **Controle**: Espaço latente interpretável",
        "- **Reconstrução**: Pode reconstruir imagens",
        "- **Regularização**: Espaço latente regularizado",
        "",
        "**VAEs - Desvantagens:**",
        "- **Qualidade**: Imagens mais suaves",
        "- **Blur**: Efeito de desfoque",
        "- **Limitação**: Espaço latente limitado",
        "- **Trade-off**: Reconstrução vs geração",
        "",
        "### Aplicações Práticas",
        "",
        "**GANs - Casos de Uso:**",
        "- **Arte digital**: Criação artística",
        "- **Data augmentation**: Aumento de datasets",
        "- **Super-resolução**: Aumento de resolução",
        "- **Transferência de estilo**: Aplicação de estilos",
        "",
        "**VAEs - Casos de Uso:**",
        "- **Compressão**: Compressão de imagens",
        "- **Anomaly detection**: Detecção de anomalias",
        "- **Representação**: Aprendizado de representações",
        "- **Interpolação**: Interpolação no espaço latente",
        "",
        "**Referências:**",
        "- [Generative Adversarial Networks - Goodfellow et al.](https://arxiv.org/abs/1406.2661)",
        "- [Auto-Encoding Variational Bayes - Kingma & Welling](https://arxiv.org/abs/1312.6114)",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Resumo do Módulo 7\n",
        "\n",
        "### Principais Conceitos Abordados\n",
        "\n",
        "1. **Introdução à Geração Sintética**\n",
        "   - Conceitos fundamentais\n",
        "   - Evolução histórica\n",
        "   - Aplicações práticas\n",
        "\n",
        "2. **Generative Adversarial Networks (GANs)**\n",
        "   - Arquitetura e componentes\n",
        "   - Processo de treinamento\n",
        "   - Tipos e variações\n",
        "\n",
        "3. **Variational Autoencoders (VAEs)**\n",
        "   - Arquitetura encoder-decoder\n",
        "   - Espaço latente\n",
        "   - Regularização KL\n",
        "\n",
        "4. **Comparação e Aplicações**\n",
        "   - Diferenças fundamentais\n",
        "   - Vantagens e desvantagens\n",
        "   - Casos de uso práticos\n",
        "\n",
        "### Demonstrações Práticas\n",
        "\n",
        "**1. GAN Simples:**\n",
        "   - Implementação completa\n",
        "   - Treinamento adversarial\n",
        "   - Geração de imagens\n",
        "\n",
        "**2. VAE Simples:**\n",
        "   - Arquitetura encoder-decoder\n",
        "   - Reconstrução e geração\n",
        "   - Análise do espaço latente\n",
        "\n",
        "### Próximos Passos\n",
        "\n",
        "No próximo módulo, exploraremos **Vision Transformers e Mecanismos de Atenção**, onde aprenderemos sobre a revolução dos transformers na visão computacional.\n",
        "\n",
        "### Referências Principais\n",
        "\n",
        "- [Generative Adversarial Networks - Goodfellow et al.](https://arxiv.org/abs/1406.2661)\n",
        "- [Auto-Encoding Variational Bayes - Kingma & Welling](https://arxiv.org/abs/1312.6114)\n",
        "- [Unsupervised Representation Learning with Deep Convolutional GANs - Radford et al.](https://arxiv.org/abs/1511.06434)\n",
        "\n",
        "---\n",
        "\n",
        "**Próximo Módulo**: Vision Transformers e Mecanismos de Atenção\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}