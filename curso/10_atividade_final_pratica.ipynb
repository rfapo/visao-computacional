{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# M√≥dulo 10: Atividade Final Pr√°tica\n",
        "\n",
        "## üéØ Objetivos de Aprendizagem\n",
        "\n",
        "Ao final deste m√≥dulo, voc√™ ser√° capaz de:\n",
        "\n",
        "- ‚úÖ Aplicar todos os conceitos aprendidos em um projeto completo\n",
        "- ‚úÖ Implementar uma solu√ß√£o de vis√£o computacional do zero\n",
        "- ‚úÖ Integrar diferentes t√©cnicas e abordagens\n",
        "- ‚úÖ Desenvolver habilidades pr√°ticas de implementa√ß√£o\n",
        "- ‚úÖ Apresentar resultados e an√°lises t√©cnicas\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ 10.1 Projeto Final: Sistema de An√°lise de Imagens Multimodal\n",
        "\n",
        "### Descri√ß√£o do Projeto\n",
        "\n",
        "**Objetivo:** Desenvolver um sistema completo de an√°lise de imagens que integre diferentes t√©cnicas de vis√£o computacional aprendidas ao longo do curso.\n",
        "\n",
        "![Descri√ß√£o Projeto Final](https://cdn.jsdelivr.net/gh/rfapo/visao-computacional@main/images/modulo10/descricao_projeto_final.png)\n",
        "\n",
        "### Funcionalidades Principais\n",
        "\n",
        "#### **1. Classifica√ß√£o de Imagens**\n",
        "- **T√©cnica**: CNNs tradicionais (ResNet)\n",
        "- **Entrada**: Imagem\n",
        "- **Sa√≠da**: Classe predita + confian√ßa\n",
        "- **Aplica√ß√£o**: Categoriza√ß√£o autom√°tica\n",
        "\n",
        "#### **2. Detec√ß√£o de Objetos**\n",
        "- **T√©cnica**: YOLO simplificado\n",
        "- **Entrada**: Imagem\n",
        "- **Sa√≠da**: Bounding boxes + classes\n",
        "- **Aplica√ß√£o**: Localiza√ß√£o de objetos\n",
        "\n",
        "#### **3. Segmenta√ß√£o de Imagens**\n",
        "- **T√©cnica**: U-Net\n",
        "- **Entrada**: Imagem\n",
        "- **Sa√≠da**: M√°scara de segmenta√ß√£o\n",
        "- **Aplica√ß√£o**: Separa√ß√£o de regi√µes\n",
        "\n",
        "#### **4. OCR e Extra√ß√£o de Texto**\n",
        "- **T√©cnica**: Tesseract/EasyOCR\n",
        "- **Entrada**: Imagem\n",
        "- **Sa√≠da**: Texto extra√≠do\n",
        "- **Aplica√ß√£o**: Reconhecimento de texto\n",
        "\n",
        "#### **5. An√°lise Multimodal**\n",
        "- **T√©cnica**: Foundation Models (CLIP/GPT-4V)\n",
        "- **Entrada**: Imagem + texto\n",
        "- **Sa√≠da**: An√°lise multimodal\n",
        "- **Aplica√ß√£o**: Compreens√£o contextual\n",
        "\n",
        "#### **6. Interface de Usu√°rio**\n",
        "- **T√©cnica**: Streamlit\n",
        "- **Entrada**: Upload de imagem\n",
        "- **Sa√≠da**: Resultados visuais\n",
        "- **Aplica√ß√£o**: Intera√ß√£o amig√°vel\n",
        "\n",
        "### Arquitetura do Sistema\n",
        "\n",
        "![Arquitetura Sistema](https://cdn.jsdelivr.net/gh/rfapo/visao-computacional@main/images/modulo10/arquitetura_sistema.png)\n",
        "\n",
        "#### **Estrutura Modular:**\n",
        "\n",
        "```\n",
        "Input Module ‚Üí Preprocessing Module ‚Üí Analysis Module ‚Üí Output Module\n",
        "     ‚Üì              ‚Üì                    ‚Üì              ‚Üì\n",
        "  Upload        Normaliza√ß√£o        An√°lise        Visualiza√ß√£o\n",
        "  Imagem        Redimensionamento   Multimodal    Resultados\n",
        "```\n",
        "\n",
        "#### **Componentes:**\n",
        "- **Input Module**: Entrada de imagens\n",
        "- **Preprocessing Module**: Pr√©-processamento\n",
        "- **Analysis Module**: An√°lise multimodal\n",
        "- **Output Module**: Resultados e visualiza√ß√£o\n",
        "\n",
        "### Tecnologias Utilizadas\n",
        "\n",
        "![Tecnologias Utilizadas](https://cdn.jsdelivr.net/gh/rfapo/visao-computacional@main/images/modulo10/tecnologias_utilizadas.png)\n",
        "\n",
        "#### **Frameworks:**\n",
        "- **PyTorch**: Deep learning\n",
        "- **OpenCV**: Processamento de imagem\n",
        "- **Transformers**: Modelos pr√©-treinados\n",
        "- **Streamlit**: Interface web\n",
        "\n",
        "#### **Modelos:**\n",
        "- **ResNet**: Classifica√ß√£o\n",
        "- **YOLO**: Detec√ß√£o\n",
        "- **U-Net**: Segmenta√ß√£o\n",
        "- **CLIP**: An√°lise multimodal\n",
        "\n",
        "---\n",
        "\n",
        "## üîç 10.2 Demonstra√ß√£o Pr√°tica: Sistema Completo\n",
        "\n",
        "Vamos implementar e demonstrar o sistema completo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "class MultimodalImageAnalysisSystem:\n",
        "    \"\"\"Sistema completo de an√°lise de imagens multimodal\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.results = {}\n",
        "        \n",
        "        # Inicializar m√≥dulos\n",
        "        self.classifier = self._build_classifier()\n",
        "        self.detector = self._build_detector()\n",
        "        self.segmenter = self._build_segmenter()\n",
        "        self.ocr_engine = self._build_ocr_engine()\n",
        "        self.multimodal_analyzer = self._build_multimodal_analyzer()\n",
        "        \n",
        "        # Transforma√ß√µes\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "    \n",
        "    def _build_classifier(self):\n",
        "        \"\"\"Constr√≥i o classificador de imagens\"\"\"\n",
        "        \n",
        "        class SimpleCNN(nn.Module):\n",
        "            def __init__(self, num_classes=10):\n",
        "                super(SimpleCNN, self).__init__()\n",
        "                \n",
        "                self.features = nn.Sequential(\n",
        "                    nn.Conv2d(3, 32, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    nn.Conv2d(32, 64, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    nn.Conv2d(64, 128, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    nn.Conv2d(128, 256, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.AdaptiveAvgPool2d((1, 1))\n",
        "                )\n",
        "                \n",
        "                self.classifier = nn.Sequential(\n",
        "                    nn.Dropout(0.5),\n",
        "                    nn.Linear(256, 128),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Dropout(0.5),\n",
        "                    nn.Linear(128, num_classes)\n",
        "                )\n",
        "                \n",
        "            def forward(self, x):\n",
        "                x = self.features(x)\n",
        "                x = x.view(x.size(0), -1)\n",
        "                x = self.classifier(x)\n",
        "                return x\n",
        "        \n",
        "        return SimpleCNN(num_classes=10).to(self.device)\n",
        "    \n",
        "    def _build_detector(self):\n",
        "        \"\"\"Constr√≥i o detector de objetos\"\"\"\n",
        "        \n",
        "        class SimpleYOLO(nn.Module):\n",
        "            def __init__(self, num_classes=5):\n",
        "                super(SimpleYOLO, self).__init__()\n",
        "                \n",
        "                self.backbone = nn.Sequential(\n",
        "                    nn.Conv2d(3, 32, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    nn.Conv2d(32, 64, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    nn.Conv2d(64, 128, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2)\n",
        "                )\n",
        "                \n",
        "                self.detection_head = nn.Sequential(\n",
        "                    nn.Conv2d(128, 256, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(256, (num_classes + 5) * 3, 1)  # 3 anchors\n",
        "                )\n",
        "                \n",
        "            def forward(self, x):\n",
        "                features = self.backbone(x)\n",
        "                detections = self.detection_head(features)\n",
        "                return detections\n",
        "        \n",
        "        return SimpleYOLO(num_classes=5).to(self.device)\n",
        "    \n",
        "    def _build_segmenter(self):\n",
        "        \"\"\"Constr√≥i o segmentador de imagens\"\"\"\n",
        "        \n",
        "        class SimpleUNet(nn.Module):\n",
        "            def __init__(self, num_classes=3):\n",
        "                super(SimpleUNet, self).__init__()\n",
        "                \n",
        "                # Encoder\n",
        "                self.enc1 = nn.Sequential(\n",
        "                    nn.Conv2d(3, 64, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(64, 64, 3, padding=1),\n",
        "                    nn.ReLU()\n",
        "                )\n",
        "                \n",
        "                self.enc2 = nn.Sequential(\n",
        "                    nn.MaxPool2d(2),\n",
        "                    nn.Conv2d(64, 128, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Conv2d(128, 128, 3, padding=1),\n",
        "                    nn.ReLU()\n",
        "                )\n",
        "                \n",
        "                # Decoder\n",
        "                self.dec1 = nn.Sequential(\n",
        "                    nn.ConvTranspose2d(128, 64, 2, stride=2),\n",
        "                    nn.Conv2d(64, 64, 3, padding=1),\n",
        "                    nn.ReLU()\n",
        "                )\n",
        "                \n",
        "                self.final = nn.Conv2d(64, num_classes, 1)\n",
        "                \n",
        "            def forward(self, x):\n",
        "                # Encoder\n",
        "                enc1 = self.enc1(x)\n",
        "                enc2 = self.enc2(enc1)\n",
        "                \n",
        "                # Decoder\n",
        "                dec1 = self.dec1(enc2)\n",
        "                \n",
        "                # Skip connection\n",
        "                dec1 = dec1 + enc1\n",
        "                \n",
        "                # Final\n",
        "                output = self.final(dec1)\n",
        "                \n",
        "                return output\n",
        "        \n",
        "        return SimpleUNet(num_classes=3).to(self.device)\n",
        "    \n",
        "    def _build_ocr_engine(self):\n",
        "        \"\"\"Constr√≥i o motor de OCR\"\"\"\n",
        "        \n",
        "        class SimpleOCR(nn.Module):\n",
        "            def __init__(self):\n",
        "                super(SimpleOCR, self).__init__()\n",
        "                \n",
        "                self.features = nn.Sequential(\n",
        "                    nn.Conv2d(3, 32, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    nn.Conv2d(32, 64, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    nn.Conv2d(64, 128, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.AdaptiveAvgPool2d((1, 1))\n",
        "                )\n",
        "                \n",
        "                self.classifier = nn.Sequential(\n",
        "                    nn.Linear(128, 64),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(64, 26)  # A-Z\n",
        "                )\n",
        "                \n",
        "            def forward(self, x):\n",
        "                x = self.features(x)\n",
        "                x = x.view(x.size(0), -1)\n",
        "                x = self.classifier(x)\n",
        "                return x\n",
        "        \n",
        "        return SimpleOCR().to(self.device)\n",
        "    \n",
        "    def _build_multimodal_analyzer(self):\n",
        "        \"\"\"Constr√≥i o analisador multimodal\"\"\"\n",
        "        \n",
        "        class SimpleMultimodal(nn.Module):\n",
        "            def __init__(self, embedding_dim=128):\n",
        "                super(SimpleMultimodal, self).__init__()\n",
        "                \n",
        "                # Image encoder\n",
        "                self.image_encoder = nn.Sequential(\n",
        "                    nn.Conv2d(3, 64, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    nn.Conv2d(64, 128, 3, padding=1),\n",
        "                    nn.ReLU(),\n",
        "                    nn.AdaptiveAvgPool2d((1, 1)),\n",
        "                    nn.Flatten(),\n",
        "                    nn.Linear(128, embedding_dim)\n",
        "                )\n",
        "                \n",
        "                # Text encoder\n",
        "                self.text_encoder = nn.Sequential(\n",
        "                    nn.Linear(100, embedding_dim),  # Simulated text input\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(embedding_dim, embedding_dim)\n",
        "                )\n",
        "                \n",
        "                # Fusion\n",
        "                self.fusion = nn.Sequential(\n",
        "                    nn.Linear(embedding_dim * 2, embedding_dim),\n",
        "                    nn.ReLU(),\n",
        "                    nn.Linear(embedding_dim, 10)  # Classification\n",
        "                )\n",
        "                \n",
        "            def forward(self, image, text):\n",
        "                # Encode image\n",
        "                img_features = self.image_encoder(image)\n",
        "                \n",
        "                # Encode text\n",
        "                text_features = self.text_encoder(text)\n",
        "                \n",
        "                # Fusion\n",
        "                combined = torch.cat([img_features, text_features], dim=1)\n",
        "                output = self.fusion(combined)\n",
        "                \n",
        "                return output\n",
        "        \n",
        "        return SimpleMultimodal(embedding_dim=128).to(self.device)\n",
        "    \n",
        "    def create_sample_image(self, width=224, height=224):\n",
        "        \"\"\"Cria uma imagem de exemplo para demonstra√ß√£o\"\"\"\n",
        "        \n",
        "        # Criar imagem com padr√µes\n",
        "        img = np.zeros((height, width, 3), dtype=np.uint8)\n",
        "        \n",
        "        # Fundo\n",
        "        img[:] = [100, 150, 200]  # Azul claro\n",
        "        \n",
        "        # Adicionar objetos\n",
        "        # C√≠rculo vermelho\n",
        "        cv2.circle(img, (80, 80), 30, (255, 0, 0), -1)\n",
        "        \n",
        "        # Ret√¢ngulo verde\n",
        "        cv2.rectangle(img, (150, 60), (200, 120), (0, 255, 0), -1)\n",
        "        \n",
        "        # Tri√¢ngulo amarelo\n",
        "        pts = np.array([[100, 150], [80, 180], [120, 180]], np.int32)\n",
        "        cv2.fillPoly(img, [pts], (0, 255, 255))\n",
        "        \n",
        "        # Adicionar texto\n",
        "        cv2.putText(img, 'Hello World!', (50, 200), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
        "        \n",
        "        return img\n",
        "    \n",
        "    def classify_image(self, image):\n",
        "        \"\"\"Classifica a imagem\"\"\"\n",
        "        \n",
        "        self.classifier.eval()\n",
        "        \n",
        "        # Pr√©-processar\n",
        "        img_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            logits = self.classifier(img_tensor)\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            predicted_class = torch.argmax(probs, dim=1).item()\n",
        "            confidence = probs[0, predicted_class].item()\n",
        "        \n",
        "        return {\n",
        "            'class': predicted_class,\n",
        "            'confidence': confidence,\n",
        "            'probabilities': probs[0].cpu().numpy()\n",
        "        }\n",
        "    \n",
        "    def detect_objects(self, image):\n",
        "        \"\"\"Detecta objetos na imagem\"\"\"\n",
        "        \n",
        "        self.detector.eval()\n",
        "        \n",
        "        # Pr√©-processar\n",
        "        img_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            detections = self.detector(img_tensor)\n",
        "        \n",
        "        # Simular detec√ß√µes\n",
        "        detections = [\n",
        "            {'bbox': [60, 50, 100, 110], 'class': 0, 'confidence': 0.85},\n",
        "            {'bbox': [140, 50, 210, 130], 'class': 1, 'confidence': 0.92},\n",
        "            {'bbox': [80, 140, 120, 190], 'class': 2, 'confidence': 0.78}\n",
        "        ]\n",
        "        \n",
        "        return detections\n",
        "    \n",
        "    def segment_image(self, image):\n",
        "        \"\"\"Segmenta a imagem\"\"\"\n",
        "        \n",
        "        self.segmenter.eval()\n",
        "        \n",
        "        # Pr√©-processar\n",
        "        img_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            segmentation = self.segmenter(img_tensor)\n",
        "            segmentation = F.softmax(segmentation, dim=1)\n",
        "            segmentation = torch.argmax(segmentation, dim=1).squeeze(0).cpu().numpy()\n",
        "        \n",
        "        return segmentation\n",
        "    \n",
        "    def extract_text(self, image):\n",
        "        \"\"\"Extrai texto da imagem\"\"\"\n",
        "        \n",
        "        # Simular OCR\n",
        "        text = \"Hello World!\"\n",
        "        confidence = 0.95\n",
        "        \n",
        "        return {\n",
        "            'text': text,\n",
        "            'confidence': confidence\n",
        "        }\n",
        "    \n",
        "    def analyze_multimodal(self, image, text_query=\"What do you see?\"):\n",
        "        \"\"\"Analisa multimodalmente\"\"\"\n",
        "        \n",
        "        self.multimodal_analyzer.eval()\n",
        "        \n",
        "        # Pr√©-processar\n",
        "        img_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
        "        text_tensor = torch.randn(1, 100).to(self.device)  # Simulated text\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output = self.multimodal_analyzer(img_tensor, text_tensor)\n",
        "            probs = F.softmax(output, dim=1)\n",
        "            predicted_class = torch.argmax(probs, dim=1).item()\n",
        "            confidence = probs[0, predicted_class].item()\n",
        "        \n",
        "        return {\n",
        "            'query': text_query,\n",
        "            'response': f\"I see objects of class {predicted_class}\",\n",
        "            'confidence': confidence\n",
        "        }\n",
        "    \n",
        "    def analyze_image(self, image):\n",
        "        \"\"\"Analisa a imagem completa\"\"\"\n",
        "        \n",
        "        print(\"=== AN√ÅLISE COMPLETA DE IMAGEM ===\")\n",
        "        \n",
        "        # Classifica√ß√£o\n",
        "        print(\"\\n1. Classifica√ß√£o:\")\n",
        "        classification = self.classify_image(image)\n",
        "        print(f\"   Classe: {classification['class']}\")\n",
        "        print(f\"   Confian√ßa: {classification['confidence']:.4f}\")\n",
        "        \n",
        "        # Detec√ß√£o\n",
        "        print(\"\\n2. Detec√ß√£o de Objetos:\")\n",
        "        detections = self.detect_objects(image)\n",
        "        for i, det in enumerate(detections):\n",
        "            print(f\"   Objeto {i+1}: Classe {det['class']}, Confian√ßa {det['confidence']:.4f}\")\n",
        "        \n",
        "        # Segmenta√ß√£o\n",
        "        print(\"\\n3. Segmenta√ß√£o:\")\n",
        "        segmentation = self.segment_image(image)\n",
        "        unique_classes = np.unique(segmentation)\n",
        "        print(f\"   Classes encontradas: {unique_classes}\")\n",
        "        \n",
        "        # OCR\n",
        "        print(\"\\n4. OCR:\")\n",
        "        ocr_result = self.extract_text(image)\n",
        "        print(f\"   Texto: {ocr_result['text']}\")\n",
        "        print(f\"   Confian√ßa: {ocr_result['confidence']:.4f}\")\n",
        "        \n",
        "        # An√°lise Multimodal\n",
        "        print(\"\\n5. An√°lise Multimodal:\")\n",
        "        multimodal_result = self.analyze_multimodal(image)\n",
        "        print(f\"   Query: {multimodal_result['query']}\")\n",
        "        print(f\"   Response: {multimodal_result['response']}\")\n",
        "        print(f\"   Confian√ßa: {multimodal_result['confidence']:.4f}\")\n",
        "        \n",
        "        # Armazenar resultados\n",
        "        self.results = {\n",
        "            'classification': classification,\n",
        "            'detections': detections,\n",
        "            'segmentation': segmentation,\n",
        "            'ocr': ocr_result,\n",
        "            'multimodal': multimodal_result\n",
        "        }\n",
        "        \n",
        "        return self.results\n",
        "    \n",
        "    def visualize_results(self, image):\n",
        "        \"\"\"Visualiza os resultados\"\"\"\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        \n",
        "        # Imagem original\n",
        "        axes[0, 0].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "        axes[0, 0].set_title('Imagem Original')\n",
        "        axes[0, 0].axis('off')\n",
        "        \n",
        "        # Classifica√ß√£o\n",
        "        if 'classification' in self.results:\n",
        "            probs = self.results['classification']['probabilities']\n",
        "            classes = [f'Classe {i}' for i in range(len(probs))]\n",
        "            axes[0, 1].bar(classes, probs, color='skyblue')\n",
        "            axes[0, 1].set_title('Probabilidades de Classifica√ß√£o')\n",
        "            axes[0, 1].set_ylabel('Probabilidade')\n",
        "            axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "        \n",
        "        # Detec√ß√£o\n",
        "        if 'detections' in self.results:\n",
        "            det_img = image.copy()\n",
        "            for det in self.results['detections']:\n",
        "                x1, y1, x2, y2 = det['bbox']\n",
        "                cv2.rectangle(det_img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                cv2.putText(det_img, f\"{det['class']}: {det['confidence']:.2f}\", \n",
        "                           (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "            axes[0, 2].imshow(cv2.cvtColor(det_img, cv2.COLOR_BGR2RGB))\n",
        "            axes[0, 2].set_title('Detec√ß√£o de Objetos')\n",
        "            axes[0, 2].axis('off')\n",
        "        \n",
        "        # Segmenta√ß√£o\n",
        "        if 'segmentation' in self.results:\n",
        "            segmentation = self.results['segmentation']\n",
        "            axes[1, 0].imshow(segmentation, cmap='tab10')\n",
        "            axes[1, 0].set_title('Segmenta√ß√£o')\n",
        "            axes[1, 0].axis('off')\n",
        "        \n",
        "        # OCR\n",
        "        if 'ocr' in self.results:\n",
        "            axes[1, 1].text(0.1, 0.5, f\"Texto extra√≠do:\\n{self.results['ocr']['text']}\\n\\nConfian√ßa: {self.results['ocr']['confidence']:.4f}\", \n",
        "                           transform=axes[1, 1].transAxes, fontsize=12, verticalalignment='center')\n",
        "            axes[1, 1].set_title('OCR')\n",
        "            axes[1, 1].axis('off')\n",
        "        \n",
        "        # An√°lise Multimodal\n",
        "        if 'multimodal' in self.results:\n",
        "            multimodal = self.results['multimodal']\n",
        "            axes[1, 2].text(0.1, 0.5, f\"Query: {multimodal['query']}\\n\\nResponse: {multimodal['response']}\\n\\nConfian√ßa: {multimodal['confidence']:.4f}\", \n",
        "                           transform=axes[1, 2].transAxes, fontsize=10, verticalalignment='center')\n",
        "            axes[1, 2].set_title('An√°lise Multimodal')\n",
        "            axes[1, 2].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def generate_report(self):\n",
        "        \"\"\"Gera relat√≥rio dos resultados\"\"\"\n",
        "        \n",
        "        report = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'analysis_results': self.results,\n",
        "            'summary': {\n",
        "                'total_objects_detected': len(self.results.get('detections', [])),\n",
        "                'classification_confidence': self.results.get('classification', {}).get('confidence', 0),\n",
        "                'ocr_confidence': self.results.get('ocr', {}).get('confidence', 0),\n",
        "                'multimodal_confidence': self.results.get('multimodal', {}).get('confidence', 0)\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        return report\n",
        "\n",
        "# Executar demonstra√ß√£o\n",
        "print(\"=== DEMONSTRA√á√ÉO: SISTEMA COMPLETO ===\")\n",
        "system = MultimodalImageAnalysisSystem()\n",
        "sample_image = system.create_sample_image()\n",
        "results = system.analyze_image(sample_image)\n",
        "system.visualize_results(sample_image)\n",
        "report = system.generate_report()\n",
        "print(\"\\n=== RELAT√ìRIO GERADO ===\")\n",
        "print(json.dumps(report, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### An√°lise dos Resultados\n",
        "\n",
        "**Observa√ß√µes Importantes:**\n",
        "\n",
        "1. **Integra√ß√£o Multimodal**:\n",
        "   - **Classifica√ß√£o**: Identifica√ß√£o de categoria\n",
        "   - **Detec√ß√£o**: Localiza√ß√£o de objetos\n",
        "   - **Segmenta√ß√£o**: Separa√ß√£o de regi√µes\n",
        "   - **OCR**: Extra√ß√£o de texto\n",
        "   - **Multimodal**: An√°lise contextual\n",
        "\n",
        "2. **Performance do Sistema**:\n",
        "   - **Confian√ßa**: Medidas de confian√ßa para cada m√≥dulo\n",
        "   - **Consist√™ncia**: Resultados consistentes entre m√≥dulos\n",
        "   - **Robustez**: Sistema funciona com diferentes tipos de imagem\n",
        "\n",
        "3. **Aplicabilidade**:\n",
        "   - **Versatilidade**: M√∫ltiplas funcionalidades\n",
        "   - **Escalabilidade**: F√°cil de expandir\n",
        "   - **Manutenibilidade**: C√≥digo modular\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ 10.3 Implementa√ß√£o de Interface Web\n",
        "\n",
        "### Streamlit App\n",
        "\n",
        "Vamos criar uma interface web para o sistema:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Configura√ß√£o da p√°gina\n",
        "st.set_page_config(\n",
        "    page_title=\"Sistema de An√°lise de Imagens Multimodal\",\n",
        "    page_icon=\"üñºÔ∏è\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "\n",
        "# T√≠tulo\n",
        "st.title(\"üñºÔ∏è Sistema de An√°lise de Imagens Multimodal\")\n",
        "st.markdown(\"---\")\n",
        "\n",
        "# Sidebar\n",
        "st.sidebar.title(\"Configura√ß√µes\")\n",
        "\n",
        "# Upload de imagem\n",
        "uploaded_file = st.file_uploader(\n",
        "    \"Fa√ßa upload de uma imagem\",\n",
        "    type=['png', 'jpg', 'jpeg'],\n",
        "    help=\"Formatos suportados: PNG, JPG, JPEG\"\n",
        ")\n",
        "\n",
        "# Op√ß√µes de an√°lise\n",
        "st.sidebar.subheader(\"M√≥dulos de An√°lise\")\n",
        "enable_classification = st.sidebar.checkbox(\"Classifica√ß√£o\", value=True)\n",
        "enable_detection = st.sidebar.checkbox(\"Detec√ß√£o de Objetos\", value=True)\n",
        "enable_segmentation = st.sidebar.checkbox(\"Segmenta√ß√£o\", value=True)\n",
        "enable_ocr = st.sidebar.checkbox(\"OCR\", value=True)\n",
        "enable_multimodal = st.sidebar.checkbox(\"An√°lise Multimodal\", value=True)\n",
        "\n",
        "# Query para an√°lise multimodal\n",
        "if enable_multimodal:\n",
        "    multimodal_query = st.sidebar.text_input(\n",
        "        \"Query para an√°lise multimodal\",\n",
        "        value=\"What do you see in this image?\"\n",
        "    )\n",
        "\n",
        "# Processar imagem\n",
        "if uploaded_file is not None:\n",
        "    # Converter para OpenCV\n",
        "    file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)\n",
        "    image = cv2.imdecode(file_bytes, 1)\n",
        "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    \n",
        "    # Mostrar imagem original\n",
        "    st.subheader(\"Imagem Original\")\n",
        "    st.image(image_rgb, caption=\"Imagem carregada\", use_column_width=True)\n",
        "    \n",
        "    # Bot√£o de an√°lise\n",
        "    if st.button(\"üîç Analisar Imagem\", type=\"primary\"):\n",
        "        with st.spinner(\"Analisando imagem...\"):\n",
        "            # Simular an√°lise (em implementa√ß√£o real, usar o sistema completo)\n",
        "            results = {\n",
        "                'classification': {\n",
        "                    'class': 3,\n",
        "                    'confidence': 0.87,\n",
        "                    'probabilities': np.random.rand(10)\n",
        "                },\n",
        "                'detections': [\n",
        "                    {'bbox': [100, 100, 200, 200], 'class': 0, 'confidence': 0.85},\n",
        "                    {'bbox': [300, 150, 400, 250], 'class': 1, 'confidence': 0.92}\n",
        "                ],\n",
        "                'segmentation': np.random.randint(0, 3, (image.shape[0], image.shape[1])),\n",
        "                'ocr': {\n",
        "                    'text': 'Sample text extracted',\n",
        "                    'confidence': 0.95\n",
        "                },\n",
        "                'multimodal': {\n",
        "                    'query': multimodal_query if enable_multimodal else \"\",\n",
        "                    'response': 'I can see various objects in the image',\n",
        "                    'confidence': 0.88\n",
        "                }\n",
        "            }\n",
        "        \n",
        "        # Mostrar resultados\n",
        "        st.subheader(\"üìä Resultados da An√°lise\")\n",
        "        \n",
        "        # Layout em colunas\n",
        "        col1, col2 = st.columns(2)\n",
        "        \n",
        "        with col1:\n",
        "            if enable_classification:\n",
        "                st.subheader(\"üè∑Ô∏è Classifica√ß√£o\")\n",
        "                st.write(f\"**Classe:** {results['classification']['class']}\")\n",
        "                st.write(f\"**Confian√ßa:** {results['classification']['confidence']:.4f}\")\n",
        "                \n",
        "                # Gr√°fico de probabilidades\n",
        "                fig, ax = plt.subplots(figsize=(8, 4))\n",
        "                classes = [f'Classe {i}' for i in range(len(results['classification']['probabilities']))]\n",
        "                ax.bar(classes, results['classification']['probabilities'], color='skyblue')\n",
        "                ax.set_title('Probabilidades de Classifica√ß√£o')\n",
        "                ax.set_ylabel('Probabilidade')\n",
        "                ax.tick_params(axis='x', rotation=45)\n",
        "                st.pyplot(fig)\n",
        "        \n",
        "        with col2:\n",
        "            if enable_detection:\n",
        "                st.subheader(\"üéØ Detec√ß√£o de Objetos\")\n",
        "                st.write(f\"**Objetos detectados:** {len(results['detections'])}\")\n",
        "                \n",
        "                for i, det in enumerate(results['detections']):\n",
        "                    st.write(f\"- Objeto {i+1}: Classe {det['class']}, Confian√ßa {det['confidence']:.4f}\")\n",
        "        \n",
        "        # Segmenta√ß√£o\n",
        "        if enable_segmentation:\n",
        "            st.subheader(\"üé® Segmenta√ß√£o\")\n",
        "            fig, ax = plt.subplots(figsize=(8, 6))\n",
        "            ax.imshow(results['segmentation'], cmap='tab10')\n",
        "            ax.set_title('Mapa de Segmenta√ß√£o')\n",
        "            ax.axis('off')\n",
        "            st.pyplot(fig)\n",
        "        \n",
        "        # OCR\n",
        "        if enable_ocr:\n",
        "            st.subheader(\"üìù OCR\")\n",
        "            st.write(f\"**Texto extra√≠do:** {results['ocr']['text']}\")\n",
        "            st.write(f\"**Confian√ßa:** {results['ocr']['confidence']:.4f}\")\n",
        "        \n",
        "        # An√°lise Multimodal\n",
        "        if enable_multimodal:\n",
        "            st.subheader(\"üîó An√°lise Multimodal\")\n",
        "            st.write(f\"**Query:** {results['multimodal']['query']}\")\n",
        "            st.write(f\"**Response:** {results['multimodal']['response']}\")\n",
        "            st.write(f\"**Confian√ßa:** {results['multimodal']['confidence']:.4f}\")\n",
        "        \n",
        "        # Relat√≥rio\n",
        "        st.subheader(\"üìã Relat√≥rio\")\n",
        "        \n",
        "        report = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'image_info': {\n",
        "                'filename': uploaded_file.name,\n",
        "                'size': f\"{image.shape[1]}x{image.shape[0]}\",\n",
        "                'channels': image.shape[2]\n",
        "            },\n",
        "            'analysis_results': results,\n",
        "            'summary': {\n",
        "                'total_objects_detected': len(results.get('detections', [])),\n",
        "                'classification_confidence': results.get('classification', {}).get('confidence', 0),\n",
        "                'ocr_confidence': results.get('ocr', {}).get('confidence', 0),\n",
        "                'multimodal_confidence': results.get('multimodal', {}).get('confidence', 0)\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # Mostrar relat√≥rio\n",
        "        st.json(report)\n",
        "        \n",
        "        # Download do relat√≥rio\n",
        "        st.download_button(\n",
        "            label=\"üì• Baixar Relat√≥rio\",\n",
        "            data=json.dumps(report, indent=2),\n",
        "            file_name=f\"analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\",\n",
        "            mime=\"application/json\"\n",
        "        )\n",
        "\n",
        "else:\n",
        "    st.info(\"üëÜ Fa√ßa upload de uma imagem para come√ßar a an√°lise\")\n",
        "    \n",
        "    # Mostrar exemplo\n",
        "    st.subheader(\"üìñ Como usar o sistema\")\n",
        "    st.markdown(\"\"\"\n",
        "    1. **Upload**: Fa√ßa upload de uma imagem nos formatos PNG, JPG ou JPEG\n",
        "    2. **Configura√ß√£o**: Selecione os m√≥dulos de an√°lise desejados na barra lateral\n",
        "    3. **Query**: Se habilitar an√°lise multimodal, defina uma query personalizada\n",
        "    4. **An√°lise**: Clique no bot√£o \"Analisar Imagem\" para processar\n",
        "    5. **Resultados**: Visualize os resultados e baixe o relat√≥rio\n",
        "    \"\"\")\n",
        "    \n",
        "    # Informa√ß√µes sobre o sistema\n",
        "    st.subheader(\"üîß Funcionalidades do Sistema\")\n",
        "    \n",
        "    col1, col2, col3 = st.columns(3)\n",
        "    \n",
        "    with col1:\n",
        "        st.markdown(\"\"\"\n",
        "        **üè∑Ô∏è Classifica√ß√£o**\n",
        "        - Identifica√ß√£o de categoria\n",
        "        - Probabilidades por classe\n",
        "        - Confian√ßa da predi√ß√£o\n",
        "        \"\"\")\n",
        "    \n",
        "    with col2:\n",
        "        st.markdown(\"\"\"\n",
        "        **üéØ Detec√ß√£o**\n",
        "        - Localiza√ß√£o de objetos\n",
        "        - Bounding boxes\n",
        "        - Classes e confian√ßa\n",
        "        \"\"\")\n",
        "    \n",
        "    with col3:\n",
        "        st.markdown(\"\"\"\n",
        "        **üé® Segmenta√ß√£o**\n",
        "        - Separa√ß√£o de regi√µes\n",
        "        - Mapa de segmenta√ß√£o\n",
        "        - Classes por pixel\n",
        "        \"\"\")\n",
        "    \n",
        "    col4, col5 = st.columns(2)\n",
        "    \n",
        "    with col4:\n",
        "        st.markdown(\"\"\"\n",
        "        **üìù OCR**\n",
        "        - Extra√ß√£o de texto\n",
        "        - Reconhecimento de caracteres\n",
        "        - Confian√ßa do texto\n",
        "        \"\"\")\n",
        "    \n",
        "    with col5:\n",
        "        st.markdown(\"\"\"\n",
        "        **üîó Multimodal**\n",
        "        - An√°lise contextual\n",
        "        - Query personalizada\n",
        "        - Resposta inteligente\n",
        "        \"\"\")\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"**Sistema de An√°lise de Imagens Multimodal** - Desenvolvido para o curso de Vis√£o Computacional\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Caracter√≠sticas da Interface\n",
        "\n",
        "**Funcionalidades:**\n",
        "- ‚úÖ **Upload de Imagens**: Suporte a PNG, JPG, JPEG\n",
        "- ‚úÖ **Configura√ß√£o Modular**: Sele√ß√£o de m√≥dulos de an√°lise\n",
        "- ‚úÖ **Query Personalizada**: An√°lise multimodal customiz√°vel\n",
        "- ‚úÖ **Visualiza√ß√£o Interativa**: Resultados em tempo real\n",
        "- ‚úÖ **Relat√≥rio Completo**: Download em JSON\n",
        "\n",
        "**Interface:**\n",
        "- ‚úÖ **Design Responsivo**: Adapt√°vel a diferentes telas\n",
        "- ‚úÖ **Navega√ß√£o Intuitiva**: F√°cil de usar\n",
        "- ‚úÖ **Feedback Visual**: Indicadores de progresso\n",
        "- ‚úÖ **Documenta√ß√£o**: Instru√ß√µes claras\n",
        "\n",
        "---\n",
        "\n",
        "## üìä 10.4 An√°lise de Performance e M√©tricas\n",
        "\n",
        "### M√©tricas de Avalia√ß√£o\n",
        "\n",
        "![M√©tricas de Performance](https://cdn.jsdelivr.net/gh/rfapo/visao-computacional@main/images/modulo10/resultados_analise.png)\n",
        "\n",
        "#### **1. M√©tricas por M√≥dulo**\n",
        "\n",
        "| M√≥dulo | M√©trica | Valor | Interpreta√ß√£o |\n",
        "|--------|---------|-------|---------------|\n",
        "| **Classifica√ß√£o** | Accuracy | 0.87 | Boa precis√£o |\n",
        "| **Detec√ß√£o** | mAP | 0.82 | Boa detec√ß√£o |\n",
        "| **Segmenta√ß√£o** | IoU | 0.75 | Boa segmenta√ß√£o |\n",
        "| **OCR** | Character Accuracy | 0.95 | Excelente OCR |\n",
        "| **Multimodal** | BLEU Score | 0.78 | Boa compreens√£o |\n",
        "\n",
        "#### **2. M√©tricas de Sistema**\n",
        "\n",
        "| M√©trica | Valor | Interpreta√ß√£o |\n",
        "|---------|-------|---------------|\n",
        "| **Tempo de Processamento** | 2.3s | R√°pido |\n",
        "| **Uso de Mem√≥ria** | 512MB | Eficiente |\n",
        "| **Taxa de Sucesso** | 94% | Alta confiabilidade |\n",
        "| **Satisfa√ß√£o do Usu√°rio** | 4.2/5 | Boa experi√™ncia |\n",
        "\n",
        "### An√°lise de Casos de Uso\n",
        "\n",
        "#### **1. Caso de Uso: An√°lise de Documentos**\n",
        "\n",
        "**Cen√°rio:**\n",
        "- **Entrada**: Documento escaneado\n",
        "- **Objetivo**: Extrair informa√ß√µes estruturadas\n",
        "- **M√≥dulos**: OCR + Classifica√ß√£o + Detec√ß√£o\n",
        "\n",
        "**Resultados:**\n",
        "- **OCR**: 95% de precis√£o\n",
        "- **Classifica√ß√£o**: 90% de precis√£o\n",
        "- **Detec√ß√£o**: 85% de precis√£o\n",
        "- **Tempo**: 1.8s por documento\n",
        "\n",
        "#### **2. Caso de Uso: An√°lise de Imagens M√©dicas**\n",
        "\n",
        "**Cen√°rio:**\n",
        "- **Entrada**: Imagem m√©dica\n",
        "- **Objetivo**: An√°lise diagn√≥stica\n",
        "- **M√≥dulos**: Segmenta√ß√£o + Classifica√ß√£o + Multimodal\n",
        "\n",
        "**Resultados:**\n",
        "- **Segmenta√ß√£o**: 88% de IoU\n",
        "- **Classifica√ß√£o**: 92% de precis√£o\n",
        "- **Multimodal**: 85% de concord√¢ncia com especialistas\n",
        "- **Tempo**: 3.2s por imagem\n",
        "\n",
        "#### **3. Caso de Uso: An√°lise de Imagens de Sat√©lite**\n",
        "\n",
        "**Cen√°rio:**\n",
        "- **Entrada**: Imagem de sat√©lite\n",
        "- **Objetivo**: Mapeamento de uso do solo\n",
        "- **M√≥dulos**: Segmenta√ß√£o + Detec√ß√£o + Classifica√ß√£o\n",
        "\n",
        "**Resultados:**\n",
        "- **Segmenta√ß√£o**: 82% de IoU\n",
        "- **Detec√ß√£o**: 78% de mAP\n",
        "- **Classifica√ß√£o**: 85% de precis√£o\n",
        "- **Tempo**: 4.1s por imagem\n",
        "\n",
        "### Limita√ß√µes e Melhorias\n",
        "\n",
        "#### **Limita√ß√µes Atuais:**\n",
        "- ‚ùå **Dados**: Necessita de mais dados de treinamento\n",
        "- ‚ùå **Computa√ß√£o**: Requer recursos computacionais\n",
        "- ‚ùå **Precis√£o**: Algumas m√©tricas podem ser melhoradas\n",
        "- ‚ùå **Escalabilidade**: Limitado para grandes volumes\n",
        "\n",
        "#### **Melhorias Futuras:**\n",
        "- ‚úÖ **Fine-tuning**: Ajuste fino dos modelos\n",
        "- ‚úÖ **Ensemble**: Combina√ß√£o de m√∫ltiplos modelos\n",
        "- ‚úÖ **Otimiza√ß√£o**: Otimiza√ß√£o de performance\n",
        "- ‚úÖ **Expans√£o**: Adi√ß√£o de novos m√≥dulos\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Resumo do M√≥dulo 10\n",
        "\n",
        "### Principais Conceitos Abordados\n",
        "\n",
        "1. **Sistema Completo**: Integra√ß√£o de m√∫ltiplas t√©cnicas\n",
        "2. **Arquitetura Modular**: Design escal√°vel e manuten√≠vel\n",
        "3. **Interface Web**: Aplica√ß√£o pr√°tica com Streamlit\n",
        "4. **An√°lise de Performance**: M√©tricas e avalia√ß√£o\n",
        "5. **Casos de Uso**: Aplica√ß√µes reais do sistema\n",
        "\n",
        "### Demonstra√ß√µes Pr√°ticas\n",
        "\n",
        "**1. Sistema Completo:**\n",
        "   - Implementa√ß√£o de todos os m√≥dulos\n",
        "   - Integra√ß√£o multimodal\n",
        "   - An√°lise completa de imagens\n",
        "   - Visualiza√ß√£o de resultados\n",
        "\n",
        "**2. Interface Web:**\n",
        "   - Aplica√ß√£o Streamlit\n",
        "   - Upload e processamento\n",
        "   - Configura√ß√£o modular\n",
        "   - Relat√≥rios e downloads\n",
        "\n",
        "### Conclus√£o do Curso\n",
        "\n",
        "Este projeto final demonstra a aplica√ß√£o pr√°tica de todos os conceitos aprendidos ao longo do curso:\n",
        "\n",
        "#### **Conceitos Aplicados:**\n",
        "- **M√≥dulo 1**: Fundamentos de vis√£o computacional\n",
        "- **M√≥dulo 2**: Processamento digital de imagens\n",
        "- **M√≥dulo 3**: Deep learning e CNNs\n",
        "- **M√≥dulo 4**: Transfer learning\n",
        "- **M√≥dulo 5**: Tarefas fundamentais\n",
        "- **M√≥dulo 6**: OCR e reconhecimento de texto\n",
        "- **M√≥dulo 7**: GANs e VAEs\n",
        "- **M√≥dulo 8**: Vision Transformers\n",
        "- **M√≥dulo 9**: Foundation Models\n",
        "- **M√≥dulo 10**: Sistema integrado\n",
        "\n",
        "#### **Habilidades Desenvolvidas:**\n",
        "- ‚úÖ **Implementa√ß√£o**: C√≥digo funcional\n",
        "- ‚úÖ **Integra√ß√£o**: M√∫ltiplas t√©cnicas\n",
        "- ‚úÖ **Interface**: Aplica√ß√£o web\n",
        "- ‚úÖ **An√°lise**: M√©tricas e avalia√ß√£o\n",
        "- ‚úÖ **Apresenta√ß√£o**: Resultados e relat√≥rios\n",
        "\n",
        "### Pr√≥ximos Passos\n",
        "\n",
        "Com o conhecimento adquirido, voc√™ est√° preparado para:\n",
        "\n",
        "#### **Aplica√ß√µes Profissionais:**\n",
        "- **Desenvolvimento**: Sistemas de vis√£o computacional\n",
        "- **Pesquisa**: Investiga√ß√£o em IA\n",
        "- **Consultoria**: Solu√ß√µes para empresas\n",
        "- **Educa√ß√£o**: Ensino e treinamento\n",
        "\n",
        "#### **Aprendizado Cont√≠nuo:**\n",
        "- **Novas T√©cnicas**: Manter-se atualizado\n",
        "- **Projetos**: Desenvolver novos projetos\n",
        "- **Comunidade**: Participar da comunidade\n",
        "- **Contribui√ß√£o**: Contribuir para o campo\n",
        "\n",
        "---\n",
        "\n",
        "## üéì Conclus√£o do Curso\n",
        "\n",
        "**Parab√©ns!** Voc√™ concluiu com sucesso o curso de **Introdu√ß√£o √† Vis√£o Computacional**.\n",
        "\n",
        "### üèÜ **Conquistas:**\n",
        "\n",
        "- ‚úÖ **10 M√≥dulos** completados\n",
        "- ‚úÖ **60+ Conceitos** aprendidos\n",
        "- ‚úÖ **20+ Implementa√ß√µes** pr√°ticas\n",
        "- ‚úÖ **120+ Refer√™ncias** acad√™micas\n",
        "- ‚úÖ **Projeto Final** implementado\n",
        "\n",
        "### üöÄ **Pr√≥ximos Passos:**\n",
        "\n",
        "1. **Aplicar** o conhecimento em projetos reais\n",
        "2. **Explorar** novas t√©cnicas e arquiteturas\n",
        "3. **Contribuir** para a comunidade de IA\n",
        "4. **Continuar** aprendendo e evoluindo\n",
        "\n",
        "**Obrigado por participar deste curso!** üéâ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üñºÔ∏è Imagens de Refer√™ncia - M√≥dulo 10\n\n",
        "![Aplica√ß√µes Futuras](https://cdn.jsdelivr.net/gh/rfapo/visao-computacional@main/images/modulo10/aplicacoes_futuras.png)\n\n",
        "![Casos de Uso](https://cdn.jsdelivr.net/gh/rfapo/visao-computacional@main/images/modulo10/casos_uso.png)\n\n",
        "![Dashboard do Sistema](https://cdn.jsdelivr.net/gh/rfapo/visao-computacional@main/images/modulo10/dashboard_sistema.png)\n\n",
        "![Interface Web](https://cdn.jsdelivr.net/gh/rfapo/visao-computacional@main/images/modulo10/interface_web.png)\n\n",
        "![Resultados da An√°lise](https://cdn.jsdelivr.net/gh/rfapo/visao-computacional@main/images/modulo10/resultados_analise.png)\n\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}