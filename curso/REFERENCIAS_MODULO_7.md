# ðŸ“š ReferÃªncias BibliogrÃ¡ficas - MÃ³dulo 7: GANs e VAEs

> Este documento complementa o [MÃ³dulo 7: GANs e VAEs - GeraÃ§Ã£o SintÃ©tica de Imagens](07_gans_vaes_geracao_sintetica.ipynb)

---

## Papers Fundamentais

### GANs (Generative Adversarial Networks)

1. **Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2014)**
   *Generative adversarial nets*
   Advances in neural information processing systems, 27.
   ðŸ“„ [arXiv:1406.2661](https://arxiv.org/abs/1406.2661)
   ðŸ’¡ **Paper original que introduziu GANs** - Revolucionou geraÃ§Ã£o de imagens com framework adversarial

2. **Radford, A., Metz, L., & Chintala, S. (2015)**
   *Unsupervised representation learning with deep convolutional generative adversarial networks*
   arXiv preprint arXiv:1511.06434.
   ðŸ“„ [arXiv:1511.06434](https://arxiv.org/abs/1511.06434)
   ðŸ’¡ **DCGAN** - Arquitetura convolucional que estabilizou treinamento de GANs

3. **Arjovsky, M., Chintala, S., & Bottou, L. (2017)**
   *Wasserstein generative adversarial networks*
   International conference on machine learning (pp. 214-223). PMLR.
   ðŸ“„ [arXiv:1701.07875](https://arxiv.org/abs/1701.07875)
   ðŸ’¡ **WGAN** - SoluÃ§Ã£o para instabilidade usando distÃ¢ncia de Wasserstein

4. **Karras, T., Laine, S., & Aila, T. (2019)**
   *A style-based generator architecture for generative adversarial networks*
   Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 4401-4410).
   ðŸ“„ [arXiv:1812.04948](https://arxiv.org/abs/1812.04948)
   ðŸ’¡ **StyleGAN** - Controle de estilo hierÃ¡rquico para geraÃ§Ã£o de faces realistas

### VAEs (Variational Autoencoders)

5. **Kingma, D. P., & Welling, M. (2013)**
   *Auto-encoding variational bayes*
   arXiv preprint arXiv:1312.6114.
   ðŸ“„ [arXiv:1312.6114](https://arxiv.org/abs/1312.6114)
   ðŸ’¡ **Paper original de VAEs** - Introduziu reparameterization trick e ELBO

6. **Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., ... & Lerchner, A. (2017)**
   *beta-vae: Learning basic visual concepts with a constrained variational framework*
   ICLR.
   ðŸ“„ [OpenReview](https://openreview.net/forum?id=Sy2fzU9gl)
   ðŸ’¡ **Î²-VAE** - Aprendizado de representaÃ§Ãµes disentangled

7. **van den Oord, A., Vinyals, O., & Kavukcuoglu, K. (2017)**
   *Neural discrete representation learning*
   Advances in neural information processing systems, 30.
   ðŸ“„ [arXiv:1711.00937](https://arxiv.org/abs/1711.00937)
   ðŸ’¡ **VQ-VAE** - QuantizaÃ§Ã£o vetorial para representaÃ§Ãµes discretas

---

## Tutoriais e Surveys

8. **Goodfellow, I. (2016)**
   *NIPS 2016 tutorial: Generative adversarial networks*
   arXiv preprint arXiv:1701.00160.
   ðŸ“„ [arXiv:1701.00160](https://arxiv.org/abs/1701.00160)
   ðŸ’¡ Tutorial oficial do criador das GANs

9. **Doersch, C. (2016)**
   *Tutorial on variational autoencoders*
   arXiv preprint arXiv:1606.05908.
   ðŸ“„ [arXiv:1606.05908](https://arxiv.org/abs/1606.05908)
   ðŸ’¡ Tutorial didÃ¡tico e completo sobre VAEs

10. **Creswell, A., White, T., Dumoulin, V., Arulkumaran, K., Sengupta, B., & Bharath, A. A. (2018)**
    *Generative adversarial networks: An overview*
    IEEE signal processing magazine, 35(1), 53-65.
    ðŸ“„ [arXiv:1710.07035](https://arxiv.org/abs/1710.07035)
    ðŸ’¡ Survey abrangente sobre GANs

---

## Variantes e Melhorias

### GANs AvanÃ§adas

11. **Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., & Courville, A. C. (2017)**
    *Improved training of wasserstein gans*
    Advances in neural information processing systems, 30.
    ðŸ“„ [arXiv:1704.00028](https://arxiv.org/abs/1704.00028)
    ðŸ’¡ **WGAN-GP** - Gradient penalty para estabilidade

12. **Karras, T., Aila, T., Laine, S., & Lehtinen, J. (2017)**
    *Progressive growing of gans for improved quality, stability, and variation*
    arXiv preprint arXiv:1710.10196.
    ðŸ“„ [arXiv:1710.10196](https://arxiv.org/abs/1710.10196)
    ðŸ’¡ **Progressive GAN** - Crescimento progressivo para alta resoluÃ§Ã£o

13. **Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., & Aila, T. (2020)**
    *Analyzing and improving the image quality of stylegan*
    Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 8110-8119).
    ðŸ“„ [arXiv:1912.04958](https://arxiv.org/abs/1912.04958)
    ðŸ’¡ **StyleGAN2** - Melhorias de qualidade e remoÃ§Ã£o de artifacts

### VAEs AvanÃ§ados

14. **Razavi, A., Van den Oord, A., & Vinyals, O. (2019)**
    *Generating diverse high-fidelity images with vq-vae-2*
    Advances in neural information processing systems, 32.
    ðŸ“„ [arXiv:1906.00446](https://arxiv.org/abs/1906.00446)
    ðŸ’¡ **VQ-VAE-2** - Alta fidelidade com arquitetura hierÃ¡rquica

15. **Kingma, D. P., & Welling, M. (2019)**
    *An introduction to variational autoencoders*
    Foundations and Trends in Machine Learning, 12(4), 307-392.
    ðŸ“„ [arXiv:1906.02691](https://arxiv.org/abs/1906.02691)
    ðŸ’¡ Tutorial completo e atualizado sobre VAEs

---

## MÃ©tricas de AvaliaÃ§Ã£o

16. **Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., & Hochreiter, S. (2017)**
    *GANs trained by a two time-scale update rule converge to a local nash equilibrium*
    Advances in neural information processing systems, 30.
    ðŸ“„ [arXiv:1706.08500](https://arxiv.org/abs/1706.08500)
    ðŸ’¡ **FID (FrÃ©chet Inception Distance)** - MÃ©trica padrÃ£o para avaliar GANs

17. **Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., & Chen, X. (2016)**
    *Improved techniques for training gans*
    Advances in neural information processing systems, 29.
    ðŸ“„ [arXiv:1606.03498](https://arxiv.org/abs/1606.03498)
    ðŸ’¡ **Inception Score** e tÃ©cnicas de treinamento

---

## Livros Recomendados

ðŸ“– **Goodfellow, I., Bengio, Y., & Courville, A. (2016)**
*Deep learning*
MIT press.
- CapÃ­tulo 20: Deep Generative Models
- [Livro Online](https://www.deeplearningbook.org/)

ðŸ“– **Murphy, K. P. (2022)**
*Probabilistic machine learning: An introduction*
MIT press.
- CapÃ­tulo 20: Variational Inference
- [Livro Online](https://probml.github.io/pml-book/book1.html)

ðŸ“– **Murphy, K. P. (2023)**
*Probabilistic machine learning: Advanced topics*
MIT press.
- CapÃ­tulo 25: Deep Generative Models
- [Livro Online](https://probml.github.io/pml-book/book2.html)

---

## Recursos Online

### Artigos Distill.pub
- [Deconvolution and Checkerboard Artifacts](https://distill.pub/2016/deconv-checkerboard/)
- [Feature Visualization](https://distill.pub/2017/feature-visualization/)

### PyTorch Tutorials
- [DCGAN Tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html)
- [VAE Tutorial](https://github.com/pytorch/examples/tree/main/vae)

### ImplementaÃ§Ãµes de ReferÃªncia
- [PyTorch-GAN](https://github.com/eriklindernoren/PyTorch-GAN) - ImplementaÃ§Ãµes de diversas arquiteturas GAN
- [StyleGAN2-ADA-PyTorch](https://github.com/NVlabs/stylegan2-ada-pytorch) - ImplementaÃ§Ã£o oficial StyleGAN2
- [Stable Diffusion](https://github.com/CompVis/stable-diffusion) - Estado da arte em geraÃ§Ã£o

---

## Estado da Arte (2023-2024)

18. **Karras, T., Aittala, M., Laine, S., HÃ¤rkÃ¶nen, E., Hellsten, J., Lehtinen, J., & Aila, T. (2021)**
    *Alias-free generative adversarial networks*
    Advances in Neural Information Processing Systems, 34, 852-863.
    ðŸ“„ [arXiv:2106.12423](https://arxiv.org/abs/2106.12423)
    ðŸ’¡ **StyleGAN3** - RotaÃ§Ã£o e translaÃ§Ã£o equivariantes

19. **Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022)**
    *High-resolution image synthesis with latent diffusion models*
    Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 10684-10695).
    ðŸ“„ [arXiv:2112.10752](https://arxiv.org/abs/2112.10752)
    ðŸ’¡ **Stable Diffusion** - Diffusion models em espaÃ§o latente

20. **Ho, J., Jain, A., & Abbeel, P. (2020)**
    *Denoising diffusion probabilistic models*
    Advances in neural information processing systems, 33, 6840-6851.
    ðŸ“„ [arXiv:2006.11239](https://arxiv.org/abs/2006.11239)
    ðŸ’¡ **DDPM** - Fundamentos de diffusion models

---

## ExercÃ­cios Propostos para Mestrado

### NÃ­vel TeÃ³rico

1. **Teoria dos Jogos**: Demonstre formalmente que o equilÃ­brio de Nash da GAN corresponde a p_g = p_data

2. **ELBO Derivation**: Derive completamente o ELBO para VAE, incluindo a forma fechada da KL divergence para distribuiÃ§Ãµes Gaussianas

3. **Wasserstein Distance**: Prove que a distÃ¢ncia de Wasserstein fornece gradientes Ãºteis mesmo quando suportes de p_data e p_g nÃ£o se sobrepÃµem

### NÃ­vel ImplementaÃ§Ã£o

4. **DCGAN**: Implemente uma DCGAN completa para CIFAR-10 e compare com a implementaÃ§Ã£o MLP

5. **Conditional GAN**: Estenda o GAN do mÃ³dulo para geraÃ§Ã£o condicional (escolha da classe)

6. **Î²-VAE**: Implemente um Î²-VAE e experimente com diferentes valores de Î². Visualize o efeito no espaÃ§o latente

### NÃ­vel Pesquisa

7. **MÃ©tricas de AvaliaÃ§Ã£o**: Implemente FID (FrÃ©chet Inception Distance) e IS (Inception Score) para avaliar seus modelos

8. **Mode Collapse**: Experimente tÃ©cnicas para mitigar mode collapse (Unrolled GAN, Minibatch Discrimination) e compare quantitativamente

9. **Ablation Study**: Realize um estudo de ablaÃ§Ã£o removendo componentes (BatchNorm, Dropout, LeakyReLU) e analise o impacto

---

## Links Ãšteis

- [GAN Lab - VisualizaÃ§Ã£o Interativa](https://poloclub.github.io/ganlab/)
- [This Person Does Not Exist](https://thispersondoesnotexist.com/) - DemonstraÃ§Ã£o de StyleGAN
- [Two Minute Papers - GANs](https://www.youtube.com/watch?v=kSLJriaOumA)

---

**Ãšltima atualizaÃ§Ã£o**: Novembro 2024
**Curso**: VisÃ£o Computacional - Mestrado
**Professor**: Rodrigo Fapo
