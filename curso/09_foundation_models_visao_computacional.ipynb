{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M√≥dulo 9: Foundation Models para Vis√£o Computacional",
    "",
    "## üéØ Objetivos de Aprendizagem",
    "",
    "Ao final deste m√≥dulo, voc√™ ser√° capaz de:",
    "",
    "- ‚úÖ Compreender o conceito de Foundation Models",
    "- ‚úÖ Conhecer modelos como CLIP, DALL-E e GPT-4V",
    "- ‚úÖ Implementar solu√ß√µes com APIs do OpenAI e Gemini",
    "- ‚úÖ Analisar vantagens e limita√ß√µes dos Foundation Models",
    "- ‚úÖ Aplicar Foundation Models em casos pr√°ticos do mercado",
    "",
    "---",
    "",
    "## üèóÔ∏è 9.1 Introdu√ß√£o aos Foundation Models",
    "",
    "### Conceito Fundamental",
    "",
    "**Foundation Models** s√£o modelos de intelig√™ncia artificial treinados em grandes quantidades de dados n√£o rotulados que podem ser adaptados para uma ampla gama de tarefas downstream atrav√©s de t√©cnicas como fine-tuning, prompt engineering e few-shot learning.",
    "",
    "![Introdu√ß√£o Foundation Models](https://crfm.stanford.edu/static/img/overview.png)",
    "",
    "### Defini√ß√£o e Caracter√≠sticas",
    "",
    "**Defini√ß√£o de Foundation Models:**",
    "- **Treinamento em escala**: Modelos treinados em datasets massivos",
    "- **Capacidade de adapta√ß√£o**: Adapta√ß√£o para m√∫ltiplas tarefas",
    "- **Emerg√™ncia de capacidades**: Habilidades que emergem com escala",
    "- **Base para aplica√ß√µes**: Funda√ß√£o para sistemas especializados",
    "",
    "**Caracter√≠sticas Principais:**",
    "- **Escala**: Milh√µes ou bilh√µes de par√¢metros",
    "- **Dados**: Treinamento em datasets diversificados",
    "- **Multimodalidade**: Capacidade de processar diferentes modalidades",
    "- **Generaliza√ß√£o**: Performance em tarefas n√£o vistas durante treinamento",
    "",
    "### Tipos de Foundation Models",
    "",
    "#### **1. Modelos de Linguagem**",
    "- **GPT**: Generative Pre-trained Transformers",
    "- **BERT**: Bidirectional Encoder Representations",
    "- **T5**: Text-to-Text Transfer Transformer",
    "- **PaLM**: Pathways Language Model",
    "",
    "#### **2. Modelos Multimodais**",
    "- **CLIP**: Contrastive Language-Image Pre-training",
    "- **DALL-E**: Text-to-Image Generation",
    "- **GPT-4V**: Vision-Language Model",
    "- **Gemini**: Multimodal Foundation Model",
    "",
    "#### **3. Modelos de Vis√£o**",
    "- **ViT**: Vision Transformer",
    "- **CLIP**: Image-Text Understanding",
    "- **DINO**: Self-supervised Vision",
    "- **MAE**: Masked Autoencoder",
    "",
    "### Evolu√ß√£o dos Foundation Models",
    "",
    "![Evolu√ß√£o Foundation Models](https://miro.medium.com/v2/resize:fit:1400/1*ZY0W3Y_P4E6r9JvCd5SuFQ.png)",
    "",
    "#### **Cronologia dos Marcos:**",
    "",
    "| Ano | Marco | Contribui√ß√£o |",
    "|-----|-------|--------------|",
    "| **2017** | Transformer | Arquitetura de aten√ß√£o |",
    "| **2018** | BERT | Representa√ß√µes bidirecionais |",
    "| **2019** | GPT-2 | Gera√ß√£o de texto |",
    "| **2020** | GPT-3 | Modelo de 175B par√¢metros |",
    "| **2021** | CLIP | Vis√£o-linguagem |",
    "| **2022** | DALL-E 2 | Gera√ß√£o de imagens |",
    "| **2023** | GPT-4 | Modelo multimodal |",
    "",
    "#### **Marcos Importantes:**",
    "- **2017**: Attention Is All You Need - Vaswani et al.",
    "- **2018**: BERT: Pre-training of Deep Bidirectional Transformers - Devlin et al.",
    "- **2020**: Language Models are Few-Shot Learners - Brown et al.",
    "- **2021**: CLIP: Learning Transferable Visual Representations - Radford et al.",
    "- **2022**: DALL-E 2: Hierarchical Text-Conditional Image Generation - Ramesh et al.",
    "",
    "---",
    "",
    "## üîó 9.2 CLIP: Contrastive Language-Image Pre-training",
    "",
    "### Conceito Fundamental",
    "",
    "**CLIP** √© um modelo que aprende a associar imagens e texto atrav√©s de treinamento contrastivo em pares de imagem-texto da internet, permitindo classifica√ß√£o zero-shot e outras tarefas multimodais.",
    "",
    "![Arquitetura CLIP](https://raw.githubusercontent.com/openai/CLIP/main/CLIP.png)",
    "",
    "### Arquitetura do CLIP",
    "",
    "#### **1. Encoder de Imagem**",
    "- **Base**: ResNet ou Vision Transformer",
    "- **Fun√ß√£o**: Converter imagem em embedding",
    "- **Sa√≠da**: Vetor de caracter√≠sticas",
    "- **Normaliza√ß√£o**: L2 normalization",
    "",
    "#### **2. Encoder de Texto**",
    "- **Base**: Transformer",
    "- **Fun√ß√£o**: Converter texto em embedding",
    "- **Sa√≠da**: Vetor de caracter√≠sticas",
    "- **Normaliza√ß√£o**: L2 normalization",
    "",
    "#### **3. Treinamento Contrastivo**",
    "- **Objetivo**: Maximizar similaridade entre pares corretos",
    "- **Fun√ß√£o**: Minimizar perda contrastiva",
    "- **Dados**: 400M pares imagem-texto",
    "- **Escala**: Modelos de diferentes tamanhos",
    "",
    "### Aplica√ß√µes do CLIP",
    "",
    "#### **1. Classifica√ß√£o Zero-Shot**",
    "- **Entrada**: Imagem + descri√ß√µes de classes",
    "- **Processo**: Comparar embedding da imagem com embeddings das classes",
    "- **Sa√≠da**: Classe mais similar",
    "- **Vantagem**: Sem necessidade de treinamento espec√≠fico",
    "",
    "#### **2. Busca de Imagens**",
    "- **Entrada**: Query de texto",
    "- **Processo**: Encontrar imagens similares ao texto",
    "- **Sa√≠da**: Imagens relevantes",
    "- **Aplica√ß√£o**: Sistemas de recomenda√ß√£o",
    "",
    "#### **3. Gera√ß√£o de Imagens**",
    "- **Entrada**: Descri√ß√£o textual",
    "- **Processo**: Usar CLIP como guia para gera√ß√£o",
    "- **Sa√≠da**: Imagem correspondente ao texto",
    "- **Exemplo**: DALL-E, Stable Diffusion",
    "",
    "### Vantagens e Limita√ß√µes",
    "",
    "#### **Vantagens:**",
    "- ‚úÖ **Zero-shot**: Funciona sem treinamento espec√≠fico",
    "- ‚úÖ **Multimodal**: Entende imagem e texto",
    "- ‚úÖ **Escal√°vel**: Performance melhora com escala",
    "- ‚úÖ **Flex√≠vel**: Adapt√°vel a diferentes tarefas",
    "",
    "#### **Limita√ß√µes:**",
    "- ‚ùå **Vi√©s**: Pode refletir vi√©s dos dados",
    "- ‚ùå **Precis√£o**: Menos preciso que modelos especializados",
    "- ‚ùå **Computa√ß√£o**: Requer recursos computacionais",
    "- ‚ùå **Dados**: Depende de qualidade dos dados de treinamento",
    "",
    "---",
    "",
    "## üé® 9.3 DALL-E: Text-to-Image Generation",
    "",
    "### Conceito Fundamental",
    "",
    "**DALL-E** √© um modelo que gera imagens a partir de descri√ß√µes textuais, combinando t√©cnicas de gera√ß√£o de imagens com compreens√£o de linguagem natural.",
    "",
    "![Arquitetura DALL-E](https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/txt2img/merged-0006.png)",
    "",
    "### Vers√µes do DALL-E",
    "",
    "#### **1. DALL-E (2021)**",
    "- **Arquitetura**: VQ-VAE + Transformer",
    "- **Resolu√ß√£o**: 256√ó256 pixels",
    "- **Treinamento**: 12B par√¢metros",
    "- **Dados**: Pares imagem-texto",
    "",
    "#### **2. DALL-E 2 (2022)**",
    "- **Arquitetura**: CLIP + Diffusion Model",
    "- **Resolu√ß√£o**: 1024√ó1024 pixels",
    "- **Treinamento**: 3.5B par√¢metros",
    "- **Melhorias**: Maior qualidade e controle",
    "",
    "### Processo de Gera√ß√£o",
    "",
    "#### **1. Encodifica√ß√£o do Texto**",
    "- **Entrada**: Descri√ß√£o textual",
    "- **Processo**: Encoder de texto (CLIP)",
    "- **Sa√≠da**: Embedding textual",
    "- **Fun√ß√£o**: Compreender o prompt",
    "",
    "#### **2. Gera√ß√£o da Imagem**",
    "- **Entrada**: Embedding textual",
    "- **Processo**: Diffusion model",
    "- **Sa√≠da**: Imagem gerada",
    "- **Fun√ß√£o**: Criar imagem correspondente",
    "",
    "#### **3. Refinamento**",
    "- **Entrada**: Imagem gerada",
    "- **Processo**: Upscaling e refinamento",
    "- **Sa√≠da**: Imagem final",
    "- **Fun√ß√£o**: Melhorar qualidade",
    "",
    "### Aplica√ß√µes do DALL-E",
    "",
    "#### **1. Arte Digital**",
    "- **Cria√ß√£o**: Obras de arte originais",
    "- **Estilos**: Diferentes estilos art√≠sticos",
    "- **Personaliza√ß√£o**: Adapta√ß√£o a prefer√™ncias",
    "- **Colabora√ß√£o**: Ferramenta para artistas",
    "",
    "#### **2. Design e Prototipagem**",
    "   - **Conceitos**: Ideias visuais r√°pidas",
    "   - **Itera√ß√£o**: M√∫ltiplas vers√µes",
    "   - **Personaliza√ß√£o**: Adapta√ß√£o a necessidades",
    "   - **Efici√™ncia**: Redu√ß√£o de tempo de desenvolvimento",
    "",
    "#### **3. Educa√ß√£o e Treinamento**",
    "   - **Visualiza√ß√£o**: Conceitos abstratos",
    "   - **Exemplos**: Ilustra√ß√µes educativas",
    "   - **Criatividade**: Estimular imagina√ß√£o",
    "   - **Acessibilidade**: Recursos visuais",
    "",
    "---",
    "",
    "## üëÅÔ∏è 9.4 GPT-4V: Vision-Language Model",
    "",
    "### Conceito Fundamental",
    "",
    "**GPT-4V** √© uma vers√£o multimodal do GPT-4 que pode processar e entender tanto texto quanto imagens, permitindo an√°lises complexas e conversa√ß√µes sobre conte√∫do visual.",
    "",
    "![Arquitetura GPT-4V](https://cdn.openai.com/research-covers/gpt-4/gpt-4.png)",
    "",
    "### Capacidades do GPT-4V",
    "",
    "#### **1. An√°lise de Imagens**",
    "- **Descri√ß√£o**: Descrever conte√∫do de imagens",
    "- **Detec√ß√£o**: Identificar objetos e cenas",
    "- **An√°lise**: Interpretar contexto e significado",
    "- **Compara√ß√£o**: Comparar m√∫ltiplas imagens",
    "",
    "#### **2. Resposta a Perguntas**",
    "- **Entrada**: Imagem + pergunta",
    "- **Processo**: An√°lise multimodal",
    "- **Sa√≠da**: Resposta baseada na imagem",
    "- **Aplica√ß√£o**: QA visual",
    "",
    "#### **3. Gera√ß√£o de Conte√∫do**",
    "- **Entrada**: Imagem + prompt",
    "- **Processo**: An√°lise + gera√ß√£o",
    "- **Sa√≠da**: Texto relacionado √† imagem",
    "- **Aplica√ß√£o**: Storytelling, descri√ß√µes",
    "",
    "### Aplica√ß√µes Pr√°ticas",
    "",
    "#### **1. Acessibilidade**",
    "- **Descri√ß√£o**: Descrever imagens para deficientes visuais",
    "- **Navega√ß√£o**: Ajudar na navega√ß√£o",
    "   - **Educa√ß√£o**: Recursos educativos",
    "   - **Inclus√£o**: Promover inclus√£o",
    "",
    "#### **2. An√°lise M√©dica**",
    "- **Diagn√≥stico**: Ajudar em diagn√≥sticos",
    "- **Educa√ß√£o**: Treinamento m√©dico",
    "   - **Pesquisa**: An√°lise de dados m√©dicos",
    "   - **Suporte**: Apoio a profissionais",
    "",
    "#### **3. An√°lise de Dados**",
    "- **Gr√°ficos**: Interpretar gr√°ficos e tabelas",
    "- **Relat√≥rios**: Gerar insights",
    "   - **Visualiza√ß√£o**: Melhorar visualiza√ß√µes",
    "   - **Decis√£o**: Apoiar tomada de decis√£o",
    "",
    "---",
    "",
    "## üîç 9.5 Demonstra√ß√£o Pr√°tica: Foundation Models Simples",
    "",
    "Vamos implementar e demonstrar conceitos de Foundation Models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class FoundationModelsDemo:\n",
    "    \"\"\"Demonstra√ß√£o de conceitos de Foundation Models\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def create_synthetic_dataset(self, num_samples=1000):\n",
    "        \"\"\"Cria dataset sint√©tico para demonstra√ß√£o\"\"\"\n",
    "        \n",
    "        # Simular dados de texto e imagem\n",
    "        text_data = []\n",
    "        image_data = []\n",
    "        labels = []\n",
    "        \n",
    "        categories = [\n",
    "            {\"text\": \"a red car\", \"image\": [1, 0, 0], \"label\": 0},\n",
    "            {\"text\": \"a blue car\", \"image\": [0, 1, 0], \"label\": 1},\n",
    "            {\"text\": \"a green car\", \"image\": [0, 0, 1], \"label\": 2},\n",
    "            {\"text\": \"a red truck\", \"image\": [1, 0, 0], \"label\": 3},\n",
    "            {\"text\": \"a blue truck\", \"image\": [0, 1, 0], \"label\": 4},\n",
    "            {\"text\": \"a green truck\", \"image\": [0, 0, 1], \"label\": 5}\n",
    "        ]\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            # Selecionar categoria aleat√≥ria\n",
    "            category = categories[i % len(categories)]\n",
    "            \n",
    "            # Adicionar varia√ß√£o ao texto\n",
    "            text_variations = [\n",
    "                category[\"text\"],\n",
    "                f\"{category['text']} in the street\",\n",
    "                f\"{category['text']} parked outside\",\n",
    "                f\"{category['text']} driving fast\",\n",
    "                f\"{category['text']} at the traffic light\"\n",
    "            ]\n",
    "            \n",
    "            text = text_variations[i % len(text_variations)]\n",
    "            \n",
    "            # Adicionar ru√≠do √† imagem\n",
    "            image = np.array(category[\"image\"]) + np.random.normal(0, 0.1, 3)\n",
    "            image = np.clip(image, 0, 1)\n",
    "            \n",
    "            text_data.append(text)\n",
    "            image_data.append(image)\n",
    "            labels.append(category[\"label\"])\n",
    "        \n",
    "        return text_data, image_data, labels\n",
    "    \n",
    "    def simple_text_encoder(self, text_data, vocab_size=1000, embedding_dim=128):\n",
    "        \"\"\"Simula um encoder de texto simples\"\"\"\n",
    "        \n",
    "        # Criar vocabul√°rio simples\n",
    "        vocab = {}\n",
    "        for text in text_data:\n",
    "            words = text.lower().split()\n",
    "            for word in words:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = len(vocab)\n",
    "        \n",
    "        # Embedding layer\n",
    "        embedding = nn.Embedding(len(vocab), embedding_dim)\n",
    "        \n",
    "        # Encoder simples\n",
    "        encoder = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Processar textos\n",
    "        text_embeddings = []\n",
    "        for text in text_data:\n",
    "            words = text.lower().split()\n",
    "            word_ids = [vocab.get(word, 0) for word in words]\n",
    "            \n",
    "            # Padding para tamanho fixo\n",
    "            max_len = 10\n",
    "            if len(word_ids) > max_len:\n",
    "                word_ids = word_ids[:max_len]\n",
    "            else:\n",
    "                word_ids.extend([0] * (max_len - len(word_ids)))\n",
    "            \n",
    "            # Converter para tensor\n",
    "            word_tensor = torch.LongTensor(word_ids)\n",
    "            \n",
    "            # Embedding\n",
    "            word_embeddings = embedding(word_tensor)\n",
    "            \n",
    "            # M√©dia dos embeddings\n",
    "            text_embedding = torch.mean(word_embeddings, dim=0)\n",
    "            \n",
    "            # Encoder\n",
    "            text_embedding = encoder(text_embedding)\n",
    "            \n",
    "            text_embeddings.append(text_embedding.detach().numpy())\n",
    "        \n",
    "        return np.array(text_embeddings), vocab\n",
    "    \n",
    "    def simple_image_encoder(self, image_data, embedding_dim=128):\n",
    "        \"\"\"Simula um encoder de imagem simples\"\"\"\n",
    "        \n",
    "        # Encoder simples\n",
    "        encoder = nn.Sequential(\n",
    "            nn.Linear(3, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embedding_dim, embedding_dim)\n",
    "        )\n",
    "        \n",
    "        # Processar imagens\n",
    "        image_embeddings = []\n",
    "        for image in image_data:\n",
    "            # Converter para tensor\n",
    "            image_tensor = torch.FloatTensor(image)\n",
    "            \n",
    "            # Encoder\n",
    "            image_embedding = encoder(image_tensor)\n",
    "            \n",
    "            image_embeddings.append(image_embedding.detach().numpy())\n",
    "        \n",
    "        return np.array(image_embeddings)\n",
    "    \n",
    "    def contrastive_loss(self, image_embeddings, text_embeddings, temperature=0.07):\n",
    "        \"\"\"Calcula perda contrastiva\"\"\"\n",
    "        \n",
    "        # Normalizar embeddings\n",
    "        image_embeddings = F.normalize(torch.FloatTensor(image_embeddings), dim=1)\n",
    "        text_embeddings = F.normalize(torch.FloatTensor(text_embeddings), dim=1)\n",
    "        \n",
    "        # Calcular similaridades\n",
    "        similarities = torch.matmul(image_embeddings, text_embeddings.T) / temperature\n",
    "        \n",
    "        # Labels (diagonal)\n",
    "        labels = torch.arange(similarities.size(0))\n",
    "        \n",
    "        # Perda de imagem para texto\n",
    "        loss_i2t = F.cross_entropy(similarities, labels)\n",
    "        \n",
    "        # Perda de texto para imagem\n",
    "        loss_t2i = F.cross_entropy(similarities.T, labels)\n",
    "        \n",
    "        # Perda total\n",
    "        total_loss = (loss_i2t + loss_t2i) / 2\n",
    "        \n",
    "        return total_loss.item()\n",
    "    \n",
    "    def zero_shot_classification(self, image_embeddings, text_embeddings, labels):\n",
    "        \"\"\"Simula classifica√ß√£o zero-shot\"\"\"\n",
    "        \n",
    "        # Normalizar embeddings\n",
    "        image_embeddings = F.normalize(torch.FloatTensor(image_embeddings), dim=1)\n",
    "        text_embeddings = F.normalize(torch.FloatTensor(text_embeddings), dim=1)\n",
    "        \n",
    "        # Calcular similaridades\n",
    "        similarities = torch.matmul(image_embeddings, text_embeddings.T)\n",
    "        \n",
    "        # Predi√ß√µes\n",
    "        predictions = torch.argmax(similarities, dim=1)\n",
    "        \n",
    "        # Calcular acur√°cia\n",
    "        accuracy = torch.mean((predictions == torch.LongTensor(labels)).float()).item()\n",
    "        \n",
    "        return predictions.numpy(), accuracy\n",
    "    \n",
    "    def visualize_embeddings(self, image_embeddings, text_embeddings, labels):\n",
    "        \"\"\"Visualiza embeddings em 2D\"\"\"\n",
    "        \n",
    "        from sklearn.manifold import TSNE\n",
    "        \n",
    "        # Combinar embeddings\n",
    "        all_embeddings = np.vstack([image_embeddings, text_embeddings])\n",
    "        \n",
    "        # t-SNE\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        embeddings_2d = tsne.fit_transform(all_embeddings)\n",
    "        \n",
    "        # Separar\n",
    "        image_2d = embeddings_2d[:len(image_embeddings)]\n",
    "        text_2d = embeddings_2d[len(image_embeddings):]\n",
    "        \n",
    "        # Visualizar\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Embeddings de imagem\n",
    "        scatter = axes[0].scatter(image_2d[:, 0], image_2d[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "        axes[0].set_title('Embeddings de Imagem')\n",
    "        axes[0].set_xlabel('t-SNE 1')\n",
    "        axes[0].set_ylabel('t-SNE 2')\n",
    "        plt.colorbar(scatter, ax=axes[0])\n",
    "        \n",
    "        # Embeddings de texto\n",
    "        scatter = axes[1].scatter(text_2d[:, 0], text_2d[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
    "        axes[1].set_title('Embeddings de Texto')\n",
    "        axes[1].set_xlabel('t-SNE 1')\n",
    "        axes[1].set_ylabel('t-SNE 2')\n",
    "        plt.colorbar(scatter, ax=axes[1])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def demonstrate_foundation_models(self):\n",
    "        \"\"\"Demonstra conceitos de Foundation Models\"\"\"\n",
    "        \n",
    "        print(\"=== DEMONSTRA√á√ÉO: FOUNDATION MODELS ===\")\n",
    "        \n",
    "        # Criar dataset\n",
    "        text_data, image_data, labels = self.create_synthetic_dataset(500)\n",
    "        print(f\"Dataset criado: {len(text_data)} amostras\")\n",
    "        print(f\"Categorias: {len(set(labels))}\")\n",
    "        \n",
    "        # Encoder de texto\n",
    "        print(\"\\n=== ENCODING DE TEXTO ===\")\n",
    "        text_embeddings, vocab = self.simple_text_encoder(text_data)\n",
    "        print(f\"Vocabul√°rio: {len(vocab)} palavras\")\n",
    "        print(f\"Embeddings de texto: {text_embeddings.shape}\")\n",
    "        \n",
    "        # Encoder de imagem\n",
    "        print(\"\\n=== ENCODING DE IMAGEM ===\")\n",
    "        image_embeddings = self.simple_image_encoder(image_data)\n",
    "        print(f\"Embeddings de imagem: {image_embeddings.shape}\")\n",
    "        \n",
    "        # Perda contrastiva\n",
    "        print(\"\\n=== PERDA CONTRASTIVA ===\")\n",
    "        contrastive_loss = self.contrastive_loss(image_embeddings, text_embeddings)\n",
    "        print(f\"Perda contrastiva: {contrastive_loss:.4f}\")\n",
    "        \n",
    "        # Classifica√ß√£o zero-shot\n",
    "        print(\"\\n=== CLASSIFICA√á√ÉO ZERO-SHOT ===\")\n",
    "        predictions, accuracy = self.zero_shot_classification(image_embeddings, text_embeddings, labels)\n",
    "        print(f\"Acur√°cia zero-shot: {accuracy:.4f}\")\n",
    "        \n",
    "        # Visualizar embeddings\n",
    "        print(\"\\n=== VISUALIZA√á√ÉO DE EMBEDDINGS ===\")\n",
    "        self.visualize_embeddings(image_embeddings, text_embeddings, labels)\n",
    "        \n",
    "        # An√°lise quantitativa\n",
    "        print(\"\\n=== AN√ÅLISE QUANTITATIVA ===\")\n",
    "        \n",
    "        # Calcular similaridades\n",
    "        image_embeddings_norm = F.normalize(torch.FloatTensor(image_embeddings), dim=1)\n",
    "        text_embeddings_norm = F.normalize(torch.FloatTensor(text_embeddings), dim=1)\n",
    "        similarities = torch.matmul(image_embeddings_norm, text_embeddings_norm.T)\n",
    "        \n",
    "        print(f\"\\nEstat√≠sticas de Similaridade:\")\n",
    "        print(f\"  - Similaridade m√°xima: {torch.max(similarities):.4f}\")\n",
    "        print(f\"  - Similaridade m√≠nima: {torch.min(similarities):.4f}\")\n",
    "        print(f\"  - Similaridade m√©dia: {torch.mean(similarities):.4f}\")\n",
    "        print(f\"  - Desvio padr√£o: {torch.std(similarities):.4f}\")\n",
    "        \n",
    "        # An√°lise por categoria\n",
    "        print(f\"\\nAn√°lise por Categoria:\")\n",
    "        for i in range(len(set(labels))):\n",
    "            mask = np.array(labels) == i\n",
    "            category_similarities = similarities[mask, i]\n",
    "            print(f\"  - Categoria {i}: Similaridade m√©dia = {torch.mean(category_similarities):.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'text_embeddings': text_embeddings,\n",
    "            'image_embeddings': image_embeddings,\n",
    "            'predictions': predictions,\n",
    "            'accuracy': accuracy,\n",
    "            'contrastive_loss': contrastive_loss\n",
    "        }\n",
    "\n",
    "# Executar demonstra√ß√£o\n",
    "print(\"=== DEMONSTRA√á√ÉO: FOUNDATION MODELS ===\")\n",
    "foundation_demo = FoundationModelsDemo()\n",
    "results = foundation_demo.demonstrate_foundation_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An√°lise dos Resultados",
    "",
    "**Observa√ß√µes Importantes:**",
    "",
    "1. **Embeddings Multimodais**:",
    "   - **Alinhamento**: Imagens e textos similares ficam pr√≥ximos",
    "   - **Separa√ß√£o**: Diferentes categorias ficam separadas",
    "   - **Consist√™ncia**: Padr√µes consistentes entre modalidades",
    "",
    "2. **Classifica√ß√£o Zero-Shot**:",
    "   - **Acur√°cia**: Performance sem treinamento espec√≠fico",
    "   - **Generaliza√ß√£o**: Funciona em categorias n√£o vistas",
    "   - **Robustez**: Resistente a varia√ß√µes",
    "",
    "3. **Perda Contrastiva**:",
    "   - **Otimiza√ß√£o**: Minimiza dist√¢ncia entre pares corretos",
    "   - **Maximiza√ß√£o**: Maximiza dist√¢ncia entre pares incorretos",
    "   - **Converg√™ncia**: Deve diminuir com treinamento",
    "",
    "---",
    "",
    "## üåê 9.6 APIs e Aplica√ß√µes Pr√°ticas",
    "",
    "### APIs Dispon√≠veis",
    "",
    "![APIs Foundation Models](https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/model_diagram.png)",
    "",
    "#### **1. OpenAI API**",
    "",
    "**Modelos Dispon√≠veis:**",
    "- **GPT-4**: Modelo de linguagem",
    "- **GPT-4V**: Modelo multimodal",
    "- **DALL-E 3**: Gera√ß√£o de imagens",
    "- **CLIP**: An√°lise imagem-texto",
    "",
    "**Caracter√≠sticas:**",
    "- **Alta qualidade**: Modelos state-of-the-art",
    "- **Facilidade de uso**: API simples",
    "- **Documenta√ß√£o**: Bem documentada",
    "- **Suporte**: Suporte t√©cnico",
    "",
    "#### **2. Google Gemini API**",
    "",
    "**Modelos Dispon√≠veis:**",
    "- **Gemini Pro**: Modelo de linguagem",
    "- **Gemini Pro Vision**: Modelo multimodal",
    "- **Imagen**: Gera√ß√£o de imagens",
    "- **PaLM**: Modelo de linguagem",
    "",
    "**Caracter√≠sticas:**",
    "- **Multimodal**: Suporte nativo a m√∫ltiplas modalidades",
    "- **Integra√ß√£o**: Integra√ß√£o com Google Cloud",
    "- **Escalabilidade**: Escal√°vel para grandes volumes",
    "- **Custo**: Pre√ßos competitivos",
    "",
    "### Casos de Uso Pr√°ticos",
    "",
    "#### **1. An√°lise de Conte√∫do**",
    "",
    "**Aplica√ß√µes:**",
    "- **Modera√ß√£o**: Modera√ß√£o de conte√∫do",
    "   - **Classifica√ß√£o**: Categoriza√ß√£o autom√°tica",
    "   - **Detec√ß√£o**: Detec√ß√£o de conte√∫do inadequado",
    "   - **An√°lise**: An√°lise de sentimento",
    "",
    "**Exemplo Pr√°tico:**",
    "- **Empresa**: Rede social",
    "   - **Problema**: Modera√ß√£o manual de conte√∫do",
    "   - **Solu√ß√£o**: GPT-4V para an√°lise autom√°tica",
    "   - **Resultado**: 90% de automa√ß√£o",
    "   - **ROI**: 80% de redu√ß√£o no tempo de modera√ß√£o",
    "",
    "#### **2. Gera√ß√£o de Conte√∫do**",
    "",
    "**Aplica√ß√µes:**",
    "- **Marketing**: Cria√ß√£o de conte√∫do",
    "   - **Design**: Gera√ß√£o de designs",
    "   - **Copywriting**: Cria√ß√£o de textos",
    "   - **Personaliza√ß√£o**: Conte√∫do personalizado",
    "",
    "**Exemplo Pr√°tico:**",
    "- **Empresa**: Ag√™ncia de marketing",
    "   - **Problema**: Cria√ß√£o manual de conte√∫do",
    "   - **Solu√ß√£o**: DALL-E + GPT-4 para gera√ß√£o",
    "   - **Resultado**: 70% de automa√ß√£o",
    "   - **ROI**: 60% de redu√ß√£o no tempo de cria√ß√£o",
    "",
    "#### **3. An√°lise de Dados**",
    "",
    "**Aplica√ß√µes:**",
    "- **Insights**: Gera√ß√£o de insights",
    "   - **Visualiza√ß√£o**: Interpreta√ß√£o de gr√°ficos",
    "   - **Relat√≥rios**: Cria√ß√£o de relat√≥rios",
    "   - **Decis√£o**: Apoio √† tomada de decis√£o",
    "",
    "**Exemplo Pr√°tico:**",
    "- **Empresa**: Empresa de consultoria",
    "   - **Problema**: An√°lise manual de dados",
    "   - **Solu√ß√£o**: GPT-4V para an√°lise de gr√°ficos",
    "   - **Resultado**: 85% de automa√ß√£o",
    "   - **ROI**: 75% de redu√ß√£o no tempo de an√°lise",
    "",
    "### Limita√ß√µes e Desafios",
    "",
    "#### **1. Custos**",
    "- **API**: Custos por uso",
    "   - **Escala**: Custos aumentam com escala",
    "   - **Modelo**: Modelos maiores s√£o mais caros",
    "   - **Otimiza√ß√£o**: Necessidade de otimiza√ß√£o",
    "",
    "#### **2. Qualidade**",
    "- **Consist√™ncia**: Qualidade pode variar",
    "   - **Vi√©s**: Pode refletir vi√©s dos dados",
    "   - **Hallucina√ß√£o**: Pode gerar conte√∫do incorreto",
    "   - **Valida√ß√£o**: Necessidade de valida√ß√£o",
    "",
    "#### **3. Controle**",
    "- **Personaliza√ß√£o**: Limitada personaliza√ß√£o",
    "   - **Fine-tuning**: Nem sempre dispon√≠vel",
    "   - **Par√¢metros**: Poucos par√¢metros ajust√°veis",
    "   - **Transpar√™ncia**: Processo pouco transparente",
    "",
    "---",
    "",
    "## üìù Resumo do M√≥dulo 9",
    "",
    "### Principais Conceitos Abordados",
    "",
    "1. **Fundamentos**: Foundation Models e caracter√≠sticas",
    "2. **CLIP**: Contrastive Language-Image Pre-training",
    "3. **DALL-E**: Text-to-Image Generation",
    "4. **GPT-4V**: Vision-Language Model",
    "5. **APIs**: Aplica√ß√µes pr√°ticas e limita√ß√µes",
    "",
    "### Demonstra√ß√µes Pr√°ticas",
    "",
    "**1. Foundation Models Simples:**",
    "   - Implementa√ß√£o de encoders de texto e imagem",
    "   - Treinamento contrastivo",
    "   - Classifica√ß√£o zero-shot",
    "   - Visualiza√ß√£o de embeddings",
    "",
    "### Pr√≥ximos Passos",
    "",
    "No **M√≥dulo 10**, aplicaremos todos os conceitos aprendidos em uma **Atividade Final Pr√°tica** completa.",
    "",
    "### Refer√™ncias Principais",
    "",
    "- [On the Opportunities and Risks of Foundation Models - Bommasani et al.](https://arxiv.org/abs/2108.07258)",
    "- [CLIP: Learning Transferable Visual Representations - Radford et al.](https://arxiv.org/abs/2103.00020)",
    "",
    "---",
    "",
    "**Pr√≥ximo M√≥dulo**: Atividade Final Pr√°tica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Conex√£o com o Pr√≥ximo M√≥dulo\n",
    "\n",
    "Agora que dominamos **Foundation Models** para vis√£o computacional, estamos preparados para aplicar todos os conceitos aprendidos em uma **Atividade Final Pr√°tica** completa.\n",
    "\n",
    "No **M√≥dulo 10**, veremos como:\n",
    "\n",
    "### üîó **Conex√µes Diretas:**\n",
    "\n",
    "1. **Foundation Models** ‚Üí **Sistema Completo**\n",
    "   - CLIP, DALL-E, GPT-4V\n",
    "   - Integra√ß√£o em sistema multimodal\n",
    "\n",
    "2. **APIs** ‚Üí **Implementa√ß√£o Pr√°tica**\n",
    "   - OpenAI e Gemini APIs\n",
    "   - Sistema funcional com APIs reais\n",
    "\n",
    "3. **Conceitos Te√≥ricos** ‚Üí **Aplica√ß√£o Pr√°tica**\n",
    "   - Todos os m√≥dulos anteriores\n",
    "   - Implementa√ß√£o de sistema completo\n",
    "\n",
    "4. **Demonstra√ß√µes** ‚Üí **Projeto Real**\n",
    "   - Demonstra√ß√µes isoladas\n",
    "   - Projeto integrado e funcional\n",
    "\n",
    "### üöÄ **Evolu√ß√£o Natural:**\n",
    "\n",
    "- **Fundamentos** ‚Üí **Aplica√ß√£o**\n",
    "- **Conceitos** ‚Üí **Implementa√ß√£o**\n",
    "- **Demonstra√ß√µes** ‚Üí **Projeto**\n",
    "- **Teoria** ‚Üí **Pr√°tica**\n",
    "\n",
    "Esta transi√ß√£o marca a **culmina√ß√£o** de todo o curso em um projeto pr√°tico completo!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üñºÔ∏è Imagens de Refer√™ncia - M√≥dulo 9",
    "",
    "![Aplica√ß√µes Pr√°ticas](https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/model_diagram.png)",
    "",
    "![Modelo CLIP](https://github.com/openai/CLIP/raw/main/CLIP.png)",
    "",
    "![Modelo DALL-E](https://miro.medium.com/v2/resize:fit:1400/1*3wU41rQD6XXwvEjwvpY3qQ.png)",
    "",
    "![Modelo GPT-4V](https://cdn.openai.com/research-covers/gpt-4/gpt-4.png)",
    "",
    ""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}