{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# M√≥dulo 9: Foundation Models para Vis√£o Computacional\n",
        "\n",
        "## üéØ Objetivos de Aprendizagem\n",
        "\n",
        "Ao final deste m√≥dulo, voc√™ ser√° capaz de:\n",
        "\n",
        "- ‚úÖ Compreender o conceito de Foundation Models\n",
        "- ‚úÖ Conhecer modelos como CLIP, DALL-E e GPT-4V\n",
        "- ‚úÖ Implementar solu√ß√µes com APIs do OpenAI e Gemini\n",
        "- ‚úÖ Analisar vantagens e limita√ß√µes dos Foundation Models\n",
        "- ‚úÖ Aplicar Foundation Models em casos pr√°ticos do mercado\n",
        "\n",
        "---\n",
        "\n",
        "## üèóÔ∏è 9.1 Introdu√ß√£o aos Foundation Models\n",
        "\n",
        "### Conceito Fundamental\n",
        "\n",
        "**Foundation Models** s√£o modelos de intelig√™ncia artificial treinados em grandes quantidades de dados n√£o rotulados que podem ser adaptados para uma ampla gama de tarefas downstream atrav√©s de t√©cnicas como fine-tuning, prompt engineering e few-shot learning.\n",
        "\n",
        "![Introdu√ß√£o Foundation Models](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo9/introducao_foundation_models.png?raw=true)\n",
        "\n",
        "### Defini√ß√£o e Caracter√≠sticas\n",
        "\n",
        "**Defini√ß√£o de Foundation Models:**\n",
        "- **Treinamento em escala**: Modelos treinados em datasets massivos\n",
        "- **Capacidade de adapta√ß√£o**: Adapta√ß√£o para m√∫ltiplas tarefas\n",
        "- **Emerg√™ncia de capacidades**: Habilidades que emergem com escala\n",
        "- **Base para aplica√ß√µes**: Funda√ß√£o para sistemas especializados\n",
        "\n",
        "**Caracter√≠sticas Principais:**\n",
        "- **Escala**: Milh√µes ou bilh√µes de par√¢metros\n",
        "- **Dados**: Treinamento em datasets diversificados\n",
        "- **Multimodalidade**: Capacidade de processar diferentes modalidades\n",
        "- **Generaliza√ß√£o**: Performance em tarefas n√£o vistas durante treinamento\n",
        "\n",
        "### Tipos de Foundation Models\n",
        "\n",
        "#### **1. Modelos de Linguagem**\n",
        "- **GPT**: Generative Pre-trained Transformers\n",
        "- **BERT**: Bidirectional Encoder Representations\n",
        "- **T5**: Text-to-Text Transfer Transformer\n",
        "- **PaLM**: Pathways Language Model\n",
        "\n",
        "#### **2. Modelos Multimodais**\n",
        "- **CLIP**: Contrastive Language-Image Pre-training\n",
        "- **DALL-E**: Text-to-Image Generation\n",
        "- **GPT-4V**: Vision-Language Model\n",
        "- **Gemini**: Multimodal Foundation Model\n",
        "\n",
        "#### **3. Modelos de Vis√£o**\n",
        "- **ViT**: Vision Transformer\n",
        "- **CLIP**: Image-Text Understanding\n",
        "- **DINO**: Self-supervised Vision\n",
        "- **MAE**: Masked Autoencoder\n",
        "\n",
        "### Evolu√ß√£o dos Foundation Models\n",
        "\n",
        "![Evolu√ß√£o Foundation Models](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo9/evolucao_foundation_models.png?raw=true)\n",
        "\n",
        "#### **Cronologia dos Marcos:**\n",
        "\n",
        "| Ano | Marco | Contribui√ß√£o |\n",
        "|-----|-------|--------------|\n",
        "| **2017** | Transformer | Arquitetura de aten√ß√£o |\n",
        "| **2018** | BERT | Representa√ß√µes bidirecionais |\n",
        "| **2019** | GPT-2 | Gera√ß√£o de texto |\n",
        "| **2020** | GPT-3 | Modelo de 175B par√¢metros |\n",
        "| **2021** | CLIP | Vis√£o-linguagem |\n",
        "| **2022** | DALL-E 2 | Gera√ß√£o de imagens |\n",
        "| **2023** | GPT-4 | Modelo multimodal |\n",
        "\n",
        "#### **Marcos Importantes:**\n",
        "- **2017**: Attention Is All You Need - Vaswani et al.\n",
        "- **2018**: BERT: Pre-training of Deep Bidirectional Transformers - Devlin et al.\n",
        "- **2020**: Language Models are Few-Shot Learners - Brown et al.\n",
        "- **2021**: CLIP: Learning Transferable Visual Representations - Radford et al.\n",
        "- **2022**: DALL-E 2: Hierarchical Text-Conditional Image Generation - Ramesh et al.\n",
        "\n",
        "---\n",
        "\n",
        "## üîó 9.2 CLIP: Contrastive Language-Image Pre-training\n",
        "\n",
        "### Conceito Fundamental\n",
        "\n",
        "**CLIP** √© um modelo que aprende a associar imagens e texto atrav√©s de treinamento contrastivo em pares de imagem-texto da internet, permitindo classifica√ß√£o zero-shot e outras tarefas multimodais.\n",
        "\n",
        "![Arquitetura CLIP](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo9/arquitetura_clip.png?raw=true)\n",
        "\n",
        "### Arquitetura do CLIP\n",
        "\n",
        "#### **1. Encoder de Imagem**\n",
        "- **Base**: ResNet ou Vision Transformer\n",
        "- **Fun√ß√£o**: Converter imagem em embedding\n",
        "- **Sa√≠da**: Vetor de caracter√≠sticas\n",
        "- **Normaliza√ß√£o**: L2 normalization\n",
        "\n",
        "#### **2. Encoder de Texto**\n",
        "- **Base**: Transformer\n",
        "- **Fun√ß√£o**: Converter texto em embedding\n",
        "- **Sa√≠da**: Vetor de caracter√≠sticas\n",
        "- **Normaliza√ß√£o**: L2 normalization\n",
        "\n",
        "#### **3. Treinamento Contrastivo**\n",
        "- **Objetivo**: Maximizar similaridade entre pares corretos\n",
        "- **Fun√ß√£o**: Minimizar perda contrastiva\n",
        "- **Dados**: 400M pares imagem-texto\n",
        "- **Escala**: Modelos de diferentes tamanhos\n",
        "\n",
        "### Aplica√ß√µes do CLIP\n",
        "\n",
        "#### **1. Classifica√ß√£o Zero-Shot**\n",
        "- **Entrada**: Imagem + descri√ß√µes de classes\n",
        "- **Processo**: Comparar embedding da imagem com embeddings das classes\n",
        "- **Sa√≠da**: Classe mais similar\n",
        "- **Vantagem**: Sem necessidade de treinamento espec√≠fico\n",
        "\n",
        "#### **2. Busca de Imagens**\n",
        "- **Entrada**: Query de texto\n",
        "- **Processo**: Encontrar imagens similares ao texto\n",
        "- **Sa√≠da**: Imagens relevantes\n",
        "- **Aplica√ß√£o**: Sistemas de recomenda√ß√£o\n",
        "\n",
        "#### **3. Gera√ß√£o de Imagens**\n",
        "- **Entrada**: Descri√ß√£o textual\n",
        "- **Processo**: Usar CLIP como guia para gera√ß√£o\n",
        "- **Sa√≠da**: Imagem correspondente ao texto\n",
        "- **Exemplo**: DALL-E, Stable Diffusion\n",
        "\n",
        "### Vantagens e Limita√ß√µes\n",
        "\n",
        "#### **Vantagens:**\n",
        "- ‚úÖ **Zero-shot**: Funciona sem treinamento espec√≠fico\n",
        "- ‚úÖ **Multimodal**: Entende imagem e texto\n",
        "- ‚úÖ **Escal√°vel**: Performance melhora com escala\n",
        "- ‚úÖ **Flex√≠vel**: Adapt√°vel a diferentes tarefas\n",
        "\n",
        "#### **Limita√ß√µes:**\n",
        "- ‚ùå **Vi√©s**: Pode refletir vi√©s dos dados\n",
        "- ‚ùå **Precis√£o**: Menos preciso que modelos especializados\n",
        "- ‚ùå **Computa√ß√£o**: Requer recursos computacionais\n",
        "- ‚ùå **Dados**: Depende de qualidade dos dados de treinamento\n",
        "\n",
        "---\n",
        "\n",
        "## üé® 9.3 DALL-E: Text-to-Image Generation\n",
        "\n",
        "### Conceito Fundamental\n",
        "\n",
        "**DALL-E** √© um modelo que gera imagens a partir de descri√ß√µes textuais, combinando t√©cnicas de gera√ß√£o de imagens com compreens√£o de linguagem natural.\n",
        "\n",
        "![Arquitetura DALL-E](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo9/arquitetura_dalle.png?raw=true)\n",
        "\n",
        "### Vers√µes do DALL-E\n",
        "\n",
        "#### **1. DALL-E (2021)**\n",
        "- **Arquitetura**: VQ-VAE + Transformer\n",
        "- **Resolu√ß√£o**: 256√ó256 pixels\n",
        "- **Treinamento**: 12B par√¢metros\n",
        "- **Dados**: Pares imagem-texto\n",
        "\n",
        "#### **2. DALL-E 2 (2022)**\n",
        "- **Arquitetura**: CLIP + Diffusion Model\n",
        "- **Resolu√ß√£o**: 1024√ó1024 pixels\n",
        "- **Treinamento**: 3.5B par√¢metros\n",
        "- **Melhorias**: Maior qualidade e controle\n",
        "\n",
        "### Processo de Gera√ß√£o\n",
        "\n",
        "#### **1. Encodifica√ß√£o do Texto**\n",
        "- **Entrada**: Descri√ß√£o textual\n",
        "- **Processo**: Encoder de texto (CLIP)\n",
        "- **Sa√≠da**: Embedding textual\n",
        "- **Fun√ß√£o**: Compreender o prompt\n",
        "\n",
        "#### **2. Gera√ß√£o da Imagem**\n",
        "- **Entrada**: Embedding textual\n",
        "- **Processo**: Diffusion model\n",
        "- **Sa√≠da**: Imagem gerada\n",
        "- **Fun√ß√£o**: Criar imagem correspondente\n",
        "\n",
        "#### **3. Refinamento**\n",
        "- **Entrada**: Imagem gerada\n",
        "- **Processo**: Upscaling e refinamento\n",
        "- **Sa√≠da**: Imagem final\n",
        "- **Fun√ß√£o**: Melhorar qualidade\n",
        "\n",
        "### Aplica√ß√µes do DALL-E\n",
        "\n",
        "#### **1. Arte Digital**\n",
        "- **Cria√ß√£o**: Obras de arte originais\n",
        "- **Estilos**: Diferentes estilos art√≠sticos\n",
        "- **Personaliza√ß√£o**: Adapta√ß√£o a prefer√™ncias\n",
        "- **Colabora√ß√£o**: Ferramenta para artistas\n",
        "\n",
        "#### **2. Design e Prototipagem**\n",
        "   - **Conceitos**: Ideias visuais r√°pidas\n",
        "   - **Itera√ß√£o**: M√∫ltiplas vers√µes\n",
        "   - **Personaliza√ß√£o**: Adapta√ß√£o a necessidades\n",
        "   - **Efici√™ncia**: Redu√ß√£o de tempo de desenvolvimento\n",
        "\n",
        "#### **3. Educa√ß√£o e Treinamento**\n",
        "   - **Visualiza√ß√£o**: Conceitos abstratos\n",
        "   - **Exemplos**: Ilustra√ß√µes educativas\n",
        "   - **Criatividade**: Estimular imagina√ß√£o\n",
        "   - **Acessibilidade**: Recursos visuais\n",
        "\n",
        "---\n",
        "\n",
        "## üëÅÔ∏è 9.4 GPT-4V: Vision-Language Model\n",
        "\n",
        "### Conceito Fundamental\n",
        "\n",
        "**GPT-4V** √© uma vers√£o multimodal do GPT-4 que pode processar e entender tanto texto quanto imagens, permitindo an√°lises complexas e conversa√ß√µes sobre conte√∫do visual.\n",
        "\n",
        "![Arquitetura GPT-4V](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo9/arquitetura_gpt4v.png?raw=true)\n",
        "\n",
        "### Capacidades do GPT-4V\n",
        "\n",
        "#### **1. An√°lise de Imagens**\n",
        "- **Descri√ß√£o**: Descrever conte√∫do de imagens\n",
        "- **Detec√ß√£o**: Identificar objetos e cenas\n",
        "- **An√°lise**: Interpretar contexto e significado\n",
        "- **Compara√ß√£o**: Comparar m√∫ltiplas imagens\n",
        "\n",
        "#### **2. Resposta a Perguntas**\n",
        "- **Entrada**: Imagem + pergunta\n",
        "- **Processo**: An√°lise multimodal\n",
        "- **Sa√≠da**: Resposta baseada na imagem\n",
        "- **Aplica√ß√£o**: QA visual\n",
        "\n",
        "#### **3. Gera√ß√£o de Conte√∫do**\n",
        "- **Entrada**: Imagem + prompt\n",
        "- **Processo**: An√°lise + gera√ß√£o\n",
        "- **Sa√≠da**: Texto relacionado √† imagem\n",
        "- **Aplica√ß√£o**: Storytelling, descri√ß√µes\n",
        "\n",
        "### Aplica√ß√µes Pr√°ticas\n",
        "\n",
        "#### **1. Acessibilidade**\n",
        "- **Descri√ß√£o**: Descrever imagens para deficientes visuais\n",
        "- **Navega√ß√£o**: Ajudar na navega√ß√£o\n",
        "   - **Educa√ß√£o**: Recursos educativos\n",
        "   - **Inclus√£o**: Promover inclus√£o\n",
        "\n",
        "#### **2. An√°lise M√©dica**\n",
        "- **Diagn√≥stico**: Ajudar em diagn√≥sticos\n",
        "- **Educa√ß√£o**: Treinamento m√©dico\n",
        "   - **Pesquisa**: An√°lise de dados m√©dicos\n",
        "   - **Suporte**: Apoio a profissionais\n",
        "\n",
        "#### **3. An√°lise de Dados**\n",
        "- **Gr√°ficos**: Interpretar gr√°ficos e tabelas\n",
        "- **Relat√≥rios**: Gerar insights\n",
        "   - **Visualiza√ß√£o**: Melhorar visualiza√ß√µes\n",
        "   - **Decis√£o**: Apoiar tomada de decis√£o\n",
        "\n",
        "---\n",
        "\n",
        "## üîç 9.5 Demonstra√ß√£o Pr√°tica: Foundation Models Simples\n",
        "\n",
        "Vamos implementar e demonstrar conceitos de Foundation Models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "class FoundationModelsDemo:\n",
        "    \"\"\"Demonstra√ß√£o de conceitos de Foundation Models\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "    def create_synthetic_dataset(self, num_samples=1000):\n",
        "        \"\"\"Cria dataset sint√©tico para demonstra√ß√£o\"\"\"\n",
        "        \n",
        "        # Simular dados de texto e imagem\n",
        "        text_data = []\n",
        "        image_data = []\n",
        "        labels = []\n",
        "        \n",
        "        categories = [\n",
        "            {\"text\": \"a red car\", \"image\": [1, 0, 0], \"label\": 0},\n",
        "            {\"text\": \"a blue car\", \"image\": [0, 1, 0], \"label\": 1},\n",
        "            {\"text\": \"a green car\", \"image\": [0, 0, 1], \"label\": 2},\n",
        "            {\"text\": \"a red truck\", \"image\": [1, 0, 0], \"label\": 3},\n",
        "            {\"text\": \"a blue truck\", \"image\": [0, 1, 0], \"label\": 4},\n",
        "            {\"text\": \"a green truck\", \"image\": [0, 0, 1], \"label\": 5}\n",
        "        ]\n",
        "        \n",
        "        for i in range(num_samples):\n",
        "            # Selecionar categoria aleat√≥ria\n",
        "            category = categories[i % len(categories)]\n",
        "            \n",
        "            # Adicionar varia√ß√£o ao texto\n",
        "            text_variations = [\n",
        "                category[\"text\"],\n",
        "                f\"{category['text']} in the street\",\n",
        "                f\"{category['text']} parked outside\",\n",
        "                f\"{category['text']} driving fast\",\n",
        "                f\"{category['text']} at the traffic light\"\n",
        "            ]\n",
        "            \n",
        "            text = text_variations[i % len(text_variations)]\n",
        "            \n",
        "            # Adicionar ru√≠do √† imagem\n",
        "            image = np.array(category[\"image\"]) + np.random.normal(0, 0.1, 3)\n",
        "            image = np.clip(image, 0, 1)\n",
        "            \n",
        "            text_data.append(text)\n",
        "            image_data.append(image)\n",
        "            labels.append(category[\"label\"])\n",
        "        \n",
        "        return text_data, image_data, labels\n",
        "    \n",
        "    def simple_text_encoder(self, text_data, vocab_size=1000, embedding_dim=128):\n",
        "        \"\"\"Simula um encoder de texto simples\"\"\"\n",
        "        \n",
        "        # Criar vocabul√°rio simples\n",
        "        vocab = {}\n",
        "        for text in text_data:\n",
        "            words = text.lower().split()\n",
        "            for word in words:\n",
        "                if word not in vocab:\n",
        "                    vocab[word] = len(vocab)\n",
        "        \n",
        "        # Embedding layer\n",
        "        embedding = nn.Embedding(len(vocab), embedding_dim)\n",
        "        \n",
        "        # Encoder simples\n",
        "        encoder = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embedding_dim, embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embedding_dim, embedding_dim)\n",
        "        )\n",
        "        \n",
        "        # Processar textos\n",
        "        text_embeddings = []\n",
        "        for text in text_data:\n",
        "            words = text.lower().split()\n",
        "            word_ids = [vocab.get(word, 0) for word in words]\n",
        "            \n",
        "            # Padding para tamanho fixo\n",
        "            max_len = 10\n",
        "            if len(word_ids) > max_len:\n",
        "                word_ids = word_ids[:max_len]\n",
        "            else:\n",
        "                word_ids.extend([0] * (max_len - len(word_ids)))\n",
        "            \n",
        "            # Converter para tensor\n",
        "            word_tensor = torch.LongTensor(word_ids)\n",
        "            \n",
        "            # Embedding\n",
        "            word_embeddings = embedding(word_tensor)\n",
        "            \n",
        "            # M√©dia dos embeddings\n",
        "            text_embedding = torch.mean(word_embeddings, dim=0)\n",
        "            \n",
        "            # Encoder\n",
        "            text_embedding = encoder(text_embedding)\n",
        "            \n",
        "            text_embeddings.append(text_embedding.detach().numpy())\n",
        "        \n",
        "        return np.array(text_embeddings), vocab\n",
        "    \n",
        "    def simple_image_encoder(self, image_data, embedding_dim=128):\n",
        "        \"\"\"Simula um encoder de imagem simples\"\"\"\n",
        "        \n",
        "        # Encoder simples\n",
        "        encoder = nn.Sequential(\n",
        "            nn.Linear(3, embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embedding_dim, embedding_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embedding_dim, embedding_dim)\n",
        "        )\n",
        "        \n",
        "        # Processar imagens\n",
        "        image_embeddings = []\n",
        "        for image in image_data:\n",
        "            # Converter para tensor\n",
        "            image_tensor = torch.FloatTensor(image)\n",
        "            \n",
        "            # Encoder\n",
        "            image_embedding = encoder(image_tensor)\n",
        "            \n",
        "            image_embeddings.append(image_embedding.detach().numpy())\n",
        "        \n",
        "        return np.array(image_embeddings)\n",
        "    \n",
        "    def contrastive_loss(self, image_embeddings, text_embeddings, temperature=0.07):\n",
        "        \"\"\"Calcula perda contrastiva\"\"\"\n",
        "        \n",
        "        # Normalizar embeddings\n",
        "        image_embeddings = F.normalize(torch.FloatTensor(image_embeddings), dim=1)\n",
        "        text_embeddings = F.normalize(torch.FloatTensor(text_embeddings), dim=1)\n",
        "        \n",
        "        # Calcular similaridades\n",
        "        similarities = torch.matmul(image_embeddings, text_embeddings.T) / temperature\n",
        "        \n",
        "        # Labels (diagonal)\n",
        "        labels = torch.arange(similarities.size(0))\n",
        "        \n",
        "        # Perda de imagem para texto\n",
        "        loss_i2t = F.cross_entropy(similarities, labels)\n",
        "        \n",
        "        # Perda de texto para imagem\n",
        "        loss_t2i = F.cross_entropy(similarities.T, labels)\n",
        "        \n",
        "        # Perda total\n",
        "        total_loss = (loss_i2t + loss_t2i) / 2\n",
        "        \n",
        "        return total_loss.item()\n",
        "    \n",
        "    def zero_shot_classification(self, image_embeddings, text_embeddings, labels):\n",
        "        \"\"\"Simula classifica√ß√£o zero-shot\"\"\"\n",
        "        \n",
        "        # Normalizar embeddings\n",
        "        image_embeddings = F.normalize(torch.FloatTensor(image_embeddings), dim=1)\n",
        "        text_embeddings = F.normalize(torch.FloatTensor(text_embeddings), dim=1)\n",
        "        \n",
        "        # Calcular similaridades\n",
        "        similarities = torch.matmul(image_embeddings, text_embeddings.T)\n",
        "        \n",
        "        # Predi√ß√µes\n",
        "        predictions = torch.argmax(similarities, dim=1)\n",
        "        \n",
        "        # Calcular acur√°cia\n",
        "        accuracy = torch.mean((predictions == torch.LongTensor(labels)).float()).item()\n",
        "        \n",
        "        return predictions.numpy(), accuracy\n",
        "    \n",
        "    def visualize_embeddings(self, image_embeddings, text_embeddings, labels):\n",
        "        \"\"\"Visualiza embeddings em 2D\"\"\"\n",
        "        \n",
        "        from sklearn.manifold import TSNE\n",
        "        \n",
        "        # Combinar embeddings\n",
        "        all_embeddings = np.vstack([image_embeddings, text_embeddings])\n",
        "        \n",
        "        # t-SNE\n",
        "        tsne = TSNE(n_components=2, random_state=42)\n",
        "        embeddings_2d = tsne.fit_transform(all_embeddings)\n",
        "        \n",
        "        # Separar\n",
        "        image_2d = embeddings_2d[:len(image_embeddings)]\n",
        "        text_2d = embeddings_2d[len(image_embeddings):]\n",
        "        \n",
        "        # Visualizar\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "        \n",
        "        # Embeddings de imagem\n",
        "        scatter = axes[0].scatter(image_2d[:, 0], image_2d[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
        "        axes[0].set_title('Embeddings de Imagem')\n",
        "        axes[0].set_xlabel('t-SNE 1')\n",
        "        axes[0].set_ylabel('t-SNE 2')\n",
        "        plt.colorbar(scatter, ax=axes[0])\n",
        "        \n",
        "        # Embeddings de texto\n",
        "        scatter = axes[1].scatter(text_2d[:, 0], text_2d[:, 1], c=labels, cmap='tab10', alpha=0.7)\n",
        "        axes[1].set_title('Embeddings de Texto')\n",
        "        axes[1].set_xlabel('t-SNE 1')\n",
        "        axes[1].set_ylabel('t-SNE 2')\n",
        "        plt.colorbar(scatter, ax=axes[1])\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def demonstrate_foundation_models(self):\n",
        "        \"\"\"Demonstra conceitos de Foundation Models\"\"\"\n",
        "        \n",
        "        print(\"=== DEMONSTRA√á√ÉO: FOUNDATION MODELS ===\")\n",
        "        \n",
        "        # Criar dataset\n",
        "        text_data, image_data, labels = self.create_synthetic_dataset(500)\n",
        "        print(f\"Dataset criado: {len(text_data)} amostras\")\n",
        "        print(f\"Categorias: {len(set(labels))}\")\n",
        "        \n",
        "        # Encoder de texto\n",
        "        print(\"\\n=== ENCODING DE TEXTO ===\")\n",
        "        text_embeddings, vocab = self.simple_text_encoder(text_data)\n",
        "        print(f\"Vocabul√°rio: {len(vocab)} palavras\")\n",
        "        print(f\"Embeddings de texto: {text_embeddings.shape}\")\n",
        "        \n",
        "        # Encoder de imagem\n",
        "        print(\"\\n=== ENCODING DE IMAGEM ===\")\n",
        "        image_embeddings = self.simple_image_encoder(image_data)\n",
        "        print(f\"Embeddings de imagem: {image_embeddings.shape}\")\n",
        "        \n",
        "        # Perda contrastiva\n",
        "        print(\"\\n=== PERDA CONTRASTIVA ===\")\n",
        "        contrastive_loss = self.contrastive_loss(image_embeddings, text_embeddings)\n",
        "        print(f\"Perda contrastiva: {contrastive_loss:.4f}\")\n",
        "        \n",
        "        # Classifica√ß√£o zero-shot\n",
        "        print(\"\\n=== CLASSIFICA√á√ÉO ZERO-SHOT ===\")\n",
        "        predictions, accuracy = self.zero_shot_classification(image_embeddings, text_embeddings, labels)\n",
        "        print(f\"Acur√°cia zero-shot: {accuracy:.4f}\")\n",
        "        \n",
        "        # Visualizar embeddings\n",
        "        print(\"\\n=== VISUALIZA√á√ÉO DE EMBEDDINGS ===\")\n",
        "        self.visualize_embeddings(image_embeddings, text_embeddings, labels)\n",
        "        \n",
        "        # An√°lise quantitativa\n",
        "        print(\"\\n=== AN√ÅLISE QUANTITATIVA ===\")\n",
        "        \n",
        "        # Calcular similaridades\n",
        "        image_embeddings_norm = F.normalize(torch.FloatTensor(image_embeddings), dim=1)\n",
        "        text_embeddings_norm = F.normalize(torch.FloatTensor(text_embeddings), dim=1)\n",
        "        similarities = torch.matmul(image_embeddings_norm, text_embeddings_norm.T)\n",
        "        \n",
        "        print(f\"\\nEstat√≠sticas de Similaridade:\")\n",
        "        print(f\"  - Similaridade m√°xima: {torch.max(similarities):.4f}\")\n",
        "        print(f\"  - Similaridade m√≠nima: {torch.min(similarities):.4f}\")\n",
        "        print(f\"  - Similaridade m√©dia: {torch.mean(similarities):.4f}\")\n",
        "        print(f\"  - Desvio padr√£o: {torch.std(similarities):.4f}\")\n",
        "        \n",
        "        # An√°lise por categoria\n",
        "        print(f\"\\nAn√°lise por Categoria:\")\n",
        "        for i in range(len(set(labels))):\n",
        "            mask = np.array(labels) == i\n",
        "            category_similarities = similarities[mask, i]\n",
        "            print(f\"  - Categoria {i}: Similaridade m√©dia = {torch.mean(category_similarities):.4f}\")\n",
        "        \n",
        "        return {\n",
        "            'text_embeddings': text_embeddings,\n",
        "            'image_embeddings': image_embeddings,\n",
        "            'predictions': predictions,\n",
        "            'accuracy': accuracy,\n",
        "            'contrastive_loss': contrastive_loss\n",
        "        }\n",
        "\n",
        "# Executar demonstra√ß√£o\n",
        "print(\"=== DEMONSTRA√á√ÉO: FOUNDATION MODELS ===\")\n",
        "foundation_demo = FoundationModelsDemo()\n",
        "results = foundation_demo.demonstrate_foundation_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### An√°lise dos Resultados\n",
        "\n",
        "**Observa√ß√µes Importantes:**\n",
        "\n",
        "1. **Embeddings Multimodais**:\n",
        "   - **Alinhamento**: Imagens e textos similares ficam pr√≥ximos\n",
        "   - **Separa√ß√£o**: Diferentes categorias ficam separadas\n",
        "   - **Consist√™ncia**: Padr√µes consistentes entre modalidades\n",
        "\n",
        "2. **Classifica√ß√£o Zero-Shot**:\n",
        "   - **Acur√°cia**: Performance sem treinamento espec√≠fico\n",
        "   - **Generaliza√ß√£o**: Funciona em categorias n√£o vistas\n",
        "   - **Robustez**: Resistente a varia√ß√µes\n",
        "\n",
        "3. **Perda Contrastiva**:\n",
        "   - **Otimiza√ß√£o**: Minimiza dist√¢ncia entre pares corretos\n",
        "   - **Maximiza√ß√£o**: Maximiza dist√¢ncia entre pares incorretos\n",
        "   - **Converg√™ncia**: Deve diminuir com treinamento\n",
        "\n",
        "---\n",
        "\n",
        "## üåê 9.6 APIs e Aplica√ß√µes Pr√°ticas\n",
        "\n",
        "### APIs Dispon√≠veis\n",
        "\n",
        "![APIs Foundation Models](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo9/apis_foundation_models.png?raw=true)\n",
        "\n",
        "#### **1. OpenAI API**\n",
        "\n",
        "**Modelos Dispon√≠veis:**\n",
        "- **GPT-4**: Modelo de linguagem\n",
        "- **GPT-4V**: Modelo multimodal\n",
        "- **DALL-E 3**: Gera√ß√£o de imagens\n",
        "- **CLIP**: An√°lise imagem-texto\n",
        "\n",
        "**Caracter√≠sticas:**\n",
        "- **Alta qualidade**: Modelos state-of-the-art\n",
        "- **Facilidade de uso**: API simples\n",
        "- **Documenta√ß√£o**: Bem documentada\n",
        "- **Suporte**: Suporte t√©cnico\n",
        "\n",
        "#### **2. Google Gemini API**\n",
        "\n",
        "**Modelos Dispon√≠veis:**\n",
        "- **Gemini Pro**: Modelo de linguagem\n",
        "- **Gemini Pro Vision**: Modelo multimodal\n",
        "- **Imagen**: Gera√ß√£o de imagens\n",
        "- **PaLM**: Modelo de linguagem\n",
        "\n",
        "**Caracter√≠sticas:**\n",
        "- **Multimodal**: Suporte nativo a m√∫ltiplas modalidades\n",
        "- **Integra√ß√£o**: Integra√ß√£o com Google Cloud\n",
        "- **Escalabilidade**: Escal√°vel para grandes volumes\n",
        "- **Custo**: Pre√ßos competitivos\n",
        "\n",
        "### Casos de Uso Pr√°ticos\n",
        "\n",
        "#### **1. An√°lise de Conte√∫do**\n",
        "\n",
        "**Aplica√ß√µes:**\n",
        "- **Modera√ß√£o**: Modera√ß√£o de conte√∫do\n",
        "   - **Classifica√ß√£o**: Categoriza√ß√£o autom√°tica\n",
        "   - **Detec√ß√£o**: Detec√ß√£o de conte√∫do inadequado\n",
        "   - **An√°lise**: An√°lise de sentimento\n",
        "\n",
        "**Exemplo Pr√°tico:**\n",
        "- **Empresa**: Rede social\n",
        "   - **Problema**: Modera√ß√£o manual de conte√∫do\n",
        "   - **Solu√ß√£o**: GPT-4V para an√°lise autom√°tica\n",
        "   - **Resultado**: 90% de automa√ß√£o\n",
        "   - **ROI**: 80% de redu√ß√£o no tempo de modera√ß√£o\n",
        "\n",
        "#### **2. Gera√ß√£o de Conte√∫do**\n",
        "\n",
        "**Aplica√ß√µes:**\n",
        "- **Marketing**: Cria√ß√£o de conte√∫do\n",
        "   - **Design**: Gera√ß√£o de designs\n",
        "   - **Copywriting**: Cria√ß√£o de textos\n",
        "   - **Personaliza√ß√£o**: Conte√∫do personalizado\n",
        "\n",
        "**Exemplo Pr√°tico:**\n",
        "- **Empresa**: Ag√™ncia de marketing\n",
        "   - **Problema**: Cria√ß√£o manual de conte√∫do\n",
        "   - **Solu√ß√£o**: DALL-E + GPT-4 para gera√ß√£o\n",
        "   - **Resultado**: 70% de automa√ß√£o\n",
        "   - **ROI**: 60% de redu√ß√£o no tempo de cria√ß√£o\n",
        "\n",
        "#### **3. An√°lise de Dados**\n",
        "\n",
        "**Aplica√ß√µes:**\n",
        "- **Insights**: Gera√ß√£o de insights\n",
        "   - **Visualiza√ß√£o**: Interpreta√ß√£o de gr√°ficos\n",
        "   - **Relat√≥rios**: Cria√ß√£o de relat√≥rios\n",
        "   - **Decis√£o**: Apoio √† tomada de decis√£o\n",
        "\n",
        "**Exemplo Pr√°tico:**\n",
        "- **Empresa**: Empresa de consultoria\n",
        "   - **Problema**: An√°lise manual de dados\n",
        "   - **Solu√ß√£o**: GPT-4V para an√°lise de gr√°ficos\n",
        "   - **Resultado**: 85% de automa√ß√£o\n",
        "   - **ROI**: 75% de redu√ß√£o no tempo de an√°lise\n",
        "\n",
        "### Limita√ß√µes e Desafios\n",
        "\n",
        "#### **1. Custos**\n",
        "- **API**: Custos por uso\n",
        "   - **Escala**: Custos aumentam com escala\n",
        "   - **Modelo**: Modelos maiores s√£o mais caros\n",
        "   - **Otimiza√ß√£o**: Necessidade de otimiza√ß√£o\n",
        "\n",
        "#### **2. Qualidade**\n",
        "- **Consist√™ncia**: Qualidade pode variar\n",
        "   - **Vi√©s**: Pode refletir vi√©s dos dados\n",
        "   - **Hallucina√ß√£o**: Pode gerar conte√∫do incorreto\n",
        "   - **Valida√ß√£o**: Necessidade de valida√ß√£o\n",
        "\n",
        "#### **3. Controle**\n",
        "- **Personaliza√ß√£o**: Limitada personaliza√ß√£o\n",
        "   - **Fine-tuning**: Nem sempre dispon√≠vel\n",
        "   - **Par√¢metros**: Poucos par√¢metros ajust√°veis\n",
        "   - **Transpar√™ncia**: Processo pouco transparente\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Resumo do M√≥dulo 9\n",
        "\n",
        "### Principais Conceitos Abordados\n",
        "\n",
        "1. **Fundamentos**: Foundation Models e caracter√≠sticas\n",
        "2. **CLIP**: Contrastive Language-Image Pre-training\n",
        "3. **DALL-E**: Text-to-Image Generation\n",
        "4. **GPT-4V**: Vision-Language Model\n",
        "5. **APIs**: Aplica√ß√µes pr√°ticas e limita√ß√µes\n",
        "\n",
        "### Demonstra√ß√µes Pr√°ticas\n",
        "\n",
        "**1. Foundation Models Simples:**\n",
        "   - Implementa√ß√£o de encoders de texto e imagem\n",
        "   - Treinamento contrastivo\n",
        "   - Classifica√ß√£o zero-shot\n",
        "   - Visualiza√ß√£o de embeddings\n",
        "\n",
        "### Pr√≥ximos Passos\n",
        "\n",
        "No **M√≥dulo 10**, aplicaremos todos os conceitos aprendidos em uma **Atividade Final Pr√°tica** completa.\n",
        "\n",
        "### Refer√™ncias Principais\n",
        "\n",
        "- [On the Opportunities and Risks of Foundation Models - Bommasani et al.](https://arxiv.org/abs/2108.07258)\n",
        "- [CLIP: Learning Transferable Visual Representations - Radford et al.](https://arxiv.org/abs/2103.00020)\n",
        "\n",
        "---\n",
        "\n",
        "**Pr√≥ximo M√≥dulo**: Atividade Final Pr√°tica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Conex√£o com o Pr√≥ximo M√≥dulo\n",
        "\n",
        "Agora que dominamos **Foundation Models** para vis√£o computacional, estamos preparados para aplicar todos os conceitos aprendidos em uma **Atividade Final Pr√°tica** completa.\n",
        "\n",
        "No **M√≥dulo 10**, veremos como:\n",
        "\n",
        "### üîó **Conex√µes Diretas:**\n",
        "\n",
        "1. **Foundation Models** ‚Üí **Sistema Completo**\n",
        "   - CLIP, DALL-E, GPT-4V\n",
        "   - Integra√ß√£o em sistema multimodal\n",
        "\n",
        "2. **APIs** ‚Üí **Implementa√ß√£o Pr√°tica**\n",
        "   - OpenAI e Gemini APIs\n",
        "   - Sistema funcional com APIs reais\n",
        "\n",
        "3. **Conceitos Te√≥ricos** ‚Üí **Aplica√ß√£o Pr√°tica**\n",
        "   - Todos os m√≥dulos anteriores\n",
        "   - Implementa√ß√£o de sistema completo\n",
        "\n",
        "4. **Demonstra√ß√µes** ‚Üí **Projeto Real**\n",
        "   - Demonstra√ß√µes isoladas\n",
        "   - Projeto integrado e funcional\n",
        "\n",
        "### üöÄ **Evolu√ß√£o Natural:**\n",
        "\n",
        "- **Fundamentos** ‚Üí **Aplica√ß√£o**\n",
        "- **Conceitos** ‚Üí **Implementa√ß√£o**\n",
        "- **Demonstra√ß√µes** ‚Üí **Projeto**\n",
        "- **Teoria** ‚Üí **Pr√°tica**\n",
        "\n",
        "Esta transi√ß√£o marca a **culmina√ß√£o** de todo o curso em um projeto pr√°tico completo!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üñºÔ∏è Imagens de Refer√™ncia - M√≥dulo 9\n\n",
        "![Aplica√ß√µes Pr√°ticas](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo9/aplicacoes_praticas.png?raw=true)\n\n",
        "![Modelo CLIP](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo9/clip_model.png?raw=true)\n\n",
        "![Modelo DALL-E](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo9/dalle_model.png?raw=true)\n\n",
        "![Modelo GPT-4V](https://raw.githubusercontent.com/rfapo/visao-computacional/main/images/modulo9/gpt4v_model.png?raw=true)\n\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}