{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# M√≥dulo 8: Vision Transformers e Mecanismos de Aten√ß√£o\n",
        "\n",
        "## üéØ Objetivos de Aprendizagem\n",
        "\n",
        "Ao final deste m√≥dulo, voc√™ ser√° capaz de:\n",
        "\n",
        "- ‚úÖ Compreender os fundamentos dos mecanismos de aten√ß√£o\n",
        "- ‚úÖ Entender a arquitetura Vision Transformer (ViT)\n",
        "- ‚úÖ Implementar modelos de aten√ß√£o para vis√£o computacional\n",
        "- ‚úÖ Analisar vantagens e limita√ß√µes dos Vision Transformers\n",
        "- ‚úÖ Aplicar ViT em tarefas pr√°ticas de vis√£o computacional\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ 8.1 Introdu√ß√£o aos Mecanismos de Aten√ß√£o\n",
        "\n",
        "### Conceito Fundamental\n",
        "\n",
        "**Mecanismos de Aten√ß√£o** s√£o componentes fundamentais que permitem aos modelos focar em partes espec√≠ficas dos dados de entrada, revolucionando tanto o processamento de linguagem natural quanto a vis√£o computacional.\n",
        "\n",
        "![Introdu√ß√£o Mecanismos Aten√ß√£o](https://github.com/rfapo/visao-computacional/blob/main/images/modulo8/introducao_mecanismos_atencao.png)\n",
        "\n",
        "### Defini√ß√£o e Caracter√≠sticas\n",
        "\n",
        "**Defini√ß√£o de Aten√ß√£o:**\n",
        "- **Foco seletivo**: Capacidade de concentrar recursos computacionais em partes relevantes\n",
        "- **Pesos din√¢micos**: Atribui√ß√£o de import√¢ncia vari√°vel a diferentes elementos\n",
        "- **Contexto global**: Considera√ß√£o de todas as informa√ß√µes dispon√≠veis\n",
        "- **Processamento paralelo**: Capacidade de processar m√∫ltiplas rela√ß√µes simultaneamente\n",
        "\n",
        "### Analogia Biol√≥gica\n",
        "\n",
        "**Sistema de Aten√ß√£o Humano:**\n",
        "- **Aten√ß√£o humana**: Foco seletivo em est√≠mulos relevantes\n",
        "- **Processamento visual**: Concentra√ß√£o em caracter√≠sticas importantes\n",
        "- **Mem√≥ria de trabalho**: Manuten√ß√£o de informa√ß√µes relevantes\n",
        "- **Decis√£o**: Integra√ß√£o de m√∫ltiplas fontes de informa√ß√£o\n",
        "\n",
        "**Analogia com Redes Neurais:**\n",
        "| Sistema Biol√≥gico | Rede Neural | Fun√ß√£o |\n",
        "|-------------------|-------------|--------|\n",
        "| **Aten√ß√£o seletiva** | Attention weights | Foco em features relevantes |\n",
        "| **Processamento paralelo** | Multi-head attention | M√∫ltiplas representa√ß√µes |\n",
        "| **Contexto global** | Self-attention | Rela√ß√µes entre todos os elementos |\n",
        "| **Mem√≥ria de trabalho** | Hidden states | Manuten√ß√£o de informa√ß√£o |\n",
        "\n",
        "### Tipos de Aten√ß√£o\n",
        "\n",
        "![Tipos de Aten√ß√£o](https://github.com/rfapo/visao-computacional/blob/main/images/modulo8/tipos_atencao.png)\n",
        "\n",
        "#### **1. Self-Attention (Aten√ß√£o Pr√≥pria)**\n",
        "\n",
        "**Caracter√≠sticas:**\n",
        "- **Rela√ß√µes internas**: Cada elemento interage com todos os outros\n",
        "- **Pesos computados**: Import√¢ncia calculada dinamicamente\n",
        "- **Contexto completo**: Considera√ß√£o de toda a sequ√™ncia\n",
        "- **Paraleliza√ß√£o**: Processamento simult√¢neo\n",
        "\n",
        "**F√≥rmula:**\n",
        "```\n",
        "Attention(Q,K,V) = softmax(QK^T/‚àöd_k)V\n",
        "```\n",
        "\n",
        "**Aplica√ß√µes:**\n",
        "- **Processamento de sequ√™ncias**: NLP, time series\n",
        "- **An√°lise de imagens**: Rela√ß√µes entre pixels\n",
        "- **Recomenda√ß√£o**: Rela√ß√µes entre usu√°rios e itens\n",
        "\n",
        "#### **2. Multi-Head Attention**\n",
        "\n",
        "**Caracter√≠sticas:**\n",
        "- **M√∫ltiplas cabe√ßas**: Diferentes representa√ß√µes de aten√ß√£o\n",
        "- **Diversidade**: Captura diferentes tipos de rela√ß√µes\n",
        "- **Concatena√ß√£o**: Combina√ß√£o de m√∫ltiplas cabe√ßas\n",
        "- **Flexibilidade**: Maior capacidade expressiva\n",
        "\n",
        "**F√≥rmula:**\n",
        "```\n",
        "MultiHead(Q,K,V) = Concat(head_1, ..., head_h)W^O\n",
        "head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\n",
        "```\n",
        "\n",
        "**Vantagens:**\n",
        "- ‚úÖ **Diversidade**: Diferentes tipos de aten√ß√£o\n",
        "- ‚úÖ **Capacidade**: Maior poder expressivo\n",
        "- ‚úÖ **Robustez**: Menos sens√≠vel a falhas\n",
        "- ‚úÖ **Flexibilidade**: Adapta√ß√£o a diferentes tarefas\n",
        "\n",
        "#### **3. Cross-Attention**\n",
        "\n",
        "**Caracter√≠sticas:**\n",
        "- **Intera√ß√£o**: Entre sequ√™ncias diferentes\n",
        "- **Query**: De uma sequ√™ncia\n",
        "- **Key/Value**: De outra sequ√™ncia\n",
        "- **Aplica√ß√£o**: Tradu√ß√£o, gera√ß√£o, multimodal\n",
        "\n",
        "**F√≥rmula:**\n",
        "```\n",
        "CrossAttention(Q,K,V) = softmax(QK^T/‚àöd_k)V\n",
        "onde Q vem de uma sequ√™ncia e K,V de outra\n",
        "```\n",
        "\n",
        "**Aplica√ß√µes:**\n",
        "- **Tradu√ß√£o**: Rela√ß√µes entre idiomas\n",
        "- **Gera√ß√£o**: Rela√ß√µes entre texto e imagem\n",
        "- **Multimodal**: Integra√ß√£o de diferentes modalidades\n",
        "\n",
        "### Evolu√ß√£o dos Mecanismos de Aten√ß√£o\n",
        "\n",
        "![Evolu√ß√£o Mecanismos Aten√ß√£o](https://github.com/rfapo/visao-computacional/blob/main/images/modulo8/evolucao_mecanismos_atencao.png)\n",
        "\n",
        "#### **Progress√£o Hist√≥rica:**\n",
        "\n",
        "| Ano | Marco | Contribui√ß√£o |\n",
        "|-----|-------|--------------|\n",
        "| **2014** | Attention mechanism | Introdu√ß√£o para tradu√ß√£o |\n",
        "| **2017** | Transformer | Self-attention puro |\n",
        "| **2018** | BERT | Aten√ß√£o bidirecional |\n",
        "| **2020** | Vision Transformer | Aten√ß√£o para imagens |\n",
        "| **2021** | CLIP | Aten√ß√£o multimodal |\n",
        "| **2022** | DALL-E | Aten√ß√£o para gera√ß√£o |\n",
        "\n",
        "#### **Marcos Importantes:**\n",
        "- **2014**: Neural Machine Translation by Jointly Learning to Align and Translate - Bahdanau et al.\n",
        "- **2017**: Attention Is All You Need - Vaswani et al.\n",
        "- **2020**: An Image is Worth 16x16 Words: Transformers for Image Recognition - Dosovitskiy et al.\n",
        "- **2021**: CLIP: Learning Transferable Visual Representations - Radford et al.\n",
        "\n",
        "---\n",
        "\n",
        "## üñºÔ∏è 8.2 Vision Transformers (ViT)\n",
        "\n",
        "### Conceito Fundamental\n",
        "\n",
        "**Vision Transformers** s√£o arquiteturas que aplicam o mecanismo de aten√ß√£o dos Transformers para processar imagens, tratando patches de imagem como \"tokens\" em uma sequ√™ncia.\n",
        "\n",
        "![Arquitetura Vision Transformer](https://github.com/rfapo/visao-computacional/blob/main/images/modulo8/arquitetura_vision_transformer.png)\n",
        "\n",
        "### Componentes Principais\n",
        "\n",
        "#### **1. Patch Embedding**\n",
        "\n",
        "**Fun√ß√£o:**\n",
        "- **Divis√£o**: Imagem dividida em patches\n",
        "- **Embedding**: Cada patch convertido em vetor\n",
        "- **Lineariza√ß√£o**: Patches tratados como sequ√™ncia\n",
        "- **Dimensionalidade**: Redu√ß√£o para dimens√£o fixa\n",
        "\n",
        "**Processo:**\n",
        "```\n",
        "Imagem (H√óW√óC) ‚Üí Patches (N√óP¬≤√óC) ‚Üí Embedding (N√óD)\n",
        "```\n",
        "\n",
        "**Par√¢metros:**\n",
        "- **Patch size**: Tamanho de cada patch (ex: 16√ó16)\n",
        "- **Embedding dimension**: Dimens√£o do vetor (ex: 768)\n",
        "- **Number of patches**: N = (H√óW)/(P¬≤)\n",
        "\n",
        "#### **2. Positional Embedding**\n",
        "\n",
        "**Fun√ß√£o:**\n",
        "- **Posi√ß√£o**: Informa√ß√£o sobre localiza√ß√£o dos patches\n",
        "- **Soma**: Adicionada ao patch embedding\n",
        "- **Aprendizado**: Par√¢metros aprend√≠veis\n",
        "- **Invari√¢ncia**: Preserva informa√ß√£o espacial\n",
        "\n",
        "**Tipos:**\n",
        "- **1D**: Posi√ß√£o linear na sequ√™ncia\n",
        "- **2D**: Posi√ß√£o (x,y) na imagem\n",
        "- **Aprend√≠vel**: Par√¢metros otimiz√°veis\n",
        "- **Sinusoidal**: Fun√ß√µes seno/cosseno\n",
        "\n",
        "#### **3. Transformer Encoder**\n",
        "\n",
        "**Estrutura:**\n",
        "```\n",
        "Multi-Head Self-Attention ‚Üí Add & Norm ‚Üí Feed Forward ‚Üí Add & Norm\n",
        "```\n",
        "\n",
        "**Componentes:**\n",
        "- **Multi-Head Attention**: M√∫ltiplas cabe√ßas de aten√ß√£o\n",
        "- **Feed Forward**: Rede feed-forward\n",
        "- **Add & Norm**: Residual connection + layer normalization\n",
        "- **Layers**: M√∫ltiplas camadas empilhadas\n",
        "\n",
        "#### **4. Classification Head**\n",
        "\n",
        "**Fun√ß√£o:**\n",
        "- **CLS token**: Token especial para classifica√ß√£o\n",
        "- **Pooling**: Agrega√ß√£o de informa√ß√µes\n",
        "- **Linear**: Camada linear final\n",
        "- **Softmax**: Probabilidades das classes\n",
        "\n",
        "**Processo:**\n",
        "```\n",
        "CLS token ‚Üí Transformer ‚Üí CLS output ‚Üí Linear ‚Üí Softmax\n",
        "```\n",
        "\n",
        "### Vantagens dos Vision Transformers\n",
        "\n",
        "#### **1. Escalabilidade**\n",
        "- **Dados**: Performance melhora com mais dados\n",
        "- **Modelo**: Arquitetura escal√°vel\n",
        "- **Computa√ß√£o**: Paraleliza√ß√£o eficiente\n",
        "- **Treinamento**: Est√°vel com grandes datasets\n",
        "\n",
        "#### **2. Interpretabilidade**\n",
        "- **Attention maps**: Visualiza√ß√£o de aten√ß√£o\n",
        "- **Patch importance**: Import√¢ncia de cada patch\n",
        "- **Global context**: Contexto global vis√≠vel\n",
        "- **Debugging**: Facilita debugging\n",
        "\n",
        "#### **3. Flexibilidade**\n",
        "- **Arquitetura**: F√°cil modifica√ß√£o\n",
        "- **Tarefas**: Adapt√°vel a diferentes tarefas\n",
        "- **Modalidades**: Extens√≠vel a outras modalidades\n",
        "- **Integra√ß√£o**: Combina√ß√£o com outras arquiteturas\n",
        "\n",
        "### Limita√ß√µes dos Vision Transformers\n",
        "\n",
        "#### **1. Dados**\n",
        "- **Requisito**: Necessita grandes datasets\n",
        "- **Overfitting**: Risco com datasets pequenos\n",
        "- **Transfer learning**: Depend√™ncia de modelos pr√©-treinados\n",
        "- **Custo**: Treinamento caro\n",
        "\n",
        "#### **2. Computa√ß√£o**\n",
        "- **Complexidade**: O(n¬≤) com n√∫mero de patches\n",
        "- **Mem√≥ria**: Alto uso de mem√≥ria\n",
        "- **Infer√™ncia**: Pode ser lenta\n",
        "- **Recursos**: Requer recursos computacionais\n",
        "\n",
        "#### **3. Indu√ß√£o**\n",
        "- **Bias**: Vi√©s para padr√µes globais\n",
        "- **Locais**: Menos eficiente para padr√µes locais\n",
        "- **CNNs**: CNNs ainda melhores para algumas tarefas\n",
        "- **H√≠brido**: Combina√ß√£o com CNNs pode ser melhor\n",
        "\n",
        "---\n",
        "\n",
        "## üîç 8.3 Demonstra√ß√£o Pr√°tica: Mecanismos de Aten√ß√£o\n",
        "\n",
        "Vamos implementar e visualizar diferentes tipos de aten√ß√£o:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "class AttentionMechanismsDemo:\n",
        "    \"\"\"Demonstra√ß√£o de diferentes mecanismos de aten√ß√£o\"\"\"\n",
        "    \n",
        "    def __init__(self, seq_len=10, d_model=64):\n",
        "        self.seq_len = seq_len\n",
        "        self.d_model = d_model\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "    def create_sample_data(self):\n",
        "        \"\"\"Cria dados de exemplo para demonstra√ß√£o\"\"\"\n",
        "        \n",
        "        # Criar sequ√™ncia de entrada\n",
        "        x = torch.randn(1, self.seq_len, self.d_model)\n",
        "        \n",
        "        # Adicionar padr√µes espec√≠ficos\n",
        "        # Padr√£o 1: Valores altos no meio\n",
        "        x[0, 3:7, :] += 2.0\n",
        "        \n",
        "        # Padr√£o 2: Valores baixos no in√≠cio\n",
        "        x[0, 0:2, :] -= 1.5\n",
        "        \n",
        "        # Padr√£o 3: Valores m√©dios no final\n",
        "        x[0, 8:, :] += 0.5\n",
        "        \n",
        "        return x\n",
        "    \n",
        "    def self_attention(self, x):\n",
        "        \"\"\"Implementa self-attention\"\"\"\n",
        "        \n",
        "        # Linear transformations\n",
        "        W_q = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        W_k = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        W_v = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        \n",
        "        # Compute Q, K, V\n",
        "        Q = W_q(x)\n",
        "        K = W_k(x)\n",
        "        V = W_v(x)\n",
        "        \n",
        "        # Compute attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_model)\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        \n",
        "        # Apply attention to values\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        \n",
        "        return output, attention_weights\n",
        "    \n",
        "    def multi_head_attention(self, x, num_heads=8):\n",
        "        \"\"\"Implementa multi-head attention\"\"\"\n",
        "        \n",
        "        class MultiHeadAttention(nn.Module):\n",
        "            def __init__(self, d_model, num_heads):\n",
        "                super(MultiHeadAttention, self).__init__()\n",
        "                self.d_model = d_model\n",
        "                self.num_heads = num_heads\n",
        "                self.d_k = d_model // num_heads\n",
        "                \n",
        "                self.W_q = nn.Linear(d_model, d_model)\n",
        "                self.W_k = nn.Linear(d_model, d_model)\n",
        "                self.W_v = nn.Linear(d_model, d_model)\n",
        "                self.W_o = nn.Linear(d_model, d_model)\n",
        "                \n",
        "            def forward(self, x):\n",
        "                batch_size, seq_len, d_model = x.size()\n",
        "                \n",
        "                # Linear transformations\n",
        "                Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "                K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "                V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
        "                \n",
        "                # Compute attention scores\n",
        "                scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
        "                attention_weights = F.softmax(scores, dim=-1)\n",
        "                \n",
        "                # Apply attention to values\n",
        "                attended_values = torch.matmul(attention_weights, V)\n",
        "                \n",
        "                # Concatenate heads\n",
        "                attended_values = attended_values.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
        "                \n",
        "                # Output projection\n",
        "                output = self.W_o(attended_values)\n",
        "                \n",
        "                return output, attention_weights\n",
        "        \n",
        "        mha = MultiHeadAttention(self.d_model, num_heads)\n",
        "        output, attention_weights = mha(x)\n",
        "        \n",
        "        return output, attention_weights\n",
        "    \n",
        "    def cross_attention(self, x1, x2):\n",
        "        \"\"\"Implementa cross-attention\"\"\"\n",
        "        \n",
        "        # Linear transformations\n",
        "        W_q = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        W_k = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        W_v = nn.Linear(self.d_model, self.d_model, bias=False)\n",
        "        \n",
        "        # Compute Q from x1, K and V from x2\n",
        "        Q = W_q(x1)\n",
        "        K = W_k(x2)\n",
        "        V = W_v(x2)\n",
        "        \n",
        "        # Compute attention scores\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_model)\n",
        "        attention_weights = F.softmax(scores, dim=-1)\n",
        "        \n",
        "        # Apply attention to values\n",
        "        output = torch.matmul(attention_weights, V)\n",
        "        \n",
        "        return output, attention_weights\n",
        "    \n",
        "    def visualize_attention_weights(self, attention_weights, title=\"Attention Weights\"):\n",
        "        \"\"\"Visualiza pesos de aten√ß√£o\"\"\"\n",
        "        \n",
        "        # Converter para numpy\n",
        "        if isinstance(attention_weights, torch.Tensor):\n",
        "            attention_weights = attention_weights.detach().numpy()\n",
        "        \n",
        "        # Remover dimens√µes de batch se necess√°rio\n",
        "        if attention_weights.ndim == 4:  # Multi-head attention\n",
        "            attention_weights = attention_weights[0]  # Primeiro batch\n",
        "        elif attention_weights.ndim == 3:\n",
        "            attention_weights = attention_weights[0]  # Primeiro batch\n",
        "        \n",
        "        # Visualizar\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        \n",
        "        if attention_weights.ndim == 3:  # Multi-head\n",
        "            # Visualizar cada cabe√ßa\n",
        "            num_heads = attention_weights.shape[0]\n",
        "            cols = min(4, num_heads)\n",
        "            rows = (num_heads + cols - 1) // cols\n",
        "            \n",
        "            for i in range(num_heads):\n",
        "                plt.subplot(rows, cols, i + 1)\n",
        "                sns.heatmap(attention_weights[i], cmap='Blues', cbar=True)\n",
        "                plt.title(f'Head {i + 1}')\n",
        "                plt.xlabel('Key Position')\n",
        "                plt.ylabel('Query Position')\n",
        "        else:  # Single head\n",
        "            sns.heatmap(attention_weights, cmap='Blues', cbar=True)\n",
        "            plt.title(title)\n",
        "            plt.xlabel('Key Position')\n",
        "            plt.ylabel('Query Position')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def demonstrate_attention_mechanisms(self):\n",
        "        \"\"\"Demonstra diferentes mecanismos de aten√ß√£o\"\"\"\n",
        "        \n",
        "        # Criar dados de exemplo\n",
        "        x = self.create_sample_data()\n",
        "        \n",
        "        print(\"=== DEMONSTRA√á√ÉO: MECANISMOS DE ATEN√á√ÉO ===\")\n",
        "        print(f\"Sequ√™ncia de entrada: {x.shape}\")\n",
        "        print(f\"Dimens√£o do modelo: {self.d_model}\")\n",
        "        print(f\"Comprimento da sequ√™ncia: {self.seq_len}\")\n",
        "        \n",
        "        # Self-attention\n",
        "        print(\"\\n1. Self-Attention:\")\n",
        "        sa_output, sa_weights = self.self_attention(x)\n",
        "        print(f\"   Output shape: {sa_output.shape}\")\n",
        "        print(f\"   Attention weights shape: {sa_weights.shape}\")\n",
        "        \n",
        "        # Multi-head attention\n",
        "        print(\"\\n2. Multi-Head Attention:\")\n",
        "        mha_output, mha_weights = self.multi_head_attention(x, num_heads=8)\n",
        "        print(f\"   Output shape: {mha_output.shape}\")\n",
        "        print(f\"   Attention weights shape: {mha_weights.shape}\")\n",
        "        \n",
        "        # Cross-attention\n",
        "        print(\"\\n3. Cross-Attention:\")\n",
        "        x2 = torch.randn(1, self.seq_len, self.d_model)  # Segunda sequ√™ncia\n",
        "        ca_output, ca_weights = self.cross_attention(x, x2)\n",
        "        print(f\"   Output shape: {ca_output.shape}\")\n",
        "        print(f\"   Attention weights shape: {ca_weights.shape}\")\n",
        "        \n",
        "        # Visualizar pesos de aten√ß√£o\n",
        "        print(\"\\n=== VISUALIZA√á√ÉO DOS PESOS DE ATEN√á√ÉO ===\")\n",
        "        \n",
        "        # Self-attention\n",
        "        self.visualize_attention_weights(sa_weights, \"Self-Attention Weights\")\n",
        "        \n",
        "        # Multi-head attention\n",
        "        self.visualize_attention_weights(mha_weights, \"Multi-Head Attention Weights\")\n",
        "        \n",
        "        # Cross-attention\n",
        "        self.visualize_attention_weights(ca_weights, \"Cross-Attention Weights\")\n",
        "        \n",
        "        # An√°lise quantitativa\n",
        "        print(\"\\n=== AN√ÅLISE QUANTITATIVA ===\")\n",
        "        \n",
        "        # Self-attention\n",
        "        sa_weights_np = sa_weights.detach().numpy()[0]\n",
        "        print(f\"\\nSelf-Attention:\")\n",
        "        print(f\"  - Peso m√°ximo: {np.max(sa_weights_np):.4f}\")\n",
        "        print(f\"  - Peso m√≠nimo: {np.min(sa_weights_np):.4f}\")\n",
        "        print(f\"  - Peso m√©dio: {np.mean(sa_weights_np):.4f}\")\n",
        "        print(f\"  - Entropia: {np.sum(-sa_weights_np * np.log(sa_weights_np + 1e-8)):.4f}\")\n",
        "        \n",
        "        # Multi-head attention\n",
        "        mha_weights_np = mha_weights.detach().numpy()[0]\n",
        "        print(f\"\\nMulti-Head Attention:\")\n",
        "        print(f\"  - Peso m√°ximo: {np.max(mha_weights_np):.4f}\")\n",
        "        print(f\"  - Peso m√≠nimo: {np.min(mha_weights_np):.4f}\")\n",
        "        print(f\"  - Peso m√©dio: {np.mean(mha_weights_np):.4f}\")\n",
        "        print(f\"  - Entropia m√©dia: {np.mean([np.sum(-head * np.log(head + 1e-8)) for head in mha_weights_np]):.4f}\")\n",
        "        \n",
        "        # Cross-attention\n",
        "        ca_weights_np = ca_weights.detach().numpy()[0]\n",
        "        print(f\"\\nCross-Attention:\")\n",
        "        print(f\"  - Peso m√°ximo: {np.max(ca_weights_np):.4f}\")\n",
        "        print(f\"  - Peso m√≠nimo: {np.min(ca_weights_np):.4f}\")\n",
        "        print(f\"  - Peso m√©dio: {np.mean(ca_weights_np):.4f}\")\n",
        "        print(f\"  - Entropia: {np.sum(-ca_weights_np * np.log(ca_weights_np + 1e-8)):.4f}\")\n",
        "        \n",
        "        return {\n",
        "            'self_attention': (sa_output, sa_weights),\n",
        "            'multi_head_attention': (mha_output, mha_weights),\n",
        "            'cross_attention': (ca_output, ca_weights)\n",
        "        }\n",
        "\n",
        "# Executar demonstra√ß√£o\n",
        "print(\"=== DEMONSTRA√á√ÉO: MECANISMOS DE ATEN√á√ÉO ===\")\n",
        "attention_demo = AttentionMechanismsDemo(seq_len=10, d_model=64)\n",
        "results = attention_demo.demonstrate_attention_mechanisms()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### An√°lise dos Resultados\n",
        "\n",
        "**Observa√ß√µes Importantes:**\n",
        "\n",
        "1. **Self-Attention**:\n",
        "   - **Pesos**: Distribui√ß√£o de aten√ß√£o entre elementos\n",
        "   - **Padr√µes**: Identifica√ß√£o de rela√ß√µes importantes\n",
        "   - **Contexto**: Considera√ß√£o de toda a sequ√™ncia\n",
        "\n",
        "2. **Multi-Head Attention**:\n",
        "   - **Diversidade**: Diferentes cabe√ßas capturam diferentes padr√µes\n",
        "   - **Robustez**: Menos sens√≠vel a falhas\n",
        "   - **Capacidade**: Maior poder expressivo\n",
        "\n",
        "3. **Cross-Attention**:\n",
        "   - **Intera√ß√£o**: Rela√ß√µes entre sequ√™ncias diferentes\n",
        "   - **Flexibilidade**: Adapta√ß√£o a diferentes modalidades\n",
        "   - **Aplica√ß√£o**: √ötil para tarefas multimodais\n",
        "\n",
        "---\n",
        "\n",
        "## üñºÔ∏è 8.4 Demonstra√ß√£o Pr√°tica: Vision Transformer Simples\n",
        "\n",
        "Vamos implementar e visualizar um Vision Transformer simples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "class SimpleVisionTransformer:\n",
        "    \"\"\"Implementa√ß√£o de um Vision Transformer simples\"\"\"\n",
        "    \n",
        "    def __init__(self, img_size=32, patch_size=8, num_classes=10, d_model=128, num_heads=8, num_layers=6):\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_classes = num_classes\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.num_layers = num_layers\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "        # Calcular n√∫mero de patches\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        \n",
        "        # Inicializar modelo\n",
        "        self.vit = self._build_vit()\n",
        "        \n",
        "    def _build_vit(self):\n",
        "        \"\"\"Constr√≥i o Vision Transformer\"\"\"\n",
        "        \n",
        "        class PatchEmbedding(nn.Module):\n",
        "            def __init__(self, img_size, patch_size, d_model):\n",
        "                super(PatchEmbedding, self).__init__()\n",
        "                self.img_size = img_size\n",
        "                self.patch_size = patch_size\n",
        "                self.num_patches = (img_size // patch_size) ** 2\n",
        "                \n",
        "                # Patch embedding\n",
        "                self.patch_embed = nn.Conv2d(3, d_model, kernel_size=patch_size, stride=patch_size)\n",
        "                \n",
        "                # Positional embedding\n",
        "                self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, d_model))\n",
        "                \n",
        "                # CLS token\n",
        "                self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
        "                \n",
        "            def forward(self, x):\n",
        "                B = x.shape[0]\n",
        "                \n",
        "                # Patch embedding\n",
        "                x = self.patch_embed(x)  # (B, d_model, H/patch_size, W/patch_size)\n",
        "                x = x.flatten(2).transpose(1, 2)  # (B, num_patches, d_model)\n",
        "                \n",
        "                # Adicionar CLS token\n",
        "                cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "                x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches + 1, d_model)\n",
        "                \n",
        "                # Adicionar positional embedding\n",
        "                x = x + self.pos_embed\n",
        "                \n",
        "                return x\n",
        "        \n",
        "        class TransformerBlock(nn.Module):\n",
        "            def __init__(self, d_model, num_heads):\n",
        "                super(TransformerBlock, self).__init__()\n",
        "                \n",
        "                # Multi-head attention\n",
        "                self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
        "                \n",
        "                # Feed forward\n",
        "                self.feed_forward = nn.Sequential(\n",
        "                    nn.Linear(d_model, d_model * 4),\n",
        "                    nn.GELU(),\n",
        "                    nn.Linear(d_model * 4, d_model)\n",
        "                )\n",
        "                \n",
        "                # Layer normalization\n",
        "                self.norm1 = nn.LayerNorm(d_model)\n",
        "                self.norm2 = nn.LayerNorm(d_model)\n",
        "                \n",
        "            def forward(self, x):\n",
        "                # Self-attention\n",
        "                attn_output, attn_weights = self.attention(x, x, x)\n",
        "                x = x + attn_output\n",
        "                x = self.norm1(x)\n",
        "                \n",
        "                # Feed forward\n",
        "                ff_output = self.feed_forward(x)\n",
        "                x = x + ff_output\n",
        "                x = self.norm2(x)\n",
        "                \n",
        "                return x, attn_weights\n",
        "        \n",
        "        class VisionTransformer(nn.Module):\n",
        "            def __init__(self, img_size, patch_size, num_classes, d_model, num_heads, num_layers):\n",
        "                super(VisionTransformer, self).__init__()\n",
        "                \n",
        "                # Patch embedding\n",
        "                self.patch_embedding = PatchEmbedding(img_size, patch_size, d_model)\n",
        "                \n",
        "                # Transformer blocks\n",
        "                self.transformer_blocks = nn.ModuleList([\n",
        "                    TransformerBlock(d_model, num_heads) for _ in range(num_layers)\n",
        "                ])\n",
        "                \n",
        "                # Classification head\n",
        "                self.classifier = nn.Linear(d_model, num_classes)\n",
        "                \n",
        "            def forward(self, x):\n",
        "                # Patch embedding\n",
        "                x = self.patch_embedding(x)\n",
        "                \n",
        "                # Transformer blocks\n",
        "                attention_weights = []\n",
        "                for transformer_block in self.transformer_blocks:\n",
        "                    x, attn_weights = transformer_block(x)\n",
        "                    attention_weights.append(attn_weights)\n",
        "                \n",
        "                # Classification\n",
        "                cls_output = x[:, 0]  # CLS token\n",
        "                logits = self.classifier(cls_output)\n",
        "                \n",
        "                return logits, attention_weights\n",
        "        \n",
        "        return VisionTransformer(\n",
        "            self.img_size, self.patch_size, self.num_classes,\n",
        "            self.d_model, self.num_heads, self.num_layers\n",
        "        ).to(self.device)\n",
        "    \n",
        "    def create_sample_images(self, num_samples=16):\n",
        "        \"\"\"Cria imagens de exemplo para demonstra√ß√£o\"\"\"\n",
        "        \n",
        "        images = []\n",
        "        \n",
        "        for _ in range(num_samples):\n",
        "            # Criar imagem com padr√µes simples\n",
        "            img = np.random.rand(3, self.img_size, self.img_size) * 2 - 1  # Normalizar para [-1, 1]\n",
        "            \n",
        "            # Adicionar padr√µes\n",
        "            if np.random.random() > 0.5:\n",
        "                # Padr√£o circular\n",
        "                center_x, center_y = np.random.randint(8, self.img_size-8, 2)\n",
        "                radius = np.random.randint(4, 8)\n",
        "                \n",
        "                y, x = np.ogrid[:self.img_size, :self.img_size]\n",
        "                mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2\n",
        "                \n",
        "                img[0, mask] = 1.0  # Red\n",
        "                img[1, mask] = 0.0  # Green\n",
        "                img[2, mask] = 0.0  # Blue\n",
        "            else:\n",
        "                # Padr√£o retangular\n",
        "                x1, y1 = np.random.randint(0, self.img_size-12, 2)\n",
        "                x2, y2 = x1 + np.random.randint(8, 16), y1 + np.random.randint(8, 16)\n",
        "                \n",
        "                img[0, y1:y2, x1:x2] = 0.0  # Red\n",
        "                img[1, y1:y2, x1:x2] = 1.0  # Green\n",
        "                img[2, y1:y2, x1:x2] = 0.0  # Blue\n",
        "            \n",
        "            images.append(img)\n",
        "        \n",
        "        return torch.FloatTensor(images)\n",
        "    \n",
        "    def visualize_patches(self, img, patch_size):\n",
        "        \"\"\"Visualiza patches de uma imagem\"\"\"\n",
        "        \n",
        "        # Converter para numpy\n",
        "        if isinstance(img, torch.Tensor):\n",
        "            img = img.detach().numpy()\n",
        "        \n",
        "        # Normalizar para [0, 1]\n",
        "        img = (img + 1) / 2\n",
        "        img = np.clip(img, 0, 1)\n",
        "        \n",
        "        # Converter para formato de visualiza√ß√£o\n",
        "        img_vis = img.transpose(1, 2, 0)\n",
        "        \n",
        "        # Criar grid de patches\n",
        "        h, w = img_vis.shape[:2]\n",
        "        num_patches_h = h // patch_size\n",
        "        num_patches_w = w // patch_size\n",
        "        \n",
        "        fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "        ax.imshow(img_vis)\n",
        "        \n",
        "        # Desenhar linhas dos patches\n",
        "        for i in range(num_patches_h + 1):\n",
        "            ax.axhline(y=i * patch_size, color='red', linewidth=1)\n",
        "        for j in range(num_patches_w + 1):\n",
        "            ax.axvline(x=j * patch_size, color='red', linewidth=1)\n",
        "        \n",
        "        ax.set_title(f'Imagem com Patches ({patch_size}√ó{patch_size})')\n",
        "        ax.axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        return num_patches_h * num_patches_w\n",
        "    \n",
        "    def visualize_attention_maps(self, attention_weights, img_size, patch_size):\n",
        "        \"\"\"Visualiza mapas de aten√ß√£o\"\"\"\n",
        "        \n",
        "        # Converter para numpy\n",
        "        if isinstance(attention_weights, torch.Tensor):\n",
        "            attention_weights = attention_weights.detach().numpy()\n",
        "        \n",
        "        # Remover dimens√µes de batch\n",
        "        if attention_weights.ndim == 4:  # Multi-head attention\n",
        "            attention_weights = attention_weights[0]  # Primeiro batch\n",
        "        elif attention_weights.ndim == 3:\n",
        "            attention_weights = attention_weights[0]  # Primeiro batch\n",
        "        \n",
        "        # Calcular n√∫mero de patches\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "        \n",
        "        # Visualizar\n",
        "        if attention_weights.ndim == 3:  # Multi-head\n",
        "            num_heads = attention_weights.shape[0]\n",
        "            cols = min(4, num_heads)\n",
        "            rows = (num_heads + cols - 1) // cols\n",
        "            \n",
        "            fig, axes = plt.subplots(rows, cols, figsize=(15, 4 * rows))\n",
        "            if rows == 1:\n",
        "                axes = [axes] if cols == 1 else axes\n",
        "            else:\n",
        "                axes = axes.flatten()\n",
        "            \n",
        "            for i in range(num_heads):\n",
        "                # CLS token attention (primeira linha)\n",
        "                cls_attention = attention_weights[i, 0, 1:]  # Remover CLS token\n",
        "                \n",
        "                # Reshape para imagem\n",
        "                patch_size_h = img_size // patch_size\n",
        "                attention_map = cls_attention.reshape(patch_size_h, patch_size_h)\n",
        "                \n",
        "                # Visualizar\n",
        "                im = axes[i].imshow(attention_map, cmap='hot', interpolation='nearest')\n",
        "                axes[i].set_title(f'Head {i + 1} - CLS Attention')\n",
        "                axes[i].axis('off')\n",
        "                plt.colorbar(im, ax=axes[i])\n",
        "            \n",
        "            # Ocultar eixos extras\n",
        "            for i in range(num_heads, len(axes)):\n",
        "                axes[i].axis('off')\n",
        "        else:  # Single head\n",
        "            # CLS token attention\n",
        "            cls_attention = attention_weights[0, 1:]  # Remover CLS token\n",
        "            \n",
        "            # Reshape para imagem\n",
        "            patch_size_h = img_size // patch_size\n",
        "            attention_map = cls_attention.reshape(patch_size_h, patch_size_h)\n",
        "            \n",
        "            # Visualizar\n",
        "            plt.figure(figsize=(8, 8))\n",
        "            plt.imshow(attention_map, cmap='hot', interpolation='nearest')\n",
        "            plt.title('CLS Token Attention Map')\n",
        "            plt.colorbar()\n",
        "            plt.axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    def demonstrate_vit(self):\n",
        "        \"\"\"Demonstra o Vision Transformer\"\"\"\n",
        "        \n",
        "        print(\"=== DEMONSTRA√á√ÉO: VISION TRANSFORMER ===\")\n",
        "        print(f\"Tamanho da imagem: {self.img_size}√ó{self.img_size}\")\n",
        "        print(f\"Tamanho do patch: {self.patch_size}√ó{self.patch_size}\")\n",
        "        print(f\"N√∫mero de patches: {self.num_patches}\")\n",
        "        print(f\"Dimens√£o do modelo: {self.d_model}\")\n",
        "        print(f\"N√∫mero de cabe√ßas: {self.num_heads}\")\n",
        "        print(f\"N√∫mero de camadas: {self.num_layers}\")\n",
        "        \n",
        "        # Criar imagens de exemplo\n",
        "        images = self.create_sample_images(16)\n",
        "        \n",
        "        # Visualizar patches\n",
        "        print(\"\\n=== VISUALIZA√á√ÉO DE PATCHES ===\")\n",
        "        num_patches = self.visualize_patches(images[0], self.patch_size)\n",
        "        print(f\"N√∫mero de patches por imagem: {num_patches}\")\n",
        "        \n",
        "        # Processar com ViT\n",
        "        print(\"\\n=== PROCESSAMENTO COM VIT ===\")\n",
        "        self.vit.eval()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # Processar primeira imagem\n",
        "            img = images[0:1].to(self.device)\n",
        "            logits, attention_weights = self.vit(img)\n",
        "            \n",
        "            print(f\"Logits shape: {logits.shape}\")\n",
        "            print(f\"N√∫mero de camadas de aten√ß√£o: {len(attention_weights)}\")\n",
        "            print(f\"Shape dos pesos de aten√ß√£o: {attention_weights[0].shape}\")\n",
        "            \n",
        "            # Predi√ß√£o\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            predicted_class = torch.argmax(probs, dim=1).item()\n",
        "            confidence = probs[0, predicted_class].item()\n",
        "            \n",
        "            print(f\"Classe predita: {predicted_class}\")\n",
        "            print(f\"Confian√ßa: {confidence:.4f}\")\n",
        "        \n",
        "        # Visualizar mapas de aten√ß√£o\n",
        "        print(\"\\n=== VISUALIZA√á√ÉO DE MAPAS DE ATEN√á√ÉO ===\")\n",
        "        \n",
        "        # Primeira camada\n",
        "        print(\"\\nPrimeira camada:\")\n",
        "        self.visualize_attention_maps(attention_weights[0], self.img_size, self.patch_size)\n",
        "        \n",
        "        # √öltima camada\n",
        "        print(\"\\n√öltima camada:\")\n",
        "        self.visualize_attention_maps(attention_weights[-1], self.img_size, self.patch_size)\n",
        "        \n",
        "        # An√°lise quantitativa\n",
        "        print(\"\\n=== AN√ÅLISE QUANTITATIVA ===\")\n",
        "        \n",
        "        # Analisar evolu√ß√£o da aten√ß√£o\n",
        "        first_layer_attention = attention_weights[0].detach().numpy()[0]  # Primeira cabe√ßa\n",
        "        last_layer_attention = attention_weights[-1].detach().numpy()[0]  # Primeira cabe√ßa\n",
        "        \n",
        "        print(f\"\\nEvolu√ß√£o da Aten√ß√£o:\")\n",
        "        print(f\"  - Primeira camada - Entropia: {np.sum(-first_layer_attention * np.log(first_layer_attention + 1e-8)):.4f}\")\n",
        "        print(f\"  - √öltima camada - Entropia: {np.sum(-last_layer_attention * np.log(last_layer_attention + 1e-8)):.4f}\")\n",
        "        print(f\"  - Diferen√ßa de entropia: {np.sum(-last_layer_attention * np.log(last_layer_attention + 1e-8)) - np.sum(-first_layer_attention * np.log(first_layer_attention + 1e-8)):.4f}\")\n",
        "        \n",
        "        # Analisar concentra√ß√£o da aten√ß√£o\n",
        "        first_cls_attention = first_layer_attention[0, 1:]  # CLS token\n",
        "        last_cls_attention = last_layer_attention[0, 1:]  # CLS token\n",
        "        \n",
        "        print(f\"\\nConcentra√ß√£o da Aten√ß√£o (CLS Token):\")\n",
        "        print(f\"  - Primeira camada - Max: {np.max(first_cls_attention):.4f}\")\n",
        "        print(f\"  - Primeira camada - Min: {np.min(first_cls_attention):.4f}\")\n",
        "        print(f\"  - √öltima camada - Max: {np.max(last_cls_attention):.4f}\")\n",
        "        print(f\"  - √öltima camada - Min: {np.min(last_cls_attention):.4f}\")\n",
        "        \n",
        "        return {\n",
        "            'logits': logits,\n",
        "            'attention_weights': attention_weights,\n",
        "            'predicted_class': predicted_class,\n",
        "            'confidence': confidence\n",
        "        }\n",
        "\n",
        "# Executar demonstra√ß√£o\n",
        "print(\"=== DEMONSTRA√á√ÉO: VISION TRANSFORMER ===\")\n",
        "vit_demo = SimpleVisionTransformer(\n",
        "    img_size=32, patch_size=8, num_classes=10,\n",
        "    d_model=128, num_heads=8, num_layers=6\n",
        ")\n",
        "results = vit_demo.demonstrate_vit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### An√°lise dos Resultados\n",
        "\n",
        "**Observa√ß√µes Importantes:**\n",
        "\n",
        "1. **Patches**:\n",
        "   - **Divis√£o**: Imagem dividida em patches regulares\n",
        "   - **Embedding**: Cada patch convertido em vetor\n",
        "   - **Posi√ß√£o**: Informa√ß√£o de posi√ß√£o preservada\n",
        "\n",
        "2. **Mapas de Aten√ß√£o**:\n",
        "   - **Evolu√ß√£o**: Aten√ß√£o muda entre camadas\n",
        "   - **Concentra√ß√£o**: √öltima camada mais concentrada\n",
        "   - **Interpretabilidade**: Visualiza√ß√£o de foco\n",
        "\n",
        "3. **Classifica√ß√£o**:\n",
        "   - **CLS Token**: Agrega informa√ß√£o global\n",
        "   - **Confian√ßa**: Medida de certeza da predi√ß√£o\n",
        "   - **Performance**: ViT funciona bem para classifica√ß√£o\n",
        "\n",
        "---\n",
        "\n",
        "## üìä 8.5 Compara√ß√£o: CNNs vs Vision Transformers\n",
        "\n",
        "### An√°lise Comparativa\n",
        "\n",
        "![Compara√ß√£o CNNs vs ViT](https://github.com/rfapo/visao-computacional/blob/main/images/modulo8/comparacao_cnns_vit.png)\n",
        "\n",
        "#### **Arquitetura**\n",
        "\n",
        "| Aspecto | CNNs | Vision Transformers |\n",
        "|---------|------|---------------------|\n",
        "| **Indu√ß√£o** | Local | Global |\n",
        "| **Paraleliza√ß√£o** | Limitada | Completa |\n",
        "| **Escalabilidade** | Moderada | Alta |\n",
        "| **Interpretabilidade** | Baixa | Alta |\n",
        "\n",
        "#### **Performance**\n",
        "\n",
        "| Aspecto | CNNs | Vision Transformers |\n",
        "|---------|------|---------------------|\n",
        "| **Dados pequenos** | Boa | Limitada |\n",
        "| **Dados grandes** | Boa | Excelente |\n",
        "| **Treinamento** | Est√°vel | Est√°vel |\n",
        "| **Infer√™ncia** | R√°pida | Moderada |\n",
        "\n",
        "#### **Aplica√ß√µes**\n",
        "\n",
        "| Aspecto | CNNs | Vision Transformers |\n",
        "|---------|------|---------------------|\n",
        "| **Classifica√ß√£o** | Excelente | Excelente |\n",
        "| **Detec√ß√£o** | Excelente | Boa |\n",
        "| **Segmenta√ß√£o** | Excelente | Boa |\n",
        "| **Transfer Learning** | Boa | Excelente |\n",
        "\n",
        "### Quando Usar Cada Um\n",
        "\n",
        "#### **Use CNNs quando:**\n",
        "- ‚úÖ **Dados limitados** est√£o dispon√≠veis\n",
        "- ‚úÖ **Padr√µes locais** s√£o importantes\n",
        "- ‚úÖ **Velocidade** √© priorit√°ria\n",
        "- ‚úÖ **Recursos limitados** est√£o dispon√≠veis\n",
        "\n",
        "#### **Use Vision Transformers quando:**\n",
        "- ‚úÖ **Grandes datasets** est√£o dispon√≠veis\n",
        "- ‚úÖ **Padr√µes globais** s√£o importantes\n",
        "- ‚úÖ **Interpretabilidade** √© necess√°ria\n",
        "- ‚úÖ **Transfer learning** √© priorit√°rio\n",
        "\n",
        "---\n",
        "\n",
        "## üìù Resumo do M√≥dulo 8\n",
        "\n",
        "### Principais Conceitos Abordados\n",
        "\n",
        "1. **Fundamentos**: Mecanismos de aten√ß√£o\n",
        "2. **Tipos**: Self-attention, Multi-head, Cross-attention\n",
        "3. **Vision Transformers**: Arquitetura para imagens\n",
        "4. **Implementa√ß√£o**: Demonstra√ß√µes pr√°ticas\n",
        "5. **Compara√ß√£o**: CNNs vs Vision Transformers\n",
        "\n",
        "### Demonstra√ß√µes Pr√°ticas\n",
        "\n",
        "**1. Mecanismos de Aten√ß√£o:**\n",
        "   - Implementa√ß√£o de diferentes tipos de aten√ß√£o\n",
        "   - Visualiza√ß√£o de pesos de aten√ß√£o\n",
        "   - An√°lise quantitativa\n",
        "\n",
        "**2. Vision Transformer:**\n",
        "   - Implementa√ß√£o de ViT simples\n",
        "   - Visualiza√ß√£o de patches e mapas de aten√ß√£o\n",
        "   - An√°lise de evolu√ß√£o da aten√ß√£o\n",
        "\n",
        "### Pr√≥ximos Passos\n",
        "\n",
        "No **M√≥dulo 9**, exploraremos **Foundation Models** para vis√£o computacional, incluindo CLIP, DALL-E e GPT-4V.\n",
        "\n",
        "### Refer√™ncias Principais\n",
        "\n",
        "- [Attention Is All You Need - Vaswani et al.](https://arxiv.org/abs/1706.03762)\n",
        "- [An Image is Worth 16x16 Words: Transformers for Image Recognition - Dosovitskiy et al.](https://arxiv.org/abs/2010.11929)\n",
        "\n",
        "---\n",
        "\n",
        "**Pr√≥ximo M√≥dulo**: Foundation Models para Vis√£o Computacional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Conex√£o com o Pr√≥ximo M√≥dulo\n",
        "\n",
        "Agora que dominamos **Vision Transformers e Mecanismos de Aten√ß√£o**, estamos preparados para explorar **Foundation Models** para vis√£o computacional.\n",
        "\n",
        "No **M√≥dulo 9**, veremos como:\n",
        "\n",
        "### üîó **Conex√µes Diretas:**\n",
        "\n",
        "1. **Aten√ß√£o** ‚Üí **Foundation Models**\n",
        "   - Vision Transformers usam aten√ß√£o\n",
        "   - Foundation Models s√£o baseados em Transformers\n",
        "\n",
        "2. **Arquiteturas Escal√°veis** ‚Üí **Modelos Massivos**\n",
        "   - ViT √© escal√°vel\n",
        "   - Foundation Models s√£o modelos massivos\n",
        "\n",
        "3. **Transfer Learning** ‚Üí **Zero-shot Learning**\n",
        "   - ViT √© bom para transfer learning\n",
        "   - Foundation Models permitem zero-shot learning\n",
        "\n",
        "4. **Interpretabilidade** ‚Üí **Capacidades Emergentes**\n",
        "   - ViT √© interpret√°vel\n",
        "   - Foundation Models t√™m capacidades emergentes\n",
        "\n",
        "### üöÄ **Evolu√ß√£o Natural:**\n",
        "\n",
        "- **Aten√ß√£o** ‚Üí **Foundation Models**\n",
        "- **Arquiteturas** ‚Üí **Modelos Massivos**\n",
        "- **Transfer Learning** ‚Üí **Zero-shot Learning**\n",
        "- **Interpretabilidade** ‚Üí **Capacidades Emergentes**\n",
        "\n",
        "Esta transi√ß√£o marca o in√≠cio da **era dos Foundation Models** em vis√£o computacional!"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}