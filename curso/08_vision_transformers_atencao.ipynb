{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MÃ³dulo 8: Vision Transformers e Mecanismos de AtenÃ§Ã£o",
    "",
    "## ğŸ¯ Objetivos de Aprendizagem",
    "",
    "Ao final deste mÃ³dulo, vocÃª serÃ¡ capaz de:",
    "",
    "- âœ… Compreender os fundamentos dos mecanismos de atenÃ§Ã£o",
    "- âœ… Entender a arquitetura Vision Transformer (ViT)",
    "- âœ… Implementar modelos de atenÃ§Ã£o para visÃ£o computacional",
    "- âœ… Analisar vantagens e limitaÃ§Ãµes dos Vision Transformers",
    "- âœ… Aplicar ViT em tarefas prÃ¡ticas de visÃ£o computacional",
    "",
    "---",
    "",
    "## ğŸ¯ 8.1 IntroduÃ§Ã£o aos Mecanismos de AtenÃ§Ã£o",
    "",
    "### Conceito Fundamental",
    "",
    "**Mecanismos de AtenÃ§Ã£o** sÃ£o componentes fundamentais que permitem aos modelos focar em partes especÃ­ficas dos dados de entrada, revolucionando tanto o processamento de linguagem natural quanto a visÃ£o computacional.",
    "",
    "![IntroduÃ§Ã£o Mecanismos AtenÃ§Ã£o](https://jalammar.github.io/images/t/transformer_self-attention_visualization.png)",
    "",
    "### DefiniÃ§Ã£o e CaracterÃ­sticas",
    "",
    "**DefiniÃ§Ã£o de AtenÃ§Ã£o:**",
    "- **Foco seletivo**: Capacidade de concentrar recursos computacionais em partes relevantes",
    "- **Pesos dinÃ¢micos**: AtribuiÃ§Ã£o de importÃ¢ncia variÃ¡vel a diferentes elementos",
    "- **Contexto global**: ConsideraÃ§Ã£o de todas as informaÃ§Ãµes disponÃ­veis",
    "- **Processamento paralelo**: Capacidade de processar mÃºltiplas relaÃ§Ãµes simultaneamente",
    "",
    "### Analogia BiolÃ³gica",
    "",
    "**Sistema de AtenÃ§Ã£o Humano:**",
    "- **AtenÃ§Ã£o humana**: Foco seletivo em estÃ­mulos relevantes",
    "- **Processamento visual**: ConcentraÃ§Ã£o em caracterÃ­sticas importantes",
    "- **MemÃ³ria de trabalho**: ManutenÃ§Ã£o de informaÃ§Ãµes relevantes",
    "- **DecisÃ£o**: IntegraÃ§Ã£o de mÃºltiplas fontes de informaÃ§Ã£o",
    "",
    "**Analogia com Redes Neurais:**",
    "| Sistema BiolÃ³gico | Rede Neural | FunÃ§Ã£o |",
    "|-------------------|-------------|--------|",
    "| **AtenÃ§Ã£o seletiva** | Attention weights | Foco em features relevantes |",
    "| **Processamento paralelo** | Multi-head attention | MÃºltiplas representaÃ§Ãµes |",
    "| **Contexto global** | Self-attention | RelaÃ§Ãµes entre todos os elementos |",
    "| **MemÃ³ria de trabalho** | Hidden states | ManutenÃ§Ã£o de informaÃ§Ã£o |",
    "",
    "### Tipos de AtenÃ§Ã£o",
    "",
    "![Tipos de AtenÃ§Ã£o](https://miro.medium.com/v2/resize:fit:1400/1*1bNW4Dp0E2vrOQgTqfVFiw.png)",
    "",
    "#### **1. Self-Attention (AtenÃ§Ã£o PrÃ³pria)**",
    "",
    "**CaracterÃ­sticas:**",
    "- **RelaÃ§Ãµes internas**: Cada elemento interage com todos os outros",
    "- **Pesos computados**: ImportÃ¢ncia calculada dinamicamente",
    "- **Contexto completo**: ConsideraÃ§Ã£o de toda a sequÃªncia",
    "- **ParalelizaÃ§Ã£o**: Processamento simultÃ¢neo",
    "",
    "**FÃ³rmula:**",
    "```",
    "Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V",
    "```",
    "",
    "**AplicaÃ§Ãµes:**",
    "- **Processamento de sequÃªncias**: NLP, time series",
    "- **AnÃ¡lise de imagens**: RelaÃ§Ãµes entre pixels",
    "- **RecomendaÃ§Ã£o**: RelaÃ§Ãµes entre usuÃ¡rios e itens",
    "",
    "#### **2. Multi-Head Attention**",
    "",
    "**CaracterÃ­sticas:**",
    "- **MÃºltiplas cabeÃ§as**: Diferentes representaÃ§Ãµes de atenÃ§Ã£o",
    "- **Diversidade**: Captura diferentes tipos de relaÃ§Ãµes",
    "- **ConcatenaÃ§Ã£o**: CombinaÃ§Ã£o de mÃºltiplas cabeÃ§as",
    "- **Flexibilidade**: Maior capacidade expressiva",
    "",
    "**FÃ³rmula:**",
    "```",
    "MultiHead(Q,K,V) = Concat(head_1, ..., head_h)W^O",
    "head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)",
    "```",
    "",
    "**Vantagens:**",
    "- âœ… **Diversidade**: Diferentes tipos de atenÃ§Ã£o",
    "- âœ… **Capacidade**: Maior poder expressivo",
    "- âœ… **Robustez**: Menos sensÃ­vel a falhas",
    "- âœ… **Flexibilidade**: AdaptaÃ§Ã£o a diferentes tarefas",
    "",
    "#### **3. Cross-Attention**",
    "",
    "**CaracterÃ­sticas:**",
    "- **InteraÃ§Ã£o**: Entre sequÃªncias diferentes",
    "- **Query**: De uma sequÃªncia",
    "- **Key/Value**: De outra sequÃªncia",
    "- **AplicaÃ§Ã£o**: TraduÃ§Ã£o, geraÃ§Ã£o, multimodal",
    "",
    "**FÃ³rmula:**",
    "```",
    "CrossAttention(Q,K,V) = softmax(QK^T/âˆšd_k)V",
    "onde Q vem de uma sequÃªncia e K,V de outra",
    "```",
    "",
    "**AplicaÃ§Ãµes:**",
    "- **TraduÃ§Ã£o**: RelaÃ§Ãµes entre idiomas",
    "- **GeraÃ§Ã£o**: RelaÃ§Ãµes entre texto e imagem",
    "- **Multimodal**: IntegraÃ§Ã£o de diferentes modalidades",
    "",
    "### EvoluÃ§Ã£o dos Mecanismos de AtenÃ§Ã£o",
    "",
    "![EvoluÃ§Ã£o Mecanismos AtenÃ§Ã£o](https://miro.medium.com/v2/resize:fit:1400/1*BHzGVskWGS_3jEcYYi6miQ.png)",
    "",
    "#### **ProgressÃ£o HistÃ³rica:**",
    "",
    "| Ano | Marco | ContribuiÃ§Ã£o |",
    "|-----|-------|--------------|",
    "| **2014** | Attention mechanism | IntroduÃ§Ã£o para traduÃ§Ã£o |",
    "| **2017** | Transformer | Self-attention puro |",
    "| **2018** | BERT | AtenÃ§Ã£o bidirecional |",
    "| **2020** | Vision Transformer | AtenÃ§Ã£o para imagens |",
    "| **2021** | CLIP | AtenÃ§Ã£o multimodal |",
    "| **2022** | DALL-E | AtenÃ§Ã£o para geraÃ§Ã£o |",
    "",
    "#### **Marcos Importantes:**",
    "- **2014**: Neural Machine Translation by Jointly Learning to Align and Translate - Bahdanau et al.",
    "- **2017**: Attention Is All You Need - Vaswani et al.",
    "- **2020**: An Image is Worth 16x16 Words: Transformers for Image Recognition - Dosovitskiy et al.",
    "- **2021**: CLIP: Learning Transferable Visual Representations - Radford et al.",
    "",
    "---",
    "",
    "## ğŸ–¼ï¸ 8.2 Vision Transformers (ViT)",
    "",
    "### Conceito Fundamental",
    "",
    "**Vision Transformers** sÃ£o arquiteturas que aplicam o mecanismo de atenÃ§Ã£o dos Transformers para processar imagens, tratando patches de imagem como \"tokens\" em uma sequÃªncia.",
    "",
    "![Arquitetura Vision Transformer](https://raw.githubusercontent.com/google-research/vision_transformer/main/vit_figure.png)",
    "",
    "### Componentes Principais",
    "",
    "#### **1. Patch Embedding**",
    "",
    "**FunÃ§Ã£o:**",
    "- **DivisÃ£o**: Imagem dividida em patches",
    "- **Embedding**: Cada patch convertido em vetor",
    "- **LinearizaÃ§Ã£o**: Patches tratados como sequÃªncia",
    "- **Dimensionalidade**: ReduÃ§Ã£o para dimensÃ£o fixa",
    "",
    "**Processo:**",
    "```",
    "Imagem (HÃ—WÃ—C) â†’ Patches (NÃ—PÂ²Ã—C) â†’ Embedding (NÃ—D)",
    "```",
    "",
    "**ParÃ¢metros:**",
    "- **Patch size**: Tamanho de cada patch (ex: 16Ã—16)",
    "- **Embedding dimension**: DimensÃ£o do vetor (ex: 768)",
    "- **Number of patches**: N = (HÃ—W)/(PÂ²)",
    "",
    "#### **2. Positional Embedding**",
    "",
    "**FunÃ§Ã£o:**",
    "- **PosiÃ§Ã£o**: InformaÃ§Ã£o sobre localizaÃ§Ã£o dos patches",
    "- **Soma**: Adicionada ao patch embedding",
    "- **Aprendizado**: ParÃ¢metros aprendÃ­veis",
    "- **InvariÃ¢ncia**: Preserva informaÃ§Ã£o espacial",
    "",
    "**Tipos:**",
    "- **1D**: PosiÃ§Ã£o linear na sequÃªncia",
    "- **2D**: PosiÃ§Ã£o (x,y) na imagem",
    "- **AprendÃ­vel**: ParÃ¢metros otimizÃ¡veis",
    "- **Sinusoidal**: FunÃ§Ãµes seno/cosseno",
    "",
    "#### **3. Transformer Encoder**",
    "",
    "**Estrutura:**",
    "```",
    "Multi-Head Self-Attention â†’ Add & Norm â†’ Feed Forward â†’ Add & Norm",
    "```",
    "",
    "**Componentes:**",
    "- **Multi-Head Attention**: MÃºltiplas cabeÃ§as de atenÃ§Ã£o",
    "- **Feed Forward**: Rede feed-forward",
    "- **Add & Norm**: Residual connection + layer normalization",
    "- **Layers**: MÃºltiplas camadas empilhadas",
    "",
    "#### **4. Classification Head**",
    "",
    "**FunÃ§Ã£o:**",
    "- **CLS token**: Token especial para classificaÃ§Ã£o",
    "- **Pooling**: AgregaÃ§Ã£o de informaÃ§Ãµes",
    "- **Linear**: Camada linear final",
    "- **Softmax**: Probabilidades das classes",
    "",
    "**Processo:**",
    "```",
    "CLS token â†’ Transformer â†’ CLS output â†’ Linear â†’ Softmax",
    "```",
    "",
    "### Vantagens dos Vision Transformers",
    "",
    "#### **1. Escalabilidade**",
    "- **Dados**: Performance melhora com mais dados",
    "- **Modelo**: Arquitetura escalÃ¡vel",
    "- **ComputaÃ§Ã£o**: ParalelizaÃ§Ã£o eficiente",
    "- **Treinamento**: EstÃ¡vel com grandes datasets",
    "",
    "#### **2. Interpretabilidade**",
    "- **Attention maps**: VisualizaÃ§Ã£o de atenÃ§Ã£o",
    "- **Patch importance**: ImportÃ¢ncia de cada patch",
    "- **Global context**: Contexto global visÃ­vel",
    "- **Debugging**: Facilita debugging",
    "",
    "#### **3. Flexibilidade**",
    "- **Arquitetura**: FÃ¡cil modificaÃ§Ã£o",
    "- **Tarefas**: AdaptÃ¡vel a diferentes tarefas",
    "- **Modalidades**: ExtensÃ­vel a outras modalidades",
    "- **IntegraÃ§Ã£o**: CombinaÃ§Ã£o com outras arquiteturas",
    "",
    "### LimitaÃ§Ãµes dos Vision Transformers",
    "",
    "#### **1. Dados**",
    "- **Requisito**: Necessita grandes datasets",
    "- **Overfitting**: Risco com datasets pequenos",
    "- **Transfer learning**: DependÃªncia de modelos prÃ©-treinados",
    "- **Custo**: Treinamento caro",
    "",
    "#### **2. ComputaÃ§Ã£o**",
    "- **Complexidade**: O(nÂ²) com nÃºmero de patches",
    "- **MemÃ³ria**: Alto uso de memÃ³ria",
    "- **InferÃªncia**: Pode ser lenta",
    "- **Recursos**: Requer recursos computacionais",
    "",
    "#### **3. InduÃ§Ã£o**",
    "- **Bias**: ViÃ©s para padrÃµes globais",
    "- **Locais**: Menos eficiente para padrÃµes locais",
    "- **CNNs**: CNNs ainda melhores para algumas tarefas",
    "- **HÃ­brido**: CombinaÃ§Ã£o com CNNs pode ser melhor",
    "",
    "---",
    "",
    "## ğŸ” 8.3 DemonstraÃ§Ã£o PrÃ¡tica: Mecanismos de AtenÃ§Ã£o",
    "",
    "Vamos implementar e visualizar diferentes tipos de atenÃ§Ã£o:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "class AttentionMechanismsDemo:\n",
    "    \"\"\"DemonstraÃ§Ã£o de diferentes mecanismos de atenÃ§Ã£o\"\"\"\n",
    "    \n",
    "    def __init__(self, seq_len=10, d_model=64):\n",
    "        self.seq_len = seq_len\n",
    "        self.d_model = d_model\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    def create_sample_data(self):\n",
    "        \"\"\"Cria dados de exemplo para demonstraÃ§Ã£o\"\"\"\n",
    "        \n",
    "        # Criar sequÃªncia de entrada\n",
    "        x = torch.randn(1, self.seq_len, self.d_model)\n",
    "        \n",
    "        # Adicionar padrÃµes especÃ­ficos\n",
    "        # PadrÃ£o 1: Valores altos no meio\n",
    "        x[0, 3:7, :] += 2.0\n",
    "        \n",
    "        # PadrÃ£o 2: Valores baixos no inÃ­cio\n",
    "        x[0, 0:2, :] -= 1.5\n",
    "        \n",
    "        # PadrÃ£o 3: Valores mÃ©dios no final\n",
    "        x[0, 8:, :] += 0.5\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def self_attention(self, x):\n",
    "        \"\"\"Implementa self-attention\"\"\"\n",
    "        \n",
    "        # Linear transformations\n",
    "        W_q = nn.Linear(self.d_model, self.d_model, bias=False)\n",
    "        W_k = nn.Linear(self.d_model, self.d_model, bias=False)\n",
    "        W_v = nn.Linear(self.d_model, self.d_model, bias=False)\n",
    "        \n",
    "        # Compute Q, K, V\n",
    "        Q = W_q(x)\n",
    "        K = W_k(x)\n",
    "        V = W_v(x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_model)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def multi_head_attention(self, x, num_heads=8):\n",
    "        \"\"\"Implementa multi-head attention\"\"\"\n",
    "        \n",
    "        class MultiHeadAttention(nn.Module):\n",
    "            def __init__(self, d_model, num_heads):\n",
    "                super(MultiHeadAttention, self).__init__()\n",
    "                self.d_model = d_model\n",
    "                self.num_heads = num_heads\n",
    "                self.d_k = d_model // num_heads\n",
    "                \n",
    "                self.W_q = nn.Linear(d_model, d_model)\n",
    "                self.W_k = nn.Linear(d_model, d_model)\n",
    "                self.W_v = nn.Linear(d_model, d_model)\n",
    "                self.W_o = nn.Linear(d_model, d_model)\n",
    "                \n",
    "            def forward(self, x):\n",
    "                batch_size, seq_len, d_model = x.size()\n",
    "                \n",
    "                # Linear transformations\n",
    "                Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "                K = self.W_k(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "                V = self.W_v(x).view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)\n",
    "                \n",
    "                # Compute attention scores\n",
    "                scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_k)\n",
    "                attention_weights = F.softmax(scores, dim=-1)\n",
    "                \n",
    "                # Apply attention to values\n",
    "                attended_values = torch.matmul(attention_weights, V)\n",
    "                \n",
    "                # Concatenate heads\n",
    "                attended_values = attended_values.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "                \n",
    "                # Output projection\n",
    "                output = self.W_o(attended_values)\n",
    "                \n",
    "                return output, attention_weights\n",
    "        \n",
    "        mha = MultiHeadAttention(self.d_model, num_heads)\n",
    "        output, attention_weights = mha(x)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def cross_attention(self, x1, x2):\n",
    "        \"\"\"Implementa cross-attention\"\"\"\n",
    "        \n",
    "        # Linear transformations\n",
    "        W_q = nn.Linear(self.d_model, self.d_model, bias=False)\n",
    "        W_k = nn.Linear(self.d_model, self.d_model, bias=False)\n",
    "        W_v = nn.Linear(self.d_model, self.d_model, bias=False)\n",
    "        \n",
    "        # Compute Q from x1, K and V from x2\n",
    "        Q = W_q(x1)\n",
    "        K = W_k(x2)\n",
    "        V = W_v(x2)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(self.d_model)\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def visualize_attention_weights(self, attention_weights, title=\"Attention Weights\"):\n",
    "        \"\"\"Visualiza pesos de atenÃ§Ã£o\"\"\"\n",
    "        \n",
    "        # Converter para numpy\n",
    "        if isinstance(attention_weights, torch.Tensor):\n",
    "            attention_weights = attention_weights.detach().numpy()\n",
    "        \n",
    "        # Remover dimensÃµes de batch se necessÃ¡rio\n",
    "        if attention_weights.ndim == 4:  # Multi-head attention\n",
    "            attention_weights = attention_weights[0]  # Primeiro batch\n",
    "        elif attention_weights.ndim == 3:\n",
    "            attention_weights = attention_weights[0]  # Primeiro batch\n",
    "        \n",
    "        # Visualizar\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        if attention_weights.ndim == 3:  # Multi-head\n",
    "            # Visualizar cada cabeÃ§a\n",
    "            num_heads = attention_weights.shape[0]\n",
    "            cols = min(4, num_heads)\n",
    "            rows = (num_heads + cols - 1) // cols\n",
    "            \n",
    "            for i in range(num_heads):\n",
    "                plt.subplot(rows, cols, i + 1)\n",
    "                sns.heatmap(attention_weights[i], cmap='Blues', cbar=True)\n",
    "                plt.title(f'Head {i + 1}')\n",
    "                plt.xlabel('Key Position')\n",
    "                plt.ylabel('Query Position')\n",
    "        else:  # Single head\n",
    "            sns.heatmap(attention_weights, cmap='Blues', cbar=True)\n",
    "            plt.title(title)\n",
    "            plt.xlabel('Key Position')\n",
    "            plt.ylabel('Query Position')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def demonstrate_attention_mechanisms(self):\n",
    "        \"\"\"Demonstra diferentes mecanismos de atenÃ§Ã£o\"\"\"\n",
    "        \n",
    "        # Criar dados de exemplo\n",
    "        x = self.create_sample_data()\n",
    "        \n",
    "        print(\"=== DEMONSTRAÃ‡ÃƒO: MECANISMOS DE ATENÃ‡ÃƒO ===\")\n",
    "        print(f\"SequÃªncia de entrada: {x.shape}\")\n",
    "        print(f\"DimensÃ£o do modelo: {self.d_model}\")\n",
    "        print(f\"Comprimento da sequÃªncia: {self.seq_len}\")\n",
    "        \n",
    "        # Self-attention\n",
    "        print(\"\\n1. Self-Attention:\")\n",
    "        sa_output, sa_weights = self.self_attention(x)\n",
    "        print(f\"   Output shape: {sa_output.shape}\")\n",
    "        print(f\"   Attention weights shape: {sa_weights.shape}\")\n",
    "        \n",
    "        # Multi-head attention\n",
    "        print(\"\\n2. Multi-Head Attention:\")\n",
    "        mha_output, mha_weights = self.multi_head_attention(x, num_heads=8)\n",
    "        print(f\"   Output shape: {mha_output.shape}\")\n",
    "        print(f\"   Attention weights shape: {mha_weights.shape}\")\n",
    "        \n",
    "        # Cross-attention\n",
    "        print(\"\\n3. Cross-Attention:\")\n",
    "        x2 = torch.randn(1, self.seq_len, self.d_model)  # Segunda sequÃªncia\n",
    "        ca_output, ca_weights = self.cross_attention(x, x2)\n",
    "        print(f\"   Output shape: {ca_output.shape}\")\n",
    "        print(f\"   Attention weights shape: {ca_weights.shape}\")\n",
    "        \n",
    "        # Visualizar pesos de atenÃ§Ã£o\n",
    "        print(\"\\n=== VISUALIZAÃ‡ÃƒO DOS PESOS DE ATENÃ‡ÃƒO ===\")\n",
    "        \n",
    "        # Self-attention\n",
    "        self.visualize_attention_weights(sa_weights, \"Self-Attention Weights\")\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.visualize_attention_weights(mha_weights, \"Multi-Head Attention Weights\")\n",
    "        \n",
    "        # Cross-attention\n",
    "        self.visualize_attention_weights(ca_weights, \"Cross-Attention Weights\")\n",
    "        \n",
    "        # AnÃ¡lise quantitativa\n",
    "        print(\"\\n=== ANÃLISE QUANTITATIVA ===\")\n",
    "        \n",
    "        # Self-attention\n",
    "        sa_weights_np = sa_weights.detach().numpy()[0]\n",
    "        print(f\"\\nSelf-Attention:\")\n",
    "        print(f\"  - Peso mÃ¡ximo: {np.max(sa_weights_np):.4f}\")\n",
    "        print(f\"  - Peso mÃ­nimo: {np.min(sa_weights_np):.4f}\")\n",
    "        print(f\"  - Peso mÃ©dio: {np.mean(sa_weights_np):.4f}\")\n",
    "        print(f\"  - Entropia: {np.sum(-sa_weights_np * np.log(sa_weights_np + 1e-8)):.4f}\")\n",
    "        \n",
    "        # Multi-head attention\n",
    "        mha_weights_np = mha_weights.detach().numpy()[0]\n",
    "        print(f\"\\nMulti-Head Attention:\")\n",
    "        print(f\"  - Peso mÃ¡ximo: {np.max(mha_weights_np):.4f}\")\n",
    "        print(f\"  - Peso mÃ­nimo: {np.min(mha_weights_np):.4f}\")\n",
    "        print(f\"  - Peso mÃ©dio: {np.mean(mha_weights_np):.4f}\")\n",
    "        print(f\"  - Entropia mÃ©dia: {np.mean([np.sum(-head * np.log(head + 1e-8)) for head in mha_weights_np]):.4f}\")\n",
    "        \n",
    "        # Cross-attention\n",
    "        ca_weights_np = ca_weights.detach().numpy()[0]\n",
    "        print(f\"\\nCross-Attention:\")\n",
    "        print(f\"  - Peso mÃ¡ximo: {np.max(ca_weights_np):.4f}\")\n",
    "        print(f\"  - Peso mÃ­nimo: {np.min(ca_weights_np):.4f}\")\n",
    "        print(f\"  - Peso mÃ©dio: {np.mean(ca_weights_np):.4f}\")\n",
    "        print(f\"  - Entropia: {np.sum(-ca_weights_np * np.log(ca_weights_np + 1e-8)):.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'self_attention': (sa_output, sa_weights),\n",
    "            'multi_head_attention': (mha_output, mha_weights),\n",
    "            'cross_attention': (ca_output, ca_weights)\n",
    "        }\n",
    "\n",
    "# Executar demonstraÃ§Ã£o\n",
    "print(\"=== DEMONSTRAÃ‡ÃƒO: MECANISMOS DE ATENÃ‡ÃƒO ===\")\n",
    "attention_demo = AttentionMechanismsDemo(seq_len=10, d_model=64)\n",
    "results = attention_demo.demonstrate_attention_mechanisms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AnÃ¡lise dos Resultados\n",
    "\n",
    "**ObservaÃ§Ãµes Importantes:**\n",
    "\n",
    "1. **Self-Attention**:\n",
    "   - **Pesos**: DistribuiÃ§Ã£o de atenÃ§Ã£o entre elementos\n",
    "   - **PadrÃµes**: IdentificaÃ§Ã£o de relaÃ§Ãµes importantes\n",
    "   - **Contexto**: ConsideraÃ§Ã£o de toda a sequÃªncia\n",
    "\n",
    "2. **Multi-Head Attention**:\n",
    "   - **Diversidade**: Diferentes cabeÃ§as capturam diferentes padrÃµes\n",
    "   - **Robustez**: Menos sensÃ­vel a falhas\n",
    "   - **Capacidade**: Maior poder expressivo\n",
    "\n",
    "3. **Cross-Attention**:\n",
    "   - **InteraÃ§Ã£o**: RelaÃ§Ãµes entre sequÃªncias diferentes\n",
    "   - **Flexibilidade**: AdaptaÃ§Ã£o a diferentes modalidades\n",
    "   - **AplicaÃ§Ã£o**: Ãštil para tarefas multimodais\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ–¼ï¸ 8.4 DemonstraÃ§Ã£o PrÃ¡tica: Vision Transformer Simples\n",
    "\n",
    "Vamos implementar e visualizar um Vision Transformer simples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "class SimpleVisionTransformer:\n",
    "    \"\"\"ImplementaÃ§Ã£o de um Vision Transformer simples\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=32, patch_size=8, num_classes=10, d_model=128, num_heads=8, num_layers=6):\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_classes = num_classes\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Calcular nÃºmero de patches\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Inicializar modelo\n",
    "        self.vit = self._build_vit()\n",
    "        \n",
    "    def _build_vit(self):\n",
    "        \"\"\"ConstrÃ³i o Vision Transformer\"\"\"\n",
    "        \n",
    "        class PatchEmbedding(nn.Module):\n",
    "            def __init__(self, img_size, patch_size, d_model):\n",
    "                super(PatchEmbedding, self).__init__()\n",
    "                self.img_size = img_size\n",
    "                self.patch_size = patch_size\n",
    "                self.num_patches = (img_size // patch_size) ** 2\n",
    "                \n",
    "                # Patch embedding\n",
    "                self.patch_embed = nn.Conv2d(3, d_model, kernel_size=patch_size, stride=patch_size)\n",
    "                \n",
    "                # Positional embedding\n",
    "                self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches + 1, d_model))\n",
    "                \n",
    "                # CLS token\n",
    "                self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "                \n",
    "            def forward(self, x):\n",
    "                B = x.shape[0]\n",
    "                \n",
    "                # Patch embedding\n",
    "                x = self.patch_embed(x)  # (B, d_model, H/patch_size, W/patch_size)\n",
    "                x = x.flatten(2).transpose(1, 2)  # (B, num_patches, d_model)\n",
    "                \n",
    "                # Adicionar CLS token\n",
    "                cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "                x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches + 1, d_model)\n",
    "                \n",
    "                # Adicionar positional embedding\n",
    "                x = x + self.pos_embed\n",
    "                \n",
    "                return x\n",
    "        \n",
    "        class TransformerBlock(nn.Module):\n",
    "            def __init__(self, d_model, num_heads):\n",
    "                super(TransformerBlock, self).__init__()\n",
    "                \n",
    "                # Multi-head attention\n",
    "                self.attention = nn.MultiheadAttention(d_model, num_heads, batch_first=True)\n",
    "                \n",
    "                # Feed forward\n",
    "                self.feed_forward = nn.Sequential(\n",
    "                    nn.Linear(d_model, d_model * 4),\n",
    "                    nn.GELU(),\n",
    "                    nn.Linear(d_model * 4, d_model)\n",
    "                )\n",
    "                \n",
    "                # Layer normalization\n",
    "                self.norm1 = nn.LayerNorm(d_model)\n",
    "                self.norm2 = nn.LayerNorm(d_model)\n",
    "                \n",
    "            def forward(self, x):\n",
    "                # Self-attention\n",
    "                attn_output, attn_weights = self.attention(x, x, x)\n",
    "                x = x + attn_output\n",
    "                x = self.norm1(x)\n",
    "                \n",
    "                # Feed forward\n",
    "                ff_output = self.feed_forward(x)\n",
    "                x = x + ff_output\n",
    "                x = self.norm2(x)\n",
    "                \n",
    "                return x, attn_weights\n",
    "        \n",
    "        class VisionTransformer(nn.Module):\n",
    "            def __init__(self, img_size, patch_size, num_classes, d_model, num_heads, num_layers):\n",
    "                super(VisionTransformer, self).__init__()\n",
    "                \n",
    "                # Patch embedding\n",
    "                self.patch_embedding = PatchEmbedding(img_size, patch_size, d_model)\n",
    "                \n",
    "                # Transformer blocks\n",
    "                self.transformer_blocks = nn.ModuleList([\n",
    "                    TransformerBlock(d_model, num_heads) for _ in range(num_layers)\n",
    "                ])\n",
    "                \n",
    "                # Classification head\n",
    "                self.classifier = nn.Linear(d_model, num_classes)\n",
    "                \n",
    "            def forward(self, x):\n",
    "                # Patch embedding\n",
    "                x = self.patch_embedding(x)\n",
    "                \n",
    "                # Transformer blocks\n",
    "                attention_weights = []\n",
    "                for transformer_block in self.transformer_blocks:\n",
    "                    x, attn_weights = transformer_block(x)\n",
    "                    attention_weights.append(attn_weights)\n",
    "                \n",
    "                # Classification\n",
    "                cls_output = x[:, 0]  # CLS token\n",
    "                logits = self.classifier(cls_output)\n",
    "                \n",
    "                return logits, attention_weights\n",
    "        \n",
    "        return VisionTransformer(\n",
    "            self.img_size, self.patch_size, self.num_classes,\n",
    "            self.d_model, self.num_heads, self.num_layers\n",
    "        ).to(self.device)\n",
    "    \n",
    "    def create_sample_images(self, num_samples=16):\n",
    "        \"\"\"Cria imagens de exemplo para demonstraÃ§Ã£o\"\"\"\n",
    "        \n",
    "        images = []\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            # Criar imagem com padrÃµes simples\n",
    "            img = np.random.rand(3, self.img_size, self.img_size) * 2 - 1  # Normalizar para [-1, 1]\n",
    "            \n",
    "            # Adicionar padrÃµes\n",
    "            if np.random.random() > 0.5:\n",
    "                # PadrÃ£o circular\n",
    "                center_x, center_y = np.random.randint(8, self.img_size-8, 2)\n",
    "                radius = np.random.randint(4, 8)\n",
    "                \n",
    "                y, x = np.ogrid[:self.img_size, :self.img_size]\n",
    "                mask = (x - center_x)**2 + (y - center_y)**2 <= radius**2\n",
    "                \n",
    "                img[0, mask] = 1.0  # Red\n",
    "                img[1, mask] = 0.0  # Green\n",
    "                img[2, mask] = 0.0  # Blue\n",
    "            else:\n",
    "                # PadrÃ£o retangular\n",
    "                x1, y1 = np.random.randint(0, self.img_size-12, 2)\n",
    "                x2, y2 = x1 + np.random.randint(8, 16), y1 + np.random.randint(8, 16)\n",
    "                \n",
    "                img[0, y1:y2, x1:x2] = 0.0  # Red\n",
    "                img[1, y1:y2, x1:x2] = 1.0  # Green\n",
    "                img[2, y1:y2, x1:x2] = 0.0  # Blue\n",
    "            \n",
    "            images.append(img)\n",
    "        \n",
    "        return torch.FloatTensor(images)\n",
    "    \n",
    "    def visualize_patches(self, img, patch_size):\n",
    "        \"\"\"Visualiza patches de uma imagem\"\"\"\n",
    "        \n",
    "        # Converter para numpy\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.detach().numpy()\n",
    "        \n",
    "        # Normalizar para [0, 1]\n",
    "        img = (img + 1) / 2\n",
    "        img = np.clip(img, 0, 1)\n",
    "        \n",
    "        # Converter para formato de visualizaÃ§Ã£o\n",
    "        img_vis = img.transpose(1, 2, 0)\n",
    "        \n",
    "        # Criar grid de patches\n",
    "        h, w = img_vis.shape[:2]\n",
    "        num_patches_h = h // patch_size\n",
    "        num_patches_w = w // patch_size\n",
    "        \n",
    "        fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "        ax.imshow(img_vis)\n",
    "        \n",
    "        # Desenhar linhas dos patches\n",
    "        for i in range(num_patches_h + 1):\n",
    "            ax.axhline(y=i * patch_size, color='red', linewidth=1)\n",
    "        for j in range(num_patches_w + 1):\n",
    "            ax.axvline(x=j * patch_size, color='red', linewidth=1)\n",
    "        \n",
    "        ax.set_title(f'Imagem com Patches ({patch_size}Ã—{patch_size})')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return num_patches_h * num_patches_w\n",
    "    \n",
    "    def visualize_attention_maps(self, attention_weights, img_size, patch_size):\n",
    "        \"\"\"Visualiza mapas de atenÃ§Ã£o\"\"\"\n",
    "        \n",
    "        # Converter para numpy\n",
    "        if isinstance(attention_weights, torch.Tensor):\n",
    "            attention_weights = attention_weights.detach().numpy()\n",
    "        \n",
    "        # Remover dimensÃµes de batch\n",
    "        if attention_weights.ndim == 4:  # Multi-head attention\n",
    "            attention_weights = attention_weights[0]  # Primeiro batch\n",
    "        elif attention_weights.ndim == 3:\n",
    "            attention_weights = attention_weights[0]  # Primeiro batch\n",
    "        \n",
    "        # Calcular nÃºmero de patches\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Visualizar\n",
    "        if attention_weights.ndim == 3:  # Multi-head\n",
    "            num_heads = attention_weights.shape[0]\n",
    "            cols = min(4, num_heads)\n",
    "            rows = (num_heads + cols - 1) // cols\n",
    "            \n",
    "            fig, axes = plt.subplots(rows, cols, figsize=(15, 4 * rows))\n",
    "            if rows == 1:\n",
    "                axes = [axes] if cols == 1 else axes\n",
    "            else:\n",
    "                axes = axes.flatten()\n",
    "            \n",
    "            for i in range(num_heads):\n",
    "                # CLS token attention (primeira linha)\n",
    "                cls_attention = attention_weights[i, 0, 1:]  # Remover CLS token\n",
    "                \n",
    "                # Reshape para imagem\n",
    "                patch_size_h = img_size // patch_size\n",
    "                attention_map = cls_attention.reshape(patch_size_h, patch_size_h)\n",
    "                \n",
    "                # Visualizar\n",
    "                im = axes[i].imshow(attention_map, cmap='hot', interpolation='nearest')\n",
    "                axes[i].set_title(f'Head {i + 1} - CLS Attention')\n",
    "                axes[i].axis('off')\n",
    "                plt.colorbar(im, ax=axes[i])\n",
    "            \n",
    "            # Ocultar eixos extras\n",
    "            for i in range(num_heads, len(axes)):\n",
    "                axes[i].axis('off')\n",
    "        else:  # Single head\n",
    "            # CLS token attention\n",
    "            cls_attention = attention_weights[0, 1:]  # Remover CLS token\n",
    "            \n",
    "            # Reshape para imagem\n",
    "            patch_size_h = img_size // patch_size\n",
    "            attention_map = cls_attention.reshape(patch_size_h, patch_size_h)\n",
    "            \n",
    "            # Visualizar\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.imshow(attention_map, cmap='hot', interpolation='nearest')\n",
    "            plt.title('CLS Token Attention Map')\n",
    "            plt.colorbar()\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def demonstrate_vit(self):\n",
    "        \"\"\"Demonstra o Vision Transformer\"\"\"\n",
    "        \n",
    "        print(\"=== DEMONSTRAÃ‡ÃƒO: VISION TRANSFORMER ===\")\n",
    "        print(f\"Tamanho da imagem: {self.img_size}Ã—{self.img_size}\")\n",
    "        print(f\"Tamanho do patch: {self.patch_size}Ã—{self.patch_size}\")\n",
    "        print(f\"NÃºmero de patches: {self.num_patches}\")\n",
    "        print(f\"DimensÃ£o do modelo: {self.d_model}\")\n",
    "        print(f\"NÃºmero de cabeÃ§as: {self.num_heads}\")\n",
    "        print(f\"NÃºmero de camadas: {self.num_layers}\")\n",
    "        \n",
    "        # Criar imagens de exemplo\n",
    "        images = self.create_sample_images(16)\n",
    "        \n",
    "        # Visualizar patches\n",
    "        print(\"\\n=== VISUALIZAÃ‡ÃƒO DE PATCHES ===\")\n",
    "        num_patches = self.visualize_patches(images[0], self.patch_size)\n",
    "        print(f\"NÃºmero de patches por imagem: {num_patches}\")\n",
    "        \n",
    "        # Processar com ViT\n",
    "        print(\"\\n=== PROCESSAMENTO COM VIT ===\")\n",
    "        self.vit.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Processar primeira imagem\n",
    "            img = images[0:1].to(self.device)\n",
    "            logits, attention_weights = self.vit(img)\n",
    "            \n",
    "            print(f\"Logits shape: {logits.shape}\")\n",
    "            print(f\"NÃºmero de camadas de atenÃ§Ã£o: {len(attention_weights)}\")\n",
    "            print(f\"Shape dos pesos de atenÃ§Ã£o: {attention_weights[0].shape}\")\n",
    "            \n",
    "            # PrediÃ§Ã£o\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            predicted_class = torch.argmax(probs, dim=1).item()\n",
    "            confidence = probs[0, predicted_class].item()\n",
    "            \n",
    "            print(f\"Classe predita: {predicted_class}\")\n",
    "            print(f\"ConfianÃ§a: {confidence:.4f}\")\n",
    "        \n",
    "        # Visualizar mapas de atenÃ§Ã£o\n",
    "        print(\"\\n=== VISUALIZAÃ‡ÃƒO DE MAPAS DE ATENÃ‡ÃƒO ===\")\n",
    "        \n",
    "        # Primeira camada\n",
    "        print(\"\\nPrimeira camada:\")\n",
    "        self.visualize_attention_maps(attention_weights[0], self.img_size, self.patch_size)\n",
    "        \n",
    "        # Ãšltima camada\n",
    "        print(\"\\nÃšltima camada:\")\n",
    "        self.visualize_attention_maps(attention_weights[-1], self.img_size, self.patch_size)\n",
    "        \n",
    "        # AnÃ¡lise quantitativa\n",
    "        print(\"\\n=== ANÃLISE QUANTITATIVA ===\")\n",
    "        \n",
    "        # Analisar evoluÃ§Ã£o da atenÃ§Ã£o\n",
    "        first_layer_attention = attention_weights[0].detach().numpy()[0]  # Primeira cabeÃ§a\n",
    "        last_layer_attention = attention_weights[-1].detach().numpy()[0]  # Primeira cabeÃ§a\n",
    "        \n",
    "        print(f\"\\nEvoluÃ§Ã£o da AtenÃ§Ã£o:\")\n",
    "        print(f\"  - Primeira camada - Entropia: {np.sum(-first_layer_attention * np.log(first_layer_attention + 1e-8)):.4f}\")\n",
    "        print(f\"  - Ãšltima camada - Entropia: {np.sum(-last_layer_attention * np.log(last_layer_attention + 1e-8)):.4f}\")\n",
    "        print(f\"  - DiferenÃ§a de entropia: {np.sum(-last_layer_attention * np.log(last_layer_attention + 1e-8)) - np.sum(-first_layer_attention * np.log(first_layer_attention + 1e-8)):.4f}\")\n",
    "        \n",
    "        # Analisar concentraÃ§Ã£o da atenÃ§Ã£o\n",
    "        first_cls_attention = first_layer_attention[0, 1:]  # CLS token\n",
    "        last_cls_attention = last_layer_attention[0, 1:]  # CLS token\n",
    "        \n",
    "        print(f\"\\nConcentraÃ§Ã£o da AtenÃ§Ã£o (CLS Token):\")\n",
    "        print(f\"  - Primeira camada - Max: {np.max(first_cls_attention):.4f}\")\n",
    "        print(f\"  - Primeira camada - Min: {np.min(first_cls_attention):.4f}\")\n",
    "        print(f\"  - Ãšltima camada - Max: {np.max(last_cls_attention):.4f}\")\n",
    "        print(f\"  - Ãšltima camada - Min: {np.min(last_cls_attention):.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'logits': logits,\n",
    "            'attention_weights': attention_weights,\n",
    "            'predicted_class': predicted_class,\n",
    "            'confidence': confidence\n",
    "        }\n",
    "\n",
    "# Executar demonstraÃ§Ã£o\n",
    "print(\"=== DEMONSTRAÃ‡ÃƒO: VISION TRANSFORMER ===\")\n",
    "vit_demo = SimpleVisionTransformer(\n",
    "    img_size=32, patch_size=8, num_classes=10,\n",
    "    d_model=128, num_heads=8, num_layers=6\n",
    ")\n",
    "results = vit_demo.demonstrate_vit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AnÃ¡lise dos Resultados",
    "",
    "**ObservaÃ§Ãµes Importantes:**",
    "",
    "1. **Patches**:",
    "   - **DivisÃ£o**: Imagem dividida em patches regulares",
    "   - **Embedding**: Cada patch convertido em vetor",
    "   - **PosiÃ§Ã£o**: InformaÃ§Ã£o de posiÃ§Ã£o preservada",
    "",
    "2. **Mapas de AtenÃ§Ã£o**:",
    "   - **EvoluÃ§Ã£o**: AtenÃ§Ã£o muda entre camadas",
    "   - **ConcentraÃ§Ã£o**: Ãšltima camada mais concentrada",
    "   - **Interpretabilidade**: VisualizaÃ§Ã£o de foco",
    "",
    "3. **ClassificaÃ§Ã£o**:",
    "   - **CLS Token**: Agrega informaÃ§Ã£o global",
    "   - **ConfianÃ§a**: Medida de certeza da prediÃ§Ã£o",
    "   - **Performance**: ViT funciona bem para classificaÃ§Ã£o",
    "",
    "---",
    "",
    "## ğŸ“Š 8.5 ComparaÃ§Ã£o: CNNs vs Vision Transformers",
    "",
    "### AnÃ¡lise Comparativa",
    "",
    "![ComparaÃ§Ã£o CNNs vs ViT](https://miro.medium.com/v2/resize:fit:1400/1*vsUdS14WxE9YqF8GBCZJHg.png)",
    "",
    "#### **Arquitetura**",
    "",
    "| Aspecto | CNNs | Vision Transformers |",
    "|---------|------|---------------------|",
    "| **InduÃ§Ã£o** | Local | Global |",
    "| **ParalelizaÃ§Ã£o** | Limitada | Completa |",
    "| **Escalabilidade** | Moderada | Alta |",
    "| **Interpretabilidade** | Baixa | Alta |",
    "",
    "#### **Performance**",
    "",
    "| Aspecto | CNNs | Vision Transformers |",
    "|---------|------|---------------------|",
    "| **Dados pequenos** | Boa | Limitada |",
    "| **Dados grandes** | Boa | Excelente |",
    "| **Treinamento** | EstÃ¡vel | EstÃ¡vel |",
    "| **InferÃªncia** | RÃ¡pida | Moderada |",
    "",
    "#### **AplicaÃ§Ãµes**",
    "",
    "| Aspecto | CNNs | Vision Transformers |",
    "|---------|------|---------------------|",
    "| **ClassificaÃ§Ã£o** | Excelente | Excelente |",
    "| **DetecÃ§Ã£o** | Excelente | Boa |",
    "| **SegmentaÃ§Ã£o** | Excelente | Boa |",
    "| **Transfer Learning** | Boa | Excelente |",
    "",
    "### Quando Usar Cada Um",
    "",
    "#### **Use CNNs quando:**",
    "- âœ… **Dados limitados** estÃ£o disponÃ­veis",
    "- âœ… **PadrÃµes locais** sÃ£o importantes",
    "- âœ… **Velocidade** Ã© prioritÃ¡ria",
    "- âœ… **Recursos limitados** estÃ£o disponÃ­veis",
    "",
    "#### **Use Vision Transformers quando:**",
    "- âœ… **Grandes datasets** estÃ£o disponÃ­veis",
    "- âœ… **PadrÃµes globais** sÃ£o importantes",
    "- âœ… **Interpretabilidade** Ã© necessÃ¡ria",
    "- âœ… **Transfer learning** Ã© prioritÃ¡rio",
    "",
    "---",
    "",
    "## ğŸ“ Resumo do MÃ³dulo 8",
    "",
    "### Principais Conceitos Abordados",
    "",
    "1. **Fundamentos**: Mecanismos de atenÃ§Ã£o",
    "2. **Tipos**: Self-attention, Multi-head, Cross-attention",
    "3. **Vision Transformers**: Arquitetura para imagens",
    "4. **ImplementaÃ§Ã£o**: DemonstraÃ§Ãµes prÃ¡ticas",
    "5. **ComparaÃ§Ã£o**: CNNs vs Vision Transformers",
    "",
    "### DemonstraÃ§Ãµes PrÃ¡ticas",
    "",
    "**1. Mecanismos de AtenÃ§Ã£o:**",
    "   - ImplementaÃ§Ã£o de diferentes tipos de atenÃ§Ã£o",
    "   - VisualizaÃ§Ã£o de pesos de atenÃ§Ã£o",
    "   - AnÃ¡lise quantitativa",
    "",
    "**2. Vision Transformer:**",
    "   - ImplementaÃ§Ã£o de ViT simples",
    "   - VisualizaÃ§Ã£o de patches e mapas de atenÃ§Ã£o",
    "   - AnÃ¡lise de evoluÃ§Ã£o da atenÃ§Ã£o",
    "",
    "### PrÃ³ximos Passos",
    "",
    "No **MÃ³dulo 9**, exploraremos **Foundation Models** para visÃ£o computacional, incluindo CLIP, DALL-E e GPT-4V.",
    "",
    "### ReferÃªncias Principais",
    "",
    "- [Attention Is All You Need - Vaswani et al.](https://arxiv.org/abs/1706.03762)",
    "- [An Image is Worth 16x16 Words: Transformers for Image Recognition - Dosovitskiy et al.](https://arxiv.org/abs/2010.11929)",
    "",
    "---",
    "",
    "**PrÃ³ximo MÃ³dulo**: Foundation Models para VisÃ£o Computacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ConexÃ£o com o PrÃ³ximo MÃ³dulo\n",
    "\n",
    "Agora que dominamos **Vision Transformers e Mecanismos de AtenÃ§Ã£o**, estamos preparados para explorar **Foundation Models** para visÃ£o computacional.\n",
    "\n",
    "No **MÃ³dulo 9**, veremos como:\n",
    "\n",
    "### ğŸ”— **ConexÃµes Diretas:**\n",
    "\n",
    "1. **AtenÃ§Ã£o** â†’ **Foundation Models**\n",
    "   - Vision Transformers usam atenÃ§Ã£o\n",
    "   - Foundation Models sÃ£o baseados em Transformers\n",
    "\n",
    "2. **Arquiteturas EscalÃ¡veis** â†’ **Modelos Massivos**\n",
    "   - ViT Ã© escalÃ¡vel\n",
    "   - Foundation Models sÃ£o modelos massivos\n",
    "\n",
    "3. **Transfer Learning** â†’ **Zero-shot Learning**\n",
    "   - ViT Ã© bom para transfer learning\n",
    "   - Foundation Models permitem zero-shot learning\n",
    "\n",
    "4. **Interpretabilidade** â†’ **Capacidades Emergentes**\n",
    "   - ViT Ã© interpretÃ¡vel\n",
    "   - Foundation Models tÃªm capacidades emergentes\n",
    "\n",
    "### ğŸš€ **EvoluÃ§Ã£o Natural:**\n",
    "\n",
    "- **AtenÃ§Ã£o** â†’ **Foundation Models**\n",
    "- **Arquiteturas** â†’ **Modelos Massivos**\n",
    "- **Transfer Learning** â†’ **Zero-shot Learning**\n",
    "- **Interpretabilidade** â†’ **Capacidades Emergentes**\n",
    "\n",
    "Esta transiÃ§Ã£o marca o inÃ­cio da **era dos Foundation Models** em visÃ£o computacional!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ–¼ï¸ Imagens de ReferÃªncia - MÃ³dulo 8",
    "",
    "![AplicaÃ§Ãµes PrÃ¡ticas](https://raw.githubusercontent.com/facebookresearch/segment-anything/main/assets/model_diagram.png)",
    "",
    "![Vantagens Vision Transformers](https://production-media.paperswithcode.com/methods/Screen_Shot_2021-01-26_at_9.43.31_PM_uI4jjMq.png)",
    "",
    "![Vision Transformers](https://raw.githubusercontent.com/google-research/vision_transformer/main/vit_figure.png)",
    "",
    ""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}